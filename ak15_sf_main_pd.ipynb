{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook for ParticleNet AK15 cc-tagger SF derivation\n",
    "## (`uproot`+`pandas` workflow)\n",
    "\n",
    "The notebook aims to\n",
    " - Make the ROOT-format **templates** for fit\n",
    " - Produce **data/MC comparison plots** under some given event selection\n",
    " - Produce **H->cc signal and g->cc proxy jets comparison plots** on various jet observables\n",
    " \n",
    "We adopt the `uproot`+`pandas`* workflow in this notebook, illustrated as follows:\n",
    "\n",
    "    Input files (flat ROOT-tuples derived from analysis NanoAOD)\n",
    "    -> load as `pandas` DataFrame (by `uproot`)\n",
    "    -> manipulate the dataframe\n",
    "    -> produce histograms (`boost_histogram`)\n",
    "    -> (1) convert to TH1D for ROOT template; or (2) plot with `mplhep` using `matplotlib` as backend\n",
    "    \n",
    " \n",
    "(*) Note: this workflow suffers from large RAM usage in the runtime. It may consume 10-30 GB of RAM if dealing with large datasets, hence set requirement to the machine. \n",
    "A smarter workflow would be `coffea` (with lazy awkward-array as backend) which the future framework is planned to be migrated on.\n",
    "\n",
    "(Updated Dec.24: The notebook using `coffea`+`awkward` workflow is available at `ak15_sf_main_ak.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make templates for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot3 as uproot\n",
    "from uproot3_methods import TLorentzVectorArray, TLorentzVector\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boost_histogram as bh\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "use_helvet = True  ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "mpl.use('AGG') # no rendering plots in the window\n",
    "\n",
    "def get_hist(array, bins=10, xmin=None, xmax=None, underflow=False, overflow=False, mergeflowbin=True, normed=False,\n",
    "            weights=None, **kwargs):\n",
    "    r\"\"\"Plot histogram from input array.\n",
    "\n",
    "    Arguments:\n",
    "        array (np.ndarray): input array.\n",
    "        bins (int, list or tuple of numbers, np.ndarray, bh.axis): bins\n",
    "        weights (None, or np.ndarray): weights\n",
    "        # normed (bool): deprecated.\n",
    "\n",
    "    Returns:\n",
    "        hist (boost_histogram.Histogram)\n",
    "    \"\"\"\n",
    "    if isinstance(bins, int):\n",
    "        if xmin is None:\n",
    "            xmin = array.min()\n",
    "        if xmax is None:\n",
    "            xmax = array.max()\n",
    "        width = 1.*(xmax-xmin)/bins\n",
    "        if mergeflowbin and underflow:\n",
    "            xmin += width\n",
    "            bins -= 1\n",
    "        if mergeflowbin and underflow:\n",
    "            xmax -= width\n",
    "            bins -= 1\n",
    "        bins = bh.axis.Regular(bins, xmin, xmax, underflow=underflow, overflow=overflow)\n",
    "    elif isinstance(bins, (list, tuple, np.ndarray)):\n",
    "        if mergeflowbin and underflow:\n",
    "            bins = bins[1:]\n",
    "        if mergeflowbin and overflow:\n",
    "            bins = bins[:-1]\n",
    "        bins = bh.axis.Variable(bins, underflow=underflow, overflow=overflow)\n",
    "\n",
    "    hist = bh.Histogram(bins, storage=bh.storage.Weight())\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(array)\n",
    "    hist.fill(array, weight=weights)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def plot_hist(hists, normed=False, **kwargs):\n",
    "    r\"\"\"Plot the histogram in the type of boost_histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(hists, (list, tuple)):\n",
    "        hists = [hists]\n",
    "    content = [h.view(flow=True).value for h in hists]\n",
    "    bins = hists[0].axes[0].edges\n",
    "    if 'bins' in kwargs:\n",
    "        bins = kwargs.pop('bins')\n",
    "    if 'yerr' in kwargs:\n",
    "        yerr = kwargs.pop('yerr')\n",
    "    else:\n",
    "        yerr = [np.sqrt(h.view(flow=True).variance) for h in hists]\n",
    "    if normed:\n",
    "        for i in range(len(content)):\n",
    "            contsum = sum(content[i])\n",
    "            content[i] /= contsum\n",
    "            yerr[i] /= contsum\n",
    "    if len(hists) == 1:\n",
    "        content, yerr = content[0], yerr[0]\n",
    "    hep.histplot(content, bins=bins, yerr=yerr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the config.yml\n",
    "import yaml\n",
    "with open('config.yml') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load files\n",
    "\n",
    "Load the ROOT files into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = config['samples']['year']\n",
    "\n",
    "lumi = {2016: 35.92, 2017: 41.53, 2018: 59.74}\n",
    "\n",
    "minimal_branches = [  ## minimal set of branches read into the notebook\n",
    "    \"run\", \"luminosityBlock\", \"event\", \"genWeight\", \"jetR\", \"passmetfilters\", \n",
    "    \"fj_1_dr_H\", \"fj_1_dr_Z\", \"fj_1_pt\", \"fj_1_eta\", \"fj_1_phi\", \"fj_1_rawmass\", \"fj_1_sdmass\", \"fj_1_tau21\", \"fj_1_btagcsvv2\", \"fj_1_btagjp\", \"fj_1_nsv\", \"fj_1_nsv_ptgt25\", \"fj_1_nsv_ptgt50\", \"fj_1_ntracks\", \"fj_1_ntracks_sv12\", \"fj_1_deltaR_sj12\", \"fj_1_sj1_pt\", \"fj_1_sj1_eta\", \"fj_1_sj1_phi\", \"fj_1_sj1_rawmass\", \"fj_1_sj1_ntracks\", \"fj_1_sj1_nsv\", \"fj_1_sj1_sv1_pt\", \"fj_1_sj1_sv1_mass\", \"fj_1_sj1_sv1_masscor\", \"fj_1_sj1_sv1_ntracks\", \"fj_1_sj1_sv1_dxy\", \"fj_1_sj1_sv1_dxysig\", \"fj_1_sj1_sv1_dlen\", \"fj_1_sj1_sv1_dlensig\", \"fj_1_sj1_sv1_chi2ndof\", \"fj_1_sj1_sv1_pangle\", \"fj_1_sj2_pt\", \"fj_1_sj2_eta\", \"fj_1_sj2_phi\", \"fj_1_sj2_rawmass\", \"fj_1_sj2_ntracks\", \"fj_1_sj2_nsv\", \"fj_1_sj2_sv1_pt\", \"fj_1_sj2_sv1_mass\", \"fj_1_sj2_sv1_masscor\", \"fj_1_sj2_sv1_ntracks\", \"fj_1_sj2_sv1_dxy\", \"fj_1_sj2_sv1_dxysig\", \"fj_1_sj2_sv1_dlen\", \"fj_1_sj2_sv1_dlensig\", \"fj_1_sj2_sv1_chi2ndof\", \"fj_1_sj2_sv1_pangle\", \"fj_1_sj12_masscor_dxysig\", \"fj_1_sfBDT\", \"fj_1_nbhadrons\", \"fj_1_nchadrons\", \"fj_1_sj1_nbhadrons\", \"fj_1_sj1_nchadrons\", \"fj_1_sj2_nbhadrons\", \"fj_1_sj2_nchadrons\", \n",
    "    \"fj_2_dr_H\", \"fj_2_dr_Z\", \"fj_2_pt\", \"fj_2_eta\", \"fj_2_phi\", \"fj_2_rawmass\", \"fj_2_sdmass\", \"fj_2_tau21\", \"fj_2_btagcsvv2\", \"fj_2_btagjp\", \"fj_2_nsv\", \"fj_2_nsv_ptgt25\", \"fj_2_nsv_ptgt50\", \"fj_2_ntracks\", \"fj_2_ntracks_sv12\", \"fj_2_deltaR_sj12\", \"fj_2_sj1_pt\", \"fj_2_sj1_eta\", \"fj_2_sj1_phi\", \"fj_2_sj1_rawmass\", \"fj_2_sj1_ntracks\", \"fj_2_sj1_nsv\", \"fj_2_sj1_sv1_pt\", \"fj_2_sj1_sv1_mass\", \"fj_2_sj1_sv1_masscor\", \"fj_2_sj1_sv1_ntracks\", \"fj_2_sj1_sv1_dxy\", \"fj_2_sj1_sv1_dxysig\", \"fj_2_sj1_sv1_dlen\", \"fj_2_sj1_sv1_dlensig\", \"fj_2_sj1_sv1_chi2ndof\", \"fj_2_sj1_sv1_pangle\", \"fj_2_sj2_pt\", \"fj_2_sj2_eta\", \"fj_2_sj2_phi\", \"fj_2_sj2_rawmass\", \"fj_2_sj2_ntracks\", \"fj_2_sj2_nsv\", \"fj_2_sj2_sv1_pt\", \"fj_2_sj2_sv1_mass\", \"fj_2_sj2_sv1_masscor\", \"fj_2_sj2_sv1_ntracks\", \"fj_2_sj2_sv1_dxy\", \"fj_2_sj2_sv1_dxysig\", \"fj_2_sj2_sv1_dlen\", \"fj_2_sj2_sv1_dlensig\", \"fj_2_sj2_sv1_chi2ndof\", \"fj_2_sj2_sv1_pangle\", \"fj_2_sj12_masscor_dxysig\", \"fj_2_sfBDT\", \"fj_2_nbhadrons\", \"fj_2_nchadrons\", \"fj_2_sj1_nbhadrons\", \"fj_2_sj1_nchadrons\", \"fj_2_sj2_nbhadrons\", \"fj_2_sj2_nchadrons\", \n",
    "    \"passHTTrig\", \"ht\", \"nlep\", \"fj_1_is_qualified\", \"fj_2_is_qualified\", \"puWeight\", \"puWeightUp\", \"puWeightDown\", \"xsecWeight\"\n",
    "]\n",
    "minimal_branches += [config['tagger']['var'].replace('fj_x', 'fj_1'), config['tagger']['var'].replace('fj_x', 'fj_2')]\n",
    "\n",
    "ext_hlt_branches = {  ## extra branches depend on year\n",
    "    2016: ['HLT_PFHT125', 'HLT_PFHT200', 'HLT_PFHT250', 'HLT_PFHT300', 'HLT_PFHT350', 'HLT_PFHT400', 'HLT_PFHT475', 'HLT_PFHT600', 'HLT_PFHT650', 'HLT_PFHT800', 'HLT_PFHT900'],\n",
    "    2017: ['HLT_PFHT180', 'HLT_PFHT250', 'HLT_PFHT370', 'HLT_PFHT430', 'HLT_PFHT510', 'HLT_PFHT590', 'HLT_PFHT680', 'HLT_PFHT780', 'HLT_PFHT890', 'HLT_PFHT1050', 'HLT_PFHT350'],\n",
    "    2018: ['HLT_PFHT180', 'HLT_PFHT250', 'HLT_PFHT370', 'HLT_PFHT430', 'HLT_PFHT510', 'HLT_PFHT590', 'HLT_PFHT680', 'HLT_PFHT780', 'HLT_PFHT890', 'HLT_PFHT1050', 'HLT_PFHT350'],\n",
    "}\n",
    "minimal_branches += ext_hlt_branches[year]\n",
    "minimal_branches += ['nPSWeight', 'PSWeight'] if year==2018 else []  ## extra PSWeight branches for 2018\n",
    "minimal_branches_for_data = set(minimal_branches) - set([\"fj_1_dr_H\", \"fj_1_dr_Z\", \"fj_2_dr_H\", \"fj_2_dr_Z\", 'genWeight', \"puWeight\", \"puWeightUp\", \"puWeightDown\", \"xsecWeight\", 'nPSWeight', 'PSWeight',\n",
    "                                'fj_1_nchadrons', 'fj_1_nbhadrons','fj_2_nbhadrons','fj_1_sj1_nbhadrons','fj_2_sj1_nbhadrons','fj_1_sj2_nbhadrons','fj_2_sj2_nbhadrons',\n",
    "                                'fj_2_nchadrons','fj_1_sj1_nchadrons','fj_2_sj1_nchadrons','fj_1_sj2_nchadrons','fj_2_sj2_nchadrons'])\n",
    "\n",
    "## Read into pandas DataFrame\n",
    "sample_prefix = f\"{config['samples']['sample_prefix']}_{year}\"\n",
    "_df0 = {}\n",
    "_df0['qcd-mg-noht'] = uproot.open(f\"{sample_prefix}/mc/qcd-mg_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['qcd-herwig-noht'] = uproot.open(f\"{sample_prefix}/mc/qcd-herwig_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['top-noht'] = uproot.open(f\"{sample_prefix}/mc/top_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['v-qq-noht'] = uproot.open(f\"{sample_prefix}/mc/v-qq_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['qcd-mg-bflav-noht'] = uproot.open(f\"{sample_prefix}/mc/qcd-mg-bflav_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['jetht-noht'] = uproot.open(f\"{sample_prefix}/data/jetht_tree.root\")['Events'].pandas.df(minimal_branches_for_data, flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing\n",
    "\n",
    "For data: apply OR of all HT trigger to enhance statistics.\n",
    "\n",
    "For MC: apply no HT trigger, based on the strategy we name it \"MC substitute\".\n",
    "\n",
    "The initial dataframe (`_df0`) is event-based, but for the purpose of fit we transform the dataframe to be jet-based. \n",
    "The new dataframe `df1` contains branches `fj_x_` that either come from `fj_1_` or `fj_2_` passing the corresponding jet-based creteria (pT>200, each subjet matched to >=1 SV, sfBDT>0.5) carried by `fj_?_is_qualified` (?=1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ Pre-processing for data  ===================\n",
    "\n",
    "## Baseline selection applied to data. \n",
    "## Note that we use the OR or all HT triggers (some are pre-scaled triggers)\n",
    "htcut_incl = '('+' | '.join(ext_hlt_branches[year])+')'\n",
    "basesel_noht_prep = f\"passmetfilters & {htcut_incl} & fj_x_pt>200 & fj_x_is_qualified\"\n",
    "sl_prep = ['jetht-noht']\n",
    "df1 = {}\n",
    "for sam in sl_prep:\n",
    "    assert 'noht' in sam\n",
    "    ## To concatenate event lists where either fj_1 is qualified OR fj_2 is qualified\n",
    "    fj_branches = [key.replace('fj_2', 'fj_x') for key in _df0[sam].keys() if (key.startswith('fj_2') and key!='fj_2_is_qualified')]  ## all fj_2_ branches expect fj_2_is_qualified\n",
    "    for i, i_inv in zip(['1','2'], ['2','1']):\n",
    "        df1[sam + i] = _df0[sam].query(basesel_noht_prep.replace('fj_x', f'fj_{i}'))  ## select events where fj_1/fj_2 is qualified\n",
    "        df1[sam + i].drop(columns=[key.replace('fj_x', f'fj_{i_inv}') for key in fj_branches], inplace=True)  ## drop fj branches for the other index\n",
    "        df1[sam + i].rename(columns={key.replace('fj_x', f'fj_{i}'): key for key in fj_branches}, inplace=True)  ## change branches name from fj_1/fj_2 to a unified name fj_x\n",
    "        df1[sam + i].loc[:, 'fj_idx'] = int(i)  ## label the jet index\n",
    "        df1[sam + i].loc[:, 'is_qcd'] = True if 'qcd' in sam else False\n",
    "    df1[sam] = pd.concat([df1[sam + '1'], df1[sam + '2']])\n",
    "    del df1[sam + '1'], df1[sam + '2']\n",
    "    del _df0[sam]  # to release memory usage if necessary\n",
    "\n",
    "## Produce new variables used for fit\n",
    "for sam in sl_prep:\n",
    "    df1[sam]['mSV12_ptmax'] = df1[sam].eval('(fj_x_sj1_sv1_pt>fj_x_sj2_sv1_pt)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_pt<=fj_x_sj2_sv1_pt)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_ptmax_log'] = df1[sam].eval('log(mSV12_ptmax)')\n",
    "    df1[sam]['mSV12_dxysig'] = df1[sam].eval('(fj_x_sj1_sv1_dxysig>fj_x_sj2_sv1_dxysig)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_dxysig<=fj_x_sj2_sv1_dxysig)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_dxysig_log'] = df1[sam].eval('log(mSV12_dxysig)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FOR TEST: to see data HT distributions passing different HT pre-scaled trigger\n",
    "# for hlt in ext_hlt_branches[year]:\n",
    "#     dftmp = _df0['jetht-noht'].query(hlt)\n",
    "#     h = get_hist(dftmp['ht'].values, bins=np.linspace(0, 2000, 201), weights=np.ones(dftmp.shape[0]))\n",
    "#     plot_hist(h, label=hlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FOR TEST: check the xsecWeight for MG samples & genWeight for Herwig sample (to avoid extremely large values) \n",
    "# from collections import Counter\n",
    "# print(Counter(_df0['qcd-mg-noht']['xsecWeight']),'\\n')\n",
    "# for i in [0.96, 0.98, 0.99]:\n",
    "#     print(_df0['qcd-herwig-noht']['genWeight'].quantile(q=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ Pre-processing for MC substitute  ===================\n",
    "\n",
    "## Baseline selection applied to MC.\n",
    "## No HT trigger is applied, based on the \"MC substitute\" strategy\n",
    "basesel_noht_prep_subst = \"passmetfilters & fj_x_pt>200 & fj_x_is_qualified\"\n",
    "sl_prep_subst = ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'subst_qcd-mg-bflav-noht']  ## mark sample name with \"subst_\" as a reminder of MC substitute\n",
    "for sam in sl_prep_subst:\n",
    "    assert 'noht' in sam\n",
    "    ## To concatenate event lists where fj_1 is qualified OR fj_2 is qualified. Same procedure here\n",
    "    fj_branches = [key.replace('fj_2', 'fj_x') for key in _df0[sam.replace('subst_','')].keys() if (key.startswith('fj_2') and key!='fj_2_is_qualified')]  ## all fj_2_ branches expect fj_2_is_qualified\n",
    "    for i, i_inv in zip(['1','2'], ['2','1']):\n",
    "        df1[sam + i] = _df0[sam.replace('subst_','')].query(basesel_noht_prep_subst.replace('fj_x', f'fj_{i}'))\n",
    "        df1[sam + i].drop(columns=[key.replace('fj_x', f'fj_{i_inv}') for key in fj_branches], inplace=True)\n",
    "        df1[sam + i].rename(columns={key.replace('fj_x', f'fj_{i}'): key for key in fj_branches}, inplace=True)\n",
    "        df1[sam + i].loc[:, 'fj_idx'] = int(i)\n",
    "        df1[sam + i].loc[:, 'is_qcd'] = True if 'qcd' in sam else False\n",
    "        if sam == 'subst_qcd-mg-noht':\n",
    "            df1[sam + i].query('xsecWeight<5.', inplace=True)  ## drop MG events with extremely large xsecWeight (coming from low HT sample in the HT-binned MG list)\n",
    "        if sam == 'subst_qcd-herwig-noht':\n",
    "            df1[sam + i].query('genWeight<{}'.format(_df0['qcd-herwig-noht']['genWeight'].quantile(q=0.96)), inplace=True)  ## drop Herwig events with extremely large genWeight\n",
    "        if year == 2016 and sam == 'subst_qcd-herwig-noht':\n",
    "            df1[sam + i].loc[:, 'xsecWeight'] = df1[sam + i]['xsecWeight'] * 2400.  ## fix a 2016 bug: Herwig sample xsec is mistaken\n",
    "    df1[sam] = pd.concat([df1[sam + '1'], df1[sam + '2']])\n",
    "    del df1[sam + '1'], df1[sam + '2']\n",
    "    del _df0[sam.replace('subst_','')]  # to release memory usage if necessary\n",
    "\n",
    "## Produce new variables used for fit\n",
    "for sam in sl_prep_subst:\n",
    "    df1[sam]['mSV12_ptmax'] = df1[sam].eval('(fj_x_sj1_sv1_pt>fj_x_sj2_sv1_pt)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_pt<=fj_x_sj2_sv1_pt)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_ptmax_log'] = df1[sam].eval('log(mSV12_ptmax)')\n",
    "    df1[sam]['mSV12_dxysig'] = df1[sam].eval('(fj_x_sj1_sv1_dxysig>fj_x_sj2_sv1_dxysig)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_dxysig<=fj_x_sj2_sv1_dxysig)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_dxysig_log'] = df1[sam].eval('log(mSV12_dxysig)')\n",
    "\n",
    "    ## PSWeight variables exclusive to 2018 datasets\n",
    "    if year==2018:\n",
    "        if df1[sam]['nPSWeight'].iloc[0] == 1:\n",
    "            df1[sam]['PSWeight1'] = df1[sam]['PSWeight2'] = df1[sam]['PSWeight3'] = df1[sam]['PSWeight4'] = df1[sam]['PSWeight']\n",
    "        else:\n",
    "            assert all(df1[sam]['nPSWeight'] == 4)\n",
    "            for i in range(4):\n",
    "                df1[sam][f'PSWeight{i+1}'] = df1[sam]['PSWeight'].map(lambda x: x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain reweight factors\n",
    "\n",
    "We need to extract some reweight factors as well as the BDT variation points specific to pT ranges. Step 1-3 are necessary for the nominal fit routine. Factors obtained from step 4-5 are for validation fits.\n",
    "\n",
    " 3-1. **MC substitute-to-data reweight factor**: reweight based on the 3D (HT, pT, jet index) grid. The goal is to bring the shape of MC (without pre-selection on the jet-HT triggers) back to the data shape (passing the logical OR of prescaled jet-HT triggers). Remember that the raw MC always yields much larger than data. New variables take the name `htwgt`, `htwgt_herwig`. (`htwgt_herwig` is derived using the Herwig QCD sample and is only used in the validation fit.)\n",
    "\n",
    " 3-2. **sfBDT reweight factor**: reweight on the sfBDT variable based on (pT, jet index) bins. The reweight factors `sfbdtwgt_g50` are obtained, which is only used to derive the systematics shape templates in the nominal fit. `sfbdtwgt_g50_herwig` is derived as well using the Herwig sample, used in the validation fit.\n",
    " \n",
    " 3-3. **sfBDT central point and variation range**: a set of sfBDT cut values which are specific for different pT range. The values are derived by judging the similarity of the tagger shape between the signal and proxy jet samples.\n",
    "\n",
    " 3-4. **Additional MC substitute-to-data reweight factor on $p_{T}$ only**: A possible replacement of the factors in step 1. This factor is only used in the validation fit to check if different reweighting schemes may affect the SF fit results. New variables take the name `ad_ptwgt` and `ad_ptwgt_herwig`.\n",
    " \n",
    " 3-5. **Proxy-to-signal reweight factor on $m_{SD}$ / $p_{T}$ / $\\tau_{21}$**: based on the shape of MC after applying the MC-to-data factors in step 1 and the H->cc signal jet shape. The factor is only used in the validation fit, in which we apply such reweight factor to both MC and data to check if the SF results are affected. New variables take the name `(mass|pt|tau21)datamcwgt` and `(mass|pt|tau21)datamcwgt_herwig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3-1. Reweight MC subsitute to data: stored as variable \"htwgt\", \"htwgt_herwig\") ===================\n",
    "\n",
    "## True: if the block has run before, we can obtain the reweight factor from the previously stored pickle output\n",
    "is_read_from_pickel = False\n",
    "\n",
    "def extract_source_to_target_ht_weight(df1, sl_rwgt_source, wgtstr_rwgt_source, sl_rwgt_target, wgtstr_rwgt_target, wgtname, ext_sl_rwgt_source=[], presel='', do_plot=True):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor on HT based on (pT, jet index) bins\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt_(source|target): sample list for the source/target in this reweighting routine\n",
    "        wgtstr_rwgt_(source|target): the weight string applied to the source/target to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "        ext_sl_rwgt_source: extra source sample list for which we also calculate the reweight factors after extracting them\n",
    "        presel: additonal pre-selection applied before reweigting\n",
    "        do_plot: if store plots of reweighting\n",
    "    \"\"\"\n",
    "    \n",
    "    rwgt_var = 'ht'\n",
    "    ## The binning info for (pT, HT) grid. Note that 2016 is different from 2017/18. The adopted HT grid is based on MC shape in each pT bin\n",
    "    rwgt_edge_dic = {}\n",
    "    rwgt_edge_dic[2016] = {\n",
    "        'jet1': {\n",
    "            'pt200to250': [250, 300, 350, 400, 450, 500, 550, 600, 650, 750],\n",
    "            'pt250to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 900],\n",
    "            'pt300to350': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1200],\n",
    "            'pt350to400': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1300],\n",
    "            'pt400to500': [350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1450],\n",
    "            'pt500to100000': [400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "        },\n",
    "        'jet2': {\n",
    "            'pt200to250': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 900, 1000],\n",
    "            'pt250to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200],\n",
    "            'pt300to350': [450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "            'pt350to400': [550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1600],\n",
    "            'pt400to500': [600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1700],\n",
    "            'pt500to100000': [650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "        },\n",
    "    }\n",
    "    rwgt_edge_dic[2017] = rwgt_edge_dic[2018] = {\n",
    "        'jet1': {\n",
    "            'pt200to250': [250, 300, 350, 400, 450, 500, 550, 600, 650, 750],\n",
    "            'pt250to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 900],\n",
    "            'pt300to350': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1200],\n",
    "            'pt350to400': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1300],\n",
    "            'pt400to500': [350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "            'pt500to100000': [500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "        },\n",
    "        'jet2': {\n",
    "            'pt200to250': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 900, 1000],\n",
    "            'pt250to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200],\n",
    "            'pt300to350': [450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "            'pt350to400': [550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1600],\n",
    "            'pt400to500': [700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1800],\n",
    "            'pt500to100000': [900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "        },\n",
    "    }\n",
    "    for sam in sl_rwgt_source + ext_sl_rwgt_source:\n",
    "        df1[sam][wgtname] = np.nan  ## initially fill the output column with NaN\n",
    "\n",
    "    if is_read_from_pickel: ## restore info from a previously stored pickle\n",
    "        import pickle\n",
    "        with open(f'plots/prep_pd/{wgtname}_{year}.pickle', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            ent_target, ent_source, rwgt = res['ent_target'], res['ent_source'], res['rwgt']\n",
    "    else:\n",
    "        ent_target, ent_source, rwgt = {}, {}, {}\n",
    "    \n",
    "    df_target = df1[sl_rwgt_target[0]] if len(sl_rwgt_target)==1 else pd.concat([df1[sam] for sam in sl_rwgt_target])\n",
    "    df_source = df1[sl_rwgt_source[0]] if len(sl_rwgt_source)==1 else pd.concat([df1[sam] for sam in sl_rwgt_source])\n",
    "    if presel != '':\n",
    "        df_target = df_target.query(presel)\n",
    "        df_source = df_source.query(presel)\n",
    "    ## Rewight separately on jet pT bins\n",
    "    for ptsel, ptlab in zip(['fj_x_pt>=200 & fj_x_pt<250', 'fj_x_pt>=250 & fj_x_pt<300', 'fj_x_pt>=300 & fj_x_pt<350', 'fj_x_pt>=350 & fj_x_pt<400', 'fj_x_pt>=400 & fj_x_pt<500', 'fj_x_pt>=500'], \n",
    "                            ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500to100000']):\n",
    "        ## Reweight separately for 1st or 2nd jet\n",
    "        for sel, lab in zip(['fj_idx==1', 'fj_idx==2'], ['jet1', 'jet2']):\n",
    "            print (' -- ', ptsel, sel)\n",
    "            rwgt_edge = rwgt_edge_dic[year][lab][ptlab]\n",
    "            if not is_read_from_pickel:\n",
    "                ## Calculate the rwgt for the first time\n",
    "                _df_target = df_target.query(f'{ptsel} & {sel}')\n",
    "                _df_source = df_source.query(f'{ptsel} & {sel}')\n",
    "                \n",
    "                ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "                ent_target[ptlab+lab] = get_hist(\n",
    "                    _df_target[rwgt_var].values, bins=rwgt_edge, \n",
    "                    weights=np.ones(_df_target.shape[0]) if wgtstr_rwgt_target=='1' else _df_target.eval(wgtstr_rwgt_target).values, \n",
    "                    underflow=True, overflow=True, mergeflowbin=False\n",
    "                ).view(flow=True).value\n",
    "                ent_source[ptlab+lab] = get_hist(\n",
    "                    _df_source[rwgt_var].values, bins=rwgt_edge, \n",
    "                    weights=np.ones(_df_source.shape[0]) if wgtstr_rwgt_source=='1' else _df_source.eval(wgtstr_rwgt_source).values, \n",
    "                    underflow=True, overflow=True, mergeflowbin=False\n",
    "                ).view(flow=True).value\n",
    "                ## Calculate the reweight factor\n",
    "                rwgt[ptlab+lab] = ent_target[ptlab+lab] / ent_source[ptlab+lab] # len=nbin+2\n",
    "            print(ent_target[ptlab+lab], '\\n', rwgt[ptlab+lab])\n",
    "            \n",
    "            ## assign the reweight factor to the new column\n",
    "            for sam in sl_rwgt_source + ext_sl_rwgt_source:\n",
    "                df1sel = df1[sam].eval(f'{ptsel} & {sel}')\n",
    "                df1[sam].loc[df1sel, wgtname] = df1[sam].loc[df1sel, rwgt_var].map(lambda val: rwgt[ptlab+lab][sum(np.array(rwgt_edge)<=val)] )\n",
    "    \n",
    "    ## check all entries are filled with valid factors\n",
    "    assert any([any(df1[sam][wgtname] == np.nan) for sam in sl_rwgt_source]) == False\n",
    "    \n",
    "    ## store into pickle for the first run\n",
    "    if not is_read_from_pickel: ## store the info for the first run\n",
    "        import pickle\n",
    "        if not os.path.exists('plots/prep_pd'):\n",
    "            os.makedirs('plots/prep_pd')\n",
    "        with open(f'plots/prep_pd/{wgtname}_{year}.pickle', 'wb') as fw:\n",
    "            pickle.dump({'ent_target':ent_target, 'ent_source':ent_source, 'rwgt':rwgt}, fw)\n",
    "\n",
    "    # =========== plot ===========\n",
    "    if do_plot:\n",
    "        mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "        for ptlab in ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500to100000']:\n",
    "            for lab, cm, cd in zip(['jet1', 'jet2'], ['blue', 'red'], ['royalblue', 'lightcoral']):\n",
    "                f = plt.figure(figsize=(12,12))\n",
    "                gs = mpl.gridspec.GridSpec(2, 1, height_ratios=[2, 1], hspace=0.04) \n",
    "                ax = f.add_subplot(gs[0])\n",
    "                hep.cms.label(data=True, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "                hep.histplot(ent_source[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][lab][ptlab])+[2500], label='Jet '+lab[-1]+' (MC)', color=cm)\n",
    "                hep.histplot(ent_target[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][lab][ptlab])+[2500], label='Jet '+lab[-1]+' (Data)', color=cd, linestyle='--')\n",
    "\n",
    "                ax.set_xlim(0, 2500); ax.set_xticklabels([]); \n",
    "                ax.set_yscale('log'); ax.set_ylabel('Events', ha='right', y=1.0)\n",
    "                ax.legend()\n",
    "                ax1 = f.add_subplot(gs[1]); \n",
    "                hep.histplot(rwgt[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][lab][ptlab])+[2500], label='Jet '+lab[-1], color=cm)\n",
    "                ax1.set_xlim(0, 2500); ax1.set_xlabel('$H_{T}$ [GeV]', ha='right', x=1.0);\n",
    "                ax1.legend()\n",
    "                ax1.set_yscale('log')\n",
    "                ax1.set_ylim(5e-3, 2e0); ax1.set_ylabel('Rwgt factor', ha='right', y=1.0);  ax1.set_yticks([1e-2,1e-1,1e0,1e1]);\n",
    "                ax1.plot([0, 2500], [1, 1], 'k:')\n",
    "\n",
    "                if not os.path.exists('plots/prep_pd'):\n",
    "                    os.makedirs('plots/prep_pd')\n",
    "                plt.savefig(f'plots/prep_pd/rwgtfac_{wgtname}_{year}_{ptlab}_{lab}.pdf')\n",
    "                plt.savefig(f'plots/prep_pd/rwgtfac_{wgtname}_{year}_{ptlab}_{lab}.png')\n",
    "    # ============================\n",
    "    \n",
    "    return {'ent_target':ent_target, 'ent_source':ent_source, 'rwgt':rwgt}\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "htwgt = extract_source_to_target_ht_weight(\n",
    "    df1, sl_rwgt_source=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], ext_sl_rwgt_source=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt_source=f\"{lumi[year]}*genWeight*xsecWeight*puWeight\",\n",
    "    sl_rwgt_target=['jetht-noht'], wgtstr_rwgt_target='1', wgtname='htwgt',\n",
    ")\n",
    "htwgt_herwig = extract_source_to_target_ht_weight(\n",
    "    df1, sl_rwgt_source=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], ext_sl_rwgt_source=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt_source=f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", \n",
    "    sl_rwgt_target=['jetht-noht'], wgtstr_rwgt_target='1', wgtname='htwgt_herwig',\n",
    ")\n",
    "\n",
    "## Calculate bflav factors: reweight bflav sample to inclusive QCD (after b selection cut)\n",
    "bflav_htwgt = extract_source_to_target_ht_weight(\n",
    "    df1, sl_rwgt_source=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt_source=f\"{lumi[year]}*genWeight*xsecWeight*puWeight\",\n",
    "    sl_rwgt_target=['subst_qcd-mg-noht'], wgtstr_rwgt_target=f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='bflav_htwgt',\n",
    "    presel='fj_x_nbhadrons>=1', do_plot=False,\n",
    ")\n",
    "for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht']:\n",
    "    df1[sam]['bflav_htwgt'] = 1.\n",
    "\n",
    "df1['subst_qcd-mg-noht'][['ht', 'fj_x_pt', 'fj_idx', 'htwgt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3-2. Extract the sfBDT>0.5 binned fractor: stored as variable \"sfbdtwgt_g50\"; similar for herwig ===================\n",
    "\n",
    "def extract_further_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname, rwgt_info, sl_ext_rwgt=[], presel='fj_x_sfBDT>=0.5'):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor (both overall and binned factor) further on sfBDT variable, after a sfBDT>0.9 selection\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name (the binned factors) stored as a new column\n",
    "        rwgt_info: info of the reweight variable, in the format of (var, nbin, xmin, xmax) or (var, edges list, None, None)\n",
    "        sl_ext_rwgt: extra sample list for which we also calculate the reweight factors after extracting them\n",
    "        presel: pre-selection before reweighting\n",
    "    \"\"\"\n",
    "    \n",
    "    for sam in sl_rwgt:\n",
    "        df1[sam][wgtname] = np.nan  ## initially fill the output column with NaN\n",
    "\n",
    "    ## Reweight based on given variable\n",
    "    rwgt_var, nbin, xmin, xmax = rwgt_info\n",
    "    if not isinstance(nbin, int):\n",
    "        rwgt_edge, xmin, xmax, nbin = nbin, min(nbin), max(nbin), len(nbin)\n",
    "    else:\n",
    "        rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    print('rwgt info: ', rwgt_var, rwgt_edge)\n",
    "    \n",
    "    ## Rewight separately on jet pT bins\n",
    "    ent_data, ent_mc, rwgt = {}, {}, {}\n",
    "    for pt_range in config['pt_range']['range']:\n",
    "        pt_range = tuple(pt_range)\n",
    "        rwgt_presel = f'fj_x_pt>={pt_range[0]} & fj_x_pt<{pt_range[1]}'\n",
    "        rwgt_sel = f'{presel} & {rwgt_presel}'; print(rwgt_sel)\n",
    "        _dffdata = df1['jetht-noht'].query(rwgt_sel)\n",
    "        _dffmc =  pd.concat([df1[sam].query(rwgt_sel) for sam in sl_rwgt])\n",
    "        \n",
    "        ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "        ent_data[pt_range] = get_hist(_dffdata[rwgt_var].values, bins=rwgt_edge, weights=np.ones(_dffdata.shape[0]), underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ent_mc[pt_range]  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc.eval(wgtstr_rwgt).values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ## Calculate the reweight factor\n",
    "        rwgt[pt_range] = np.nan_to_num(ent_data[pt_range] / ent_mc[pt_range], nan=0) # len=nbin+2\n",
    "        ## assign the reweight factor to the new column\n",
    "        for sam in sl_rwgt + sl_ext_rwgt:\n",
    "            df1[sam].loc[df1[sam].eval(rwgt_presel), wgtname] = df1[sam].query(rwgt_presel)[rwgt_var].map(lambda val: rwgt[pt_range][sum(np.array(rwgt_edge)<=val)] )\n",
    "        print (ent_data[pt_range], rwgt[pt_range])\n",
    "    \n",
    "    ## Store reweight factors\n",
    "    import pickle\n",
    "    if not os.path.exists('plots/prep_pd'):\n",
    "        os.makedirs('plots/prep_pd')\n",
    "    with open(f'plots/prep_pd/{wgtname}_{year}.pickle', 'wb') as fw:\n",
    "        pickle.dump({'ent_data':ent_data, 'ent_mc':ent_mc, 'rwgt':rwgt}, fw)\n",
    "    \n",
    "    return {'ent_data':ent_data, 'ent_mc':ent_mc, 'rwgt':rwgt}\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_further_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt\",\n",
    "                             wgtname='sfbdtwgt_g50', rwgt_info=('fj_x_sfBDT', 25, 0.5, 1.))\n",
    "extract_further_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig\",\n",
    "                             wgtname='sfbdtwgt_g50_herwig', rwgt_info=('fj_x_sfBDT', 25, 0.5, 1.))\n",
    "\n",
    "df1['subst_qcd-mg-noht'][['fj_x_pt', 'fj_idx', 'fj_x_sfBDT', 'sfbdtwgt_g50']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3-3. Determine the optimal sfBDT cut value for each pT range  ===================\n",
    "\n",
    "# First load the h->cc signal ntuple. Adopt the selction used in the analysis\n",
    "if 'df_comp' not in globals():  \n",
    "    import re\n",
    "    _df0['vhcc-2L'] = uproot.open(f\"{re.search('^.+/trees', sample_prefix)[0]}/20210117_VH_extjetvar_{year}_2L/mc/vhcc_tree.root\")['Events'].pandas.df()\n",
    "\n",
    "    boosted = \"v_pt>200 & ak15_pt>200 & dphi_V_ak15>2.5 & ak15_sdmass>50 & ak15_sdmass<200\"\n",
    "    basecut_vhcc_2L = \"v_mass>75 & v_mass<105 & ((abs(lep1_pdgId)==11 & passTrigEl) | (abs(lep1_pdgId)==13 & passTrigMu)) & \" + boosted + \" & n_ak4<3\"\n",
    "    df_comp = {}\n",
    "    df_comp['vhcc-2L'] = _df0['vhcc-2L'].query(basecut_vhcc_2L)\n",
    "\n",
    "def extract_bdt_sequence(df1, sl_pxy, wgtstr, pxy_base_sel):\n",
    "    r\"\"\"Extract the sfBDT sequence for specified pT range, based on the signal/proxy similarity\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_pxy: proxy sample list\n",
    "        wgtstr: the weight string applied to proxy samples\n",
    "        pxy_base_sel: base selections to proxy for deriving the sfBDT sequence. Impose 'c' category requirement using fj_x_nb(c)hadrons\n",
    "    \"\"\"\n",
    "\n",
    "    ## Edges based on the tagger WPs\n",
    "    edges = [0.] + sorted([rg[0] for rg in config['tagger']['working_points']['range'].values()]) + [1.]\n",
    "    rat_pxy = {}\n",
    "    bdt_seq = {}\n",
    "\n",
    "    ## Extract the optimal sfBDT and variation cut values for each pT range\n",
    "    _dffpxy_base = pd.concat([df1[sam].query(pxy_base_sel) for sam in sl_pxy]) # fj_x_nb(c)hadrons correspond to the defination of 'c' category\n",
    "                              \n",
    "    for ptmin, ptmax in config['pt_range']['range']:\n",
    "        print('pt range: ', ptmin, ptmax)\n",
    "\n",
    "        ## Calculate the proportion of LP+MP+HP over inclusive tagger score for \"signal jets\"\n",
    "        _dffhcc = df_comp['vhcc-2L'].query(f'ak15_pt>{ptmin} & ak15_pt<{ptmax}')\n",
    "        h = get_hist(_dffhcc['ak15_ParticleNetMD_HccVsQCD'].values, bins=edges, weights=_dffhcc.eval('genWeight*xsecWeight*puWeight').values)\n",
    "        rat_hcc = np.array([h.view().value[0], h.view().value[1], h.view().value[2], h.view().value[3], sum(h.view().value[1:])]) / sum(h.view().value) ## <LP, LP, MP, TP, LP+MP+TP\n",
    "\n",
    "        ## Calculate the proportion for \"proxy jets\" as sfBDT floats\n",
    "        _dffpxy = _dffpxy_base.query(f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax}')\n",
    "        ratios = [[], [], [], [], []]\n",
    "        bdt_range = np.arange(0.7, 0.996, 0.002)\n",
    "        for bdt in bdt_range:  # loop oversf BDT grid\n",
    "            _dffpxy = _dffpxy.query(f'fj_x_sfBDT>{bdt}')\n",
    "            h = get_hist(_dffpxy['fj_x_ParticleNetMD_XccVsQCD'].values, bins=edges, weights=_dffpxy.eval(wgtstr).values)\n",
    "            rat = np.array([h.view().value[0], h.view().value[1], h.view().value[2], h.view().value[3], sum(h.view().value[1:])]) / sum(h.view().value) ## <LP, LP, MP, TP, LP+MP+TP\n",
    "    #         print(rat)\n",
    "            rat_pxy[((ptmin,ptmax), np.round(bdt,3))] = rat\n",
    "            for j in range(5):\n",
    "                ratios[j].append(rat[j])\n",
    "\n",
    "        ## Get sfBDT cut WP\n",
    "        from scipy.interpolate import interp1d\n",
    "        bdt_wp = interp1d(ratios[4], bdt_range)(rat_hcc[4]) # chosen BDT WP: proxy proportion under LP+MP+TP reaches signal\n",
    "        bdt_wp_hi = interp1d(ratios[3], bdt_range)(rat_hcc[3]) # chosen BDT WP (for 4/5's upper bound): proxy proportion under TP reaches signal\n",
    "        rat_wp, rat_wp_hi = rat_hcc[4], interp1d(bdt_range, ratios[4])(bdt_wp_hi) # corresponding LP+MP+TP proportion\n",
    "        step = (rat_wp_hi - rat_wp) / 4\n",
    "        rat_seq = np.linspace(rat_wp-step*5, rat_wp+step*5, 11) # derive an arithmetic sequence\n",
    "        bdt_seq[(ptmin,ptmax)] = interp1d(ratios[4], bdt_range, fill_value=\"extrapolate\")(rat_seq)\n",
    "        print('BDT seq: ', bdt_seq[(ptmin,ptmax)])\n",
    "\n",
    "    df1[f\"bdt_seq_{config['pt_range']['name']}\"] = bdt_seq\n",
    "    df1[f\"rat_pxy_{config['pt_range']['name']}\"] = rat_pxy\n",
    "\n",
    "    import pickle\n",
    "    if not os.path.exists('plots/prep_pd'):\n",
    "        os.makedirs('plots/prep_pd')\n",
    "    with open(f'plots/prep_pd/bdt_seq_{year}.pickle', 'wb') as fw:\n",
    "        pickle.dump({k:df1[k] for k in df1 if k.startswith('bdt_seq') or k.startswith('rat_pxy')}, fw)\n",
    "\n",
    "extract_bdt_sequence(df1, sl_pxy = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'],\n",
    "                     wgtstr='genWeight*xsecWeight*puWeight*htwgt',\n",
    "                     pxy_base_sel='(fj_x_nbhadrons==0) & (fj_x_nchadrons>=1) & (fj_x_sfBDT>0.7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3-4. [additional] Reweight MC subsitute to data on pT: stored as variable \"ad_ptwgt\", \"ad_ptwgt_herwig\" ===================\n",
    "\n",
    "def extract_mc_to_data_pt_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname, sl_ext_rwgt=[]):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor on pT as a optional choice\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "        sl_ext_rwgt: extra sample list for which we also calculate the reweight factors after extracting them\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply simple 1D reweight to pT\n",
    "    rwgt_var, nbin, xmin, xmax  = 'fj_x_pt', 20, 200., 1200.\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Rewight separately on 1st/2nd jet\n",
    "    for sel, lab in zip(['fj_idx==1', 'fj_idx==2'], ['jet1', 'jet2']):\n",
    "        # Previously this extra factor is extracted with a presel of sfBDT>0.9. Now given that the sfBDT is optimized by pT range thus not fixed, we relax this cut\n",
    "        _dffdata = df1['jetht-noht'].query(sel)\n",
    "        _dffmc =  pd.concat([df1[sam].query(sel) for sam in sl_rwgt])\n",
    "        \n",
    "        ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "        ent_data = get_hist(_dffdata[rwgt_var].values, bins=rwgt_edge, weights=np.ones(_dffdata.shape[0]), underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ent_mc  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc.eval(wgtstr_rwgt).values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ## Calculate the reweight factor\n",
    "        rwgt = ent_data / ent_mc # len=nbin+2\n",
    "        \n",
    "        ## assign the reweight factor to the new column\n",
    "        for sam in sl_rwgt + sl_ext_rwgt:\n",
    "            df1sel = df1[sam].eval(sel)\n",
    "            df1[sam].loc[df1sel, wgtname] = df1[sam].loc[df1sel, rwgt_var].map(lambda val: rwgt[int(max(0, min(nbin+1, np.floor((val-1.*xmin)/(1.*xmax-xmin)*nbin) +1 )))] )\n",
    "        print (ent_data, rwgt)\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_mc_to_data_pt_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight\",        wgtname='ad_ptwgt')\n",
    "extract_mc_to_data_pt_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='ad_ptwgt_herwig')\n",
    "\n",
    "df1['subst_qcd-mg-noht'][['ht', 'fj_x_pt', 'fj_idx', 'htwgt', 'sfbdtwgt_g50', 'ad_ptwgt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3-5. [additional] Reweight MC (proxy jet) to H->cc signal jet on either mass/pT/tau21: stored as variable \"(mass|pt|tau21)datamcwgt\"; similar for herwig  ===================\n",
    "\n",
    "# First load the h->cc signal ntuple. Adopt the selction used in the analysis\n",
    "if 'df_comp' not in globals():  \n",
    "    import re\n",
    "    _df0['vhcc-2L'] = uproot.open(f\"{re.search('^.+/trees', sample_prefix)[0]}/20210117_VH_extjetvar_{year}_2L/mc/vhcc_tree.root\")['Events'].pandas.df()\n",
    "\n",
    "    boosted = \"v_pt>200 & ak15_pt>200 & dphi_V_ak15>2.5 & ak15_sdmass>50 & ak15_sdmass<200\"\n",
    "    basecut_vhcc_2L = \"v_mass>75 & v_mass<105 & ((abs(lep1_pdgId)==11 & passTrigEl) | (abs(lep1_pdgId)==13 & passTrigMu)) & \" + boosted + \" & n_ak4<3\"\n",
    "    df_comp = {}\n",
    "    df_comp['vhcc-2L'] = _df0['vhcc-2L'].query(basecut_vhcc_2L)\n",
    "\n",
    "def extract_mc_to_signal_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname, rwgt_info, sl_ext_rwgt=[]):\n",
    "    r\"\"\"Extract the \"MC subsisute (proxy) to H->cc signal jet\" reweight factor on possible variable\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "        rwgt_info: variable and binning info for this reweighting routine\n",
    "        sl_ext_rwgt: extra sample list for which we also calculate the reweight factors after extracting them\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reweight info extracted from the function argument\n",
    "    rwgt_var, nbin, xmin, xmax, rwgt_var_nom  = rwgt_info\n",
    "    print('rwgt info: ', rwgt_var, nbin, xmin, xmax)\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Requires the selection sfBDT>0.9 which is (averagely) used in the fit region\n",
    "    rwgt_sel = 'fj_x_sfBDT>0.9'\n",
    "    \n",
    "    ## Get MC and h->cc signal histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "    _dffmc =  pd.concat([df1[sam].query(rwgt_sel) for sam in sl_rwgt])\n",
    "    _dffmc_wgt = _dffmc.eval(wgtstr_rwgt)\n",
    "    ent_mc  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc_wgt.values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "    yield_mc = _dffmc_wgt.sum()\n",
    "    _dffhcc_wgt = df_comp['vhcc-2L'].eval('genWeight*xsecWeight*puWeight')\n",
    "    ent_hcc  = get_hist(df_comp['vhcc-2L'][rwgt_var_nom].values, bins=rwgt_edge, weights=_dffhcc_wgt.values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "    yield_hcc = _dffhcc_wgt.sum()\n",
    "    \n",
    "    ## Calculate the reweight factor, and clip to (0, 50)\n",
    "    rwgt = (ent_hcc/yield_hcc) / (ent_mc/yield_mc) # len=nbin+2\n",
    "    rwgt = np.clip(rwgt, 0, 50)\n",
    "    \n",
    "    ## assign the reweight factor to the new column (to both MC and data)\n",
    "    for sam in sl_rwgt + sl_ext_rwgt + ['jetht-noht']:\n",
    "        df1[sam][wgtname] = df1[sam][rwgt_var].map(lambda val: rwgt[int(max(0, min(nbin+1, np.floor((val-1.*xmin)/(1.*xmax-xmin)*nbin) +1 )))] )\n",
    "    print (ent_hcc, rwgt)\n",
    "\n",
    "## For each reweight variable, calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt\",\n",
    "                            wgtname='massdatamcwgt', rwgt_info=('fj_x_sdmass', 15, 50, 200, 'ak15_sdmass'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig\",\n",
    "                            wgtname='massdatamcwgt_herwig', rwgt_info=('fj_x_sdmass', 15, 50, 200, 'ak15_sdmass'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt\",\n",
    "                            wgtname='ptdatamcwgt', rwgt_info=('fj_x_pt', 20, 200, 1200, 'ak15_pt'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig\",\n",
    "                            wgtname='ptdatamcwgt_herwig', rwgt_info=('fj_x_pt', 20, 200, 1200, 'ak15_pt'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt\",\n",
    "                            wgtname='tau21datamcwgt', rwgt_info=('fj_x_tau21', 20, 0, 1, 'ak15_tau21'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], sl_ext_rwgt=['subst_qcd-mg-bflav-noht'], wgtstr_rwgt=f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig\",\n",
    "                            wgtname='tau21datamcwgt_herwig', rwgt_info=('fj_x_tau21', 20, 0, 1, 'ak15_tau21'))\n",
    "\n",
    "df1['jetht-noht'][['fj_x_sdmass', 'massdatamcwgt', 'fj_x_pt', 'ptdatamcwgt', 'fj_x_tau21', 'tau21datamcwgt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Read from pre-stored dataframe\n",
    "# year=2018\n",
    "# lumi = {2016: 35.92, 2017: 41.53, 2018: 59.74}\n",
    "\n",
    "# import pickle\n",
    "# with open(f'bak-pnV02-bdtoptimV3-{year}.pickle', 'rb') as f:\n",
    "#     df1 = pickle.load(f)\n",
    "# sample_prefix = f'/home/pku/licq/hcc/samples/trees_sf/hqu/20210102_pnV02_ak15_qcd_{year}'\n",
    "# _df0 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make ROOT templates\n",
    "\n",
    "We produce the ROOT templates using the DataFrame in this step. The outputs are ROOT files with neat structure. After the further reorganization, they can be used as the Higgs Combine input to implement the fit.\n",
    "\n",
    "As a reference, we provide an example of the output files and their structure. \n",
    "E.g., for a **given fit variable**, **given tagger WP** and a **certain jet-pT bin** for **a single fit**, the output ROOT templates should include the pass and fail MC template in the B/C/L flavors, the data template, and the MC systematics for all specified shape uncertainties. The files are organized in the following structure:\n",
    "```\n",
    " 20210315_SF2018_AK15_qcd_ak_pnV02_HP_msv12_dxysig_log_var22binsv2  [use variable: msv12_dxysig_log, Tight WP]\n",
    "     Cards\n",
    "         pt250to350   [given pT bin]\n",
    "             bdt719   [the sfBDT cut points]\n",
    "              nominal                    [the nominal histograms]\n",
    "               inputs_fail.root           [include four TH1D: flvC, flvB, flvL, data_obs]\n",
    "               inputs_pass.root           [..]\n",
    "              fracBBDown                 [shape uncertainty plots]\n",
    "               inputs_fail.root           [include three TH1D: flvC_fracBBDown, flvB_fracBBDown, flvL_fracBBDown]\n",
    "               inputs_pass.root           [..]\n",
    "              fracBBUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              fracCCDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              fracCCUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              fracLightDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              fracLightUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              fitVarRwgtDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              fitVarRwgtUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              psWeightFsrDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              psWeightFsrUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              psWeightIsrDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              psWeightIsrUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              puDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              puUp\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              sfBDTRwgtDown\n",
    "               inputs_fail.root\n",
    "               inputs_pass.root\n",
    "              sfBDTRwgtUp\n",
    "                  inputs_fail.root\n",
    "                  inputs_pass.root\n",
    "             bdt752\n",
    "                 nominal\n",
    "                  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template making is organized in three nested functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### =========================================================================== Global parameters =========================================================================== ####\n",
    "g_make_template_mode = 'main'\n",
    "r\"\"\"Options:\n",
    "        main           : the main fit\n",
    "        val_pt         : the validation fit -- to use an optional MC subsitute-to-data strategy, i.e. on pT variable only\n",
    "        val_tosig_mass : the validation fit -- additionally reweight MC & data to h->cc signal jet on mass\n",
    "        val_tosig_pt   : the validation fit -- additionally reweight MC & data to h->cc signal jet on pt  \n",
    "        val_tosig_tau21: the validation fit -- additionally reweight MC & data to h->cc signal jet on tau21\n",
    "        val_crop_bin   : the validation fit -- cropping the marginal bins for fit\n",
    "\"\"\"\n",
    "\n",
    "g_outdir_prefix = f'20210315_SF{year}_AK15_qcd'\n",
    "r\"\"\"Prefix for the output dir name \"\"\"\n",
    "\n",
    "g_make_unce_types = {'nominal':True, 'pu':True, 'fracBB':True, 'fracCC':True, 'fracLight':True, 'psWeightIsr':True, 'psWeightFsr':True, 'sfBDTRwgt':True, 'fitVarRwgt':True}\n",
    "r\"\"\"The uncertainty types used in the fit. Use False or remove the key to disable an certain unce type\n",
    "    Note: \"qcdSyst\" and \"qcdKdeSyst\" is not used in this verision. \"psWeightIsr\" and \"psWeightFsr\" works fine in 2018 while in 2016/17 one need to first garantee the 2018 histograms exist\n",
    "          so the unce can be transferred.\n",
    "\"\"\"\n",
    "\n",
    "g_do_fit_for_var = [1, 2, 3]\n",
    "r\"\"\" Do fit for which variable\"\"\"\n",
    "\n",
    "g_mode_bdt_runlist = 'all'\n",
    "r\"\"\"Mode of BDT list for the run. Set 'all' for all 11 BDT values, or 'central' for the central BDT value only\"\"\"\n",
    "\n",
    "g_pt_range = config['pt_range']['range']\n",
    "r\"\"\"pT range for define separate fit points\"\"\"\n",
    "\n",
    "g_tagger_range = config['tagger']['working_points']['range']\n",
    "g_tagger_var = config['tagger']['var']\n",
    "r\"\"\"Trigger info\"\"\"\n",
    "\n",
    "g_use_bflav = True\n",
    "r\"\"\"Use additional B flavor MC samples to improve the statistics for the 'b' catogory\"\"\"\n",
    "\n",
    "g_mode_psWeight_run_templ = None\n",
    "r\"\"\"Set None for the normal run. If set to 2016 or 2017, produce the 2018 templates for psWeightIsr/Fsr unce that can be migarated to 2016/2017 conditions. sfBDT cut value set under the 2016/2017 condition.\"\"\"\n",
    "\n",
    "g_dryrun = False\n",
    "r\"\"\"Launch a test process only without writing the ROOT template files\"\"\"\n",
    "\n",
    "#### ===================================================================================================================================================================================== ####\n",
    "\n",
    "## Fit info: in the format of [ (fit var, nbins/edges, xmin/None, xmax/None, (underflow, overflow), label), outputdir lambda func ]\n",
    "g_fitinfo = {\n",
    "    1: [ ##  main fit var\n",
    "        ('mSV12_dxysig_log', [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, (True, True), 'mSV12_dxysig_log'), \n",
    "        lambda prefix, wp, bdt, pt_range, sys_name: f'results/{prefix}_{wp}_msv12_dxysig_log_var22binsv2/Cards/pt{pt_range[0]}to{pt_range[1]}/bdt{int(bdt*1000)}/{sys_name}/'\n",
    "    ],\n",
    "    2: [ ## the other var for validation\n",
    "        ('mSV12_ptmax_log', [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, (True, True), 'mSV12_ptmax_log'), \n",
    "        lambda prefix, wp, bdt, pt_range, sys_name: f'results/{prefix}_{wp}_msv12_ptmax_log_var22binsv2/Cards/pt{pt_range[0]}to{pt_range[1]}/bdt{int(bdt*1000)}/{sys_name}/'\n",
    "    ],\n",
    "    3: [ ## the other var for validation\n",
    "        ('fj_x_btagcsvv2', [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, (True, True), 'CSVv2'), \n",
    "        lambda prefix, wp, bdt, pt_range, sys_name: f'results/{prefix}_{wp}_csvv2_var22binsv2/Cards/pt{pt_range[0]}to{pt_range[1]}/bdt{int(bdt*1000)}/{sys_name}/'\n",
    "    ],\n",
    "    901: [ ## crop the marginal bins for the main var as a validation\n",
    "        ('mSV12_dxysig_log', [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8], None, None, (False, False), 'mSV12_dxysig_log'), \n",
    "        lambda prefix, wp, bdt, pt_range, sys_name: f'results/{prefix}_{wp}_msv12_dxysig_log_var22binsv2/Cards/pt{pt_range[0]}to{pt_range[1]}/bdt{int(bdt*1000)}/{sys_name}/'\n",
    "    ],\n",
    "}\n",
    "\n",
    "## Necessary KDE parameters used in qcdKdeSyst unce\n",
    "g_custom_kde_bw = {'fj_x_btagcsvv2':15, 'mSV12_ptmax_log':4, 'mSV12_dxysig_log':4}\n",
    "g_custom_kde_binmask = {'fj_x_btagcsvv2':[0], 'mSV12_ptmax_log':[-0.4,1.8,2.5,3.2], 'mSV12_dxysig_log':[-0.8,-0.4,1.8,2.5]}\n",
    "\n",
    "## Some other global vars\n",
    "g_do_sfBDT_points = None\n",
    "g_outdir_prefix_used = None\n",
    "g_hist_qcdsyst = {}\n",
    "g_wgtstr_dm_sys_fac = {}\n",
    "g_hist_fitvar_rwgt = {}\n",
    "\n",
    "def check_consistency(): ## Consistency check for gloal params\n",
    "    assert g_make_template_mode in ['main', 'val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21', 'val_vary_sfbdt', 'val_crop_bin'], \\\n",
    "        'Specified mode cannot be recognized.'\n",
    "    \n",
    "    global g_do_fit_for_var\n",
    "    if g_make_template_mode in ['val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21'] and g_do_fit_for_var != [1]:\n",
    "        print('Warning: for validation fit, set the fit information to the main variable (1) only')\n",
    "        g_do_fit_for_var = [1]\n",
    "    if g_make_template_mode == 'val_crop_bin' and g_do_fit_for_var.keys() != [901]:\n",
    "        print('Warning: for validation fit on cropping the marginal bins, set the fit information to the cropped main variable (901) only')\n",
    "        g_do_fit_for_var = [901]\n",
    "    \n",
    "    global g_mode_bdt_runlist\n",
    "    if g_make_template_mode.startswith('val_') and g_mode_bdt_runlist != 'central':\n",
    "        print('Warning: for validation fit, set the BDT run list to central')\n",
    "        g_mode_bdt_runlist = 'central'\n",
    "    \n",
    "    global g_do_sfBDT_points\n",
    "    if g_mode_bdt_runlist == 'all':\n",
    "        g_do_sfBDT_points = df1[f\"bdt_seq_{config['pt_range']['name']}\"]\n",
    "    elif g_mode_bdt_runlist == 'central':\n",
    "        _points = df1[f\"bdt_seq_{config['pt_range']['name']}\"]\n",
    "        g_do_sfBDT_points = {k:[_points[k][int((len(_points[k])-1)/2)]] for k in _points}\n",
    "    else:\n",
    "        raise RuntimeError('Specified mode for BDT runlist cannot be recognized.')\n",
    "    \n",
    "    global g_outdir_prefix_used\n",
    "    g_outdir_prefix_used = g_outdir_prefix + '_' + config['tagger']['working_points']['name']\n",
    "    if g_make_template_mode.startswith('val_'):\n",
    "        g_outdir_prefix_used += '_-' + g_make_template_mode + '-'\n",
    "    \n",
    "    if g_mode_psWeight_run_templ is not None:\n",
    "        assert year==2018, 'g_mode_psWeight_run_templ only set for year 2016/2017'\n",
    "        assert int(g_mode_psWeight_run_templ) in [2016, 2017], 'g_mode_psWeight_run_templ can only be 2016 or 2017'\n",
    "        import pickle\n",
    "        with open(f'plots/prep/bdt_seq_{g_mode_psWeight_run_templ}.pickle', 'rb') as f:\n",
    "            g_do_sfBDT_points = pickle.load(f)[f\"bdt_seq_{config['pt_range']['name']}\"]\n",
    "            g_outdir_prefix_used += f\"_psWeight{g_mode_psWeight_run_templ}\"\n",
    "            g_make_unce_types = {'nominal':True, 'psWeightIsr':True, 'psWeightFsr':True}\n",
    "\n",
    "def launch_maker():\n",
    "    r\"\"\"Depth 0: Main function to launch the fit given the global parameters\n",
    "    \"\"\"\n",
    "    check_consistency()\n",
    "    \n",
    "    print('Launch variablel list:', g_do_fit_for_var)\n",
    "    for _ifit in g_do_fit_for_var:\n",
    "        for _wp in g_tagger_range:\n",
    "            \n",
    "            ## Get fit info and output lambda func\n",
    "            fitinfo, outdir_func = g_fitinfo[_ifit]\n",
    "\n",
    "            ## The default args in the main fit\n",
    "            args = {\n",
    "                'wgtstr_dm': f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt', 'wgtstr_dm_data': None,\n",
    "                'sl_dm': ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'],\n",
    "                'sl_dm_herwig': ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'],\n",
    "                'config_dm': {\n",
    "                    'data':  '',\n",
    "                    'flvB':  'fj_x_nbhadrons>=1',\n",
    "                    'flvC':  'fj_x_nbhadrons==0 & fj_x_nchadrons>=1',\n",
    "                    'flvL':  'fj_x_nbhadrons==0 & fj_x_nchadrons==0',\n",
    "                },\n",
    "                'categories_dm': ['flvL', 'flvB', 'flvC', 'data'],\n",
    "                'catMap': {\n",
    "                    'pass': f'{g_tagger_var}>{g_tagger_range[_wp][0]:.3f} & {g_tagger_var}<={g_tagger_range[_wp][1]:.3f}',\n",
    "                    'fail': f'{g_tagger_var}<={g_tagger_range[_wp][0]:.3f} | {g_tagger_var}>{g_tagger_range[_wp][1]:.3f}',\n",
    "                },\n",
    "                'use_bflav': g_use_bflav, 'args_bflav': {\n",
    "                    'sl_dm_bflav': ['subst_qcd-mg-bflav-noht'], 'sl_dm_bflav_orig': ['subst_qcd-mg-noht'],\n",
    "                    'wgtstropt_bflav': lambda s: s.replace('fj_x_htwgt', '(fj_x_htwgt*fj_x_bflav_htwgt)'),\n",
    "                },\n",
    "            }\n",
    "            ## Modify args according to specified global param\n",
    "            if g_make_template_mode == 'val_pt':\n",
    "                args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*ad_ptwgt', None\n",
    "            elif g_make_template_mode == 'val_tosig_mass':\n",
    "                args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*massdatamcwgt', 'massdatamcwgt'\n",
    "            elif g_make_template_mode == 'val_tosig_pt':\n",
    "                args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*ptdatamcwgt', 'ptdatamcwgt'\n",
    "            elif g_make_template_mode == 'val_tosig_tau21':\n",
    "                args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*tau21datamcwgt', 'tau21datamcwgt'\n",
    "\n",
    "            wrapperPt(df1, fitinfo, lambda bdt, pt_range, sys_name: outdir_func(g_outdir_prefix_used, _wp, bdt, pt_range, sys_name), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapperPt(df2, fitinfo, outdir_func, args):\n",
    "    r\"\"\"Depth 1: Process the pT cut and wrap all other following steps\n",
    "    \"\"\"\n",
    "    print('Launch pT range:', g_pt_range)\n",
    "    for pt_range in g_pt_range:\n",
    "        pt_range = tuple(pt_range)\n",
    "        print ('pt range:', pt_range)\n",
    "        \n",
    "        ## df2->df2a: apply the pT cut (to speed up)\n",
    "        df2a = {}\n",
    "        for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'] + (['subst_qcd-mg-bflav-noht'] if args['use_bflav'] else []):\n",
    "            df2a[sam] = df2[sam].query(f'fj_x_pt>={pt_range[0]} & fj_x_pt<{pt_range[1]}')\n",
    "        \n",
    "        sfBDT_list = g_do_sfBDT_points[pt_range]\n",
    "        if isinstance(sfBDT_list, dict):\n",
    "            sfBDT_list = sfBDT_list.values()\n",
    "        for sfBDT_val in sfBDT_list:\n",
    "            print(' sfBDT cut at:', sfBDT_val)\n",
    "            \n",
    "            ## df2a->df3: apply the corresponding bdt cut\n",
    "            df3 = {}\n",
    "            for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'] + (['subst_qcd-mg-bflav-noht'] if args['use_bflav'] else []):\n",
    "                df3[sam] = df2a[sam].query(f'fj_x_pt>={pt_range[0]} & fj_x_pt<{pt_range[1]} & fj_x_sfBDT>{sfBDT_val}')\n",
    "\n",
    "            makeTemplatesWrapper(df3, fitinfo, lambda sys_name: outdir_func(sfBDT_val, pt_range, sys_name), sfBDT_val, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTemplatesWrapper(df3, fitinfo, outdir_func, sfBDT_val, args):\n",
    "    r\"\"\"Depth 2: Specify which template (nominal or any shape uncertainty) to make in this step\n",
    "    \"\"\"\n",
    "    global g_wgtstr_dm_sys_fac, g_hist_qcdsyst, g_hist_fitvar_rwgt\n",
    "    g_wgtstr_dm_sys_fac, g_hist_qcdsyst = {}, {} ## clear\n",
    "    g_hist_fitvar_rwgt = {}\n",
    "    \n",
    "    wgtstr_dm = args['wgtstr_dm']\n",
    "    if 'nominal' in g_make_unce_types.keys() and g_make_unce_types['nominal']:\n",
    "        sys_name = 'nominal'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    \n",
    "    ## Below we extract hists for all unce type\n",
    "    if 'pu' in g_make_unce_types.keys() and g_make_unce_types['pu']: \n",
    "        sys_name = 'puUp'; wgtstr_dm_sys = wgtstr_dm.replace('puWeight','puWeightUp'); makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = 'puDown'; wgtstr_dm_sys = wgtstr_dm.replace('puWeight','puWeightDown'); makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "\n",
    "    if 'fracBB' in g_make_unce_types.keys() and g_make_unce_types['fracBB']: \n",
    "        sys_name = \"fracBBUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons>1) + 1.0*(fj_x_nbhadrons<=1))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"fracBBDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons>1) + 1.0*(fj_x_nbhadrons<=1))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    if 'fracCC' in g_make_unce_types.keys() and g_make_unce_types['fracCC']: \n",
    "        sys_name = \"fracCCUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons==0 & fj_x_nchadrons>1) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons>1)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"fracCCDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons==0 & fj_x_nchadrons>1) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons>1)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    if 'fracLight' in g_make_unce_types.keys() and g_make_unce_types['fracLight']: \n",
    "        sys_name = \"fracLightUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons==0 & fj_x_nchadrons==0) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons==0)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"fracLightDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons==0 & fj_x_nchadrons==0) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons==0)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "\n",
    "    ## Below unce is not as easily extracted as above by specifying a different weight string. They may need *special treatment* implemented in the depth-3 function\n",
    "    if 'qcdSyst' in g_make_unce_types.keys() and g_make_unce_types['qcdSyst']: \n",
    "        sys_name = \"qcdSystUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"qcdSystDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    if 'qcdKdeSyst' in g_make_unce_types.keys() and g_make_unce_types['qcdKdeSyst']: \n",
    "        sys_name = \"qcdKdeSystUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"qcdKdeSystDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    if 'psWeightIsr' in g_make_unce_types.keys() and g_make_unce_types['psWeightIsr']: \n",
    "        sys_name = \"psWeightIsrUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"psWeightIsrDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    if 'psWeightFsr' in g_make_unce_types.keys() and g_make_unce_types['psWeightFsr']: \n",
    "        sys_name = \"psWeightFsrUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = \"psWeightFsrDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "\n",
    "    if 'sfBDTRwgt' in g_make_unce_types.keys() and g_make_unce_types['sfBDTRwgt']: \n",
    "        sys_name = 'sfBDTRwgtUp'; wgtstr_dm_sys = wgtstr_dm;'''factors decided by special_wgtstr argument'''; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, special_wgtstr='sfbdtwgt_g50')\n",
    "        sys_name = 'sfBDTRwgtDown'; wgtstr_dm_sys = wgtstr_dm;'''factors decided by special_wgtstr argument'''; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, special_wgtstr='sfbdtwgt_g50')\n",
    "    \n",
    "    if 'fitVarRwgt' in g_make_unce_types.keys() and g_make_unce_types['fitVarRwgt']: \n",
    "        sys_name = 'fitVarRwgtUp'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        sys_name = 'fitVarRwgtDown'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTemplates(df3, fitinfo, outputdir, sys_name, wgtstr_dm_sys, args, special_wgtstr=None):\n",
    "    r\"\"\"Depth 3: The very base implementation that apply the final pass/fail cut and make the template\n",
    "    \"\"\"\n",
    "    \n",
    "    wgtstr_dm, wgtstr_dm_data, sl_dm, sl_dm_herwig, config_dm, categories_dm, catMap = args['wgtstr_dm'], args['wgtstr_dm_data'], args['sl_dm'], args['sl_dm_herwig'], args['config_dm'], args['categories_dm'], args['catMap']\n",
    "    \n",
    "    ## Create the output root file\n",
    "    if not os.path.exists(outputdir) and not g_dryrun:\n",
    "        os.makedirs(outputdir)\n",
    "\n",
    "    import ROOT, array  ## use ROOT to write file...\n",
    "    vname, nbin, xmin, xmax, (underflow, overflow), vlabel = fitinfo\n",
    "    ## Tranfer the {nbin, xmin, xmax} set to the real bin edge if necessary\n",
    "    if not isinstance(nbin, int):\n",
    "        edges = nbin\n",
    "        nbin = len(edges)-1 # reset nbin to \"real\" nbin\n",
    "        edges_inroot = (len(edges)-1, array.array('f', edges))\n",
    "    else:\n",
    "        edges = np.linspace(xmin, xmax, nbin+1)\n",
    "        edges_inroot = (nbin, xmin, xmax)\n",
    "\n",
    "    ## Impose the overall factor between MC and data\n",
    "    def extract_factor_overal(_sl, _wgtstr):\n",
    "        return np.round(df3[_sl[-1]].shape[0] * 1. / sum([df3[sam].eval(_wgtstr).sum() for sam in _sl[:-1]]), 4)\n",
    "    \n",
    "    if special_wgtstr is None: ## no special weight string provided -> use the nominal one\n",
    "        if any([_sys in sys_name for _sys in ['qcdSyst','qcdKdeSyst']]): # note that qcd syst uses the setting of the herwig sample\n",
    "            fac_overal = g_wgtstr_dm_sys_fac['qcdSystUp'] if 'qcdSystUp' in g_wgtstr_dm_sys_fac else \\\n",
    "                         g_wgtstr_dm_sys_fac['qcdKdeSystUp'] if 'qcdKdeSystUp' in g_wgtstr_dm_sys_fac else None\n",
    "            if fac_overal is None:\n",
    "                fac_overal = extract_factor_overal(sl_dm_herwig, wgtstr_dm.replace('htwgt','htwgt_herwig'))\n",
    "        else:  # nominal case\n",
    "            fac_overal = g_wgtstr_dm_sys_fac['nominal'] if 'nominal' in g_wgtstr_dm_sys_fac else None\n",
    "            if fac_overal is None:\n",
    "                fac_overal = extract_factor_overal(sl_dm, wgtstr_dm)\n",
    "        # equip the weight factor\n",
    "        g_wgtstr_dm_sys_fac[sys_name] = fac_overal\n",
    "        wgtstr_dm_sys = wgtstr_dm_sys+f'*{fac_overal}'\n",
    "\n",
    "    else: ## special weight string specified\n",
    "        if sys_name.endswith('Up'):\n",
    "            fac_overal = extract_factor_overal(sl_dm, wgtstr_dm+f'*{special_wgtstr}')\n",
    "            # equip the weight factor\n",
    "            g_wgtstr_dm_sys_fac[sys_name] = fac_overal\n",
    "            wgtstr_dm_sys = wgtstr_dm+f'*{special_wgtstr}*{fac_overal}'\n",
    "        else:\n",
    "            wgtstr_dm_sys = wgtstr_dm+f\"*(2*{g_wgtstr_dm_sys_fac['nominal']}-{special_wgtstr}*{g_wgtstr_dm_sys_fac[sys_name.replace('Down','Up')]})\"\n",
    "\n",
    "    print (fitinfo, outputdir, sys_name, wgtstr_dm_sys)\n",
    "    \n",
    "    ## Preprocess for fitVarRwgt\n",
    "    if sys_name == 'fitVarRwgtUp':\n",
    "        _df_mc = pd.concat([df3[sam] for sam in sl_dm[:-1]])\n",
    "        _df_data = df3[sl_dm[-1]]\n",
    "        _h_data = get_hist(_df_data[vname].values, bins=edges, weights=np.ones(_df_data.shape[0]) if wgtstr_dm_data==None else _df_data.eval(wgtstr_dm_data).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "        _h_mc = get_hist(_df_mc[vname].values, bins=edges, weights=_df_mc.eval(wgtstr_dm_sys).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "        g_hist_fitvar_rwgt[sys_name] = _h_data.value / _h_mc.value\n",
    "    \n",
    "    ## Loop over pass and fail region\n",
    "    for b in ['pass', 'fail']:\n",
    "        try:\n",
    "            if not g_dryrun:\n",
    "                fw = ROOT.TFile(outputdir+f'inputs_{b}.root', 'recreate')\n",
    "            \n",
    "            hv, hist = {}, {}\n",
    "            hname_suf = '_'+sys_name if sys_name!='nominal' else ''  ## suffix to the hist name (the Higgs Combine syntax)\n",
    "            print (' -- ', catMap[b])\n",
    "            \n",
    "            ## MC and data dataframe after applying the final selection\n",
    "            df_mc = pd.concat([df3[sam].query(catMap[b]) for sam in sl_dm[:-1]])\n",
    "            df_data = df3[sl_dm[-1]].query(catMap[b])\n",
    "            \n",
    "            ## Preprocessing for herwig related dataframe if we mean to calculate qcdSyst / qcdKdeSyst unce in this iteration\n",
    "            if 'qcdSyst' in sys_name or 'qcdKdeSyst' in sys_name:\n",
    "                df_mc_herwig = pd.concat([df3[sam].query(catMap[b]) for sam in sl_dm_herwig[:-1]])\n",
    "\n",
    "            # Loop over categories: flvC/flvB/flvL/data\n",
    "            for cat in config_dm:\n",
    "                ## hv[] holds the boosted-histogram type derived from the dataframe, hist[] holds the TH1D type to be stored in ROOT\n",
    "                if cat=='data' and sys_name == 'nominal':\n",
    "                    ## Get the data hist\n",
    "                    hv['data'] = get_hist(df_data[vname].values, bins=edges, weights=np.ones(df_data.shape[0]) if wgtstr_dm_data==None else df_data.eval(wgtstr_dm_data).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                    # Initialize the TH1D hist\n",
    "                    hist['data'] = ROOT.TH1D('data_obs', 'data_obs;'+vname, *edges_inroot) \n",
    "                if cat!='data':\n",
    "                    df_mc_tmp = df_mc.query(config_dm[cat]) ## category selection based on flavor\n",
    "                    ## Get the MC hist for certain flavor\n",
    "                    hv[cat] = get_hist(df_mc_tmp[vname].values, bins=edges, weights=df_mc_tmp.eval(wgtstr_dm_sys).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                    # Initialize the TH1D hist\n",
    "                    hist[cat] = ROOT.TH1D(cat+hname_suf, cat+hname_suf+';'+vname, *edges_inroot) # init TH1 hist\n",
    "                    hist[cat].Sumw2()\n",
    "            \n",
    "                    ## For qcdSyst / qcdKdeSyst unce that is actually related to Herwig, hv[cat] is dummy here, \n",
    "                    ## and we mean to obtain hv[cat+'_herwig.value'] that will be later filled into hist[cat]\n",
    "                    if sys_name=='qcdSystUp':\n",
    "                        ## Get the Herwig fit for certain flavor\n",
    "                        df_mc_herwig_tmp = df_mc_herwig.query(config_dm[cat]) ## cat selection\n",
    "                        wgtstr_dm_sys_herwig = wgtstr_dm_sys.replace('htwgt','htwgt_herwig').replace('sfbdtwgt_g50','sfbdtwgt_g50_herwig').replace('ad_ptwgt','ad_ptwgt_herwig').replace('datamcwgt','datamcwgt_herwig')\n",
    "                        hv[cat+'_herwig.value'] = get_hist(df_mc_herwig_tmp[vname].values, bins=edges, \n",
    "                                                     weights=df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, \n",
    "                                                     underflow=underflow, overflow=overflow).view(flow=True).value\n",
    "                        ## Store the histogram into global var so we can recycle the same hist in the \"Down\" routine\n",
    "                        g_hist_qcdsyst[(sys_name, b, cat)] = hv[cat+'_herwig.value']\n",
    "                    \n",
    "                    ## Extract the KDE shape directly from herwig shape\n",
    "                    if sys_name=='qcdKdeSystUp':\n",
    "                        df_mc_herwig_tmp = df_mc_herwig.query(config_dm[cat])\n",
    "                        wgtstr_dm_sys_herwig = wgtstr_dm_sys.replace('htwgt','htwgt_herwig').replace('sfbdtwgt_g50','sfbdtwgt_g50_herwig').replace('ad_ptwgt','ad_ptwgt_herwig').replace('datamcwgt','datamcwgt_herwig')\n",
    "                        hv_herwig_orig_value = get_hist(df_mc_herwig_tmp[vname].values, bins=edges, \n",
    "                                                     weights=df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, \n",
    "                                                     underflow=underflow, overflow=overflow).view(flow=True).value\n",
    "                        \n",
    "                        ## Calculate KDE shape, apply two times so that we specify a finer KDE bindwidth based on the first result\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(df_mc_herwig_tmp[vname].values, weights=np.clip(df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, 0, +np.inf))\n",
    "                        kde = gaussian_kde(df_mc_herwig_tmp[vname].values, weights=np.clip(df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, 0, +np.inf), bw_method=kde.factor/g_custom_kde_bw[vname])\n",
    "                        kde_int = np.zeros([nbin, 2])\n",
    "                        \n",
    "                        ## Integrate the KDE function to obtain KDE histogram\n",
    "                        for i, (low, high) in enumerate(zip(edges[:-1], edges[1:])):\n",
    "                            if low in g_custom_kde_binmask[vname]:\n",
    "                                continue\n",
    "                            kde_int[i] = [kde.integrate_box_1d(low, high), hv_herwig_orig_value[i]]\n",
    "                        # print('rescale kde sum to original herwig sum: ', kde_int[:,1].sum() / kde_int[:,0].sum())\n",
    "                        kde_int[:,0] *= kde_int[:,1].sum() / kde_int[:,0].sum()\n",
    "                        \n",
    "                        ## Fill with original madgraph hist if we plan to mask the bin for KDE. \n",
    "                        ## This is based on the fact that KDE cannot model the hist well in the marginal bins\n",
    "                        hv[cat+'_herwig.value'] = np.array([kde_int[i][0] if kde_int[i][0]!=0 else hv[cat].value[i] for i in range(nbin)])\n",
    "                        \n",
    "                        ## Store the histogram into global var so we can recycle the same hist in the \"Down\" routine\n",
    "                        g_hist_qcdsyst[(sys_name, b, cat)] = hv[cat+'_herwig.value']\n",
    "            \n",
    "                    ## Extract the PSWeight histogram\n",
    "                    if 'psWeight' in sys_name:\n",
    "                        if year==2018:  ## for 2018, calculate the hist by PSWeight vars \n",
    "                            ps_idx = {'psWeightIsrUp':2, 'psWeightIsrDown':0, 'psWeightFsrUp':3, 'psWeightFsrDown':1}\n",
    "                            hv[cat] = get_hist(df_mc_tmp[vname].values, bins=edges, weights=df_mc_tmp.eval(wgtstr_dm_sys+f'*PSWeight{ps_idx[sys_name]+1}').values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                        else:  ## for 2016/17 extract the PSWeight hist based on 2018 result (transfer the ratio for PSWeight/nominal)\n",
    "                            import re\n",
    "                            outputdir_ps_18 = re.sub('^(.+)_SF201[6-8]_(.*)_([A-Z]P_.*)$', f'\\g<1>_SF2018_\\g<2>_psWeight{year}_\\g<3>', outputdir)\n",
    "                            hv_nom_18 = uproot.open(outputdir_ps_18.replace(sys_name, 'nominal')+f'inputs_{b}.root')[cat]\n",
    "                            hv_ps_18 = uproot.open(outputdir_ps_18+f'inputs_{b}.root')[cat+'_'+sys_name]\n",
    "                            hv[cat].value *= hv_ps_18.values / hv_nom_18.values\n",
    "                        # print (hv[cat].value)\n",
    "                    \n",
    "                    ## Extract the sfBDTFloAround histogram.\n",
    "                    ## Method: to utilize the nominal hist for sfbdt>0.95 or 0.85 and migrate the MC-to-data confidence level in the 0.90 case\n",
    "                    if 'sfBDTFloAround' in sys_name:\n",
    "                        from scipy.stats import chi2\n",
    "                        hv_data = uproot.open(outputdir.replace(sys_name, 'nominal')+f'inputs_{b}.root')['data_obs'].values  ## nominal data hist for 0.90\n",
    "                        _bdtname = '95' if 'Up' in sys_name else '85'\n",
    "                        fr = uproot.open(outputdir.replace(sys_name, 'nominal').replace(f'/bdt{int(g_sfBDT_val_list[-1]*1000)}/',f'/bdt{_bdtname}0/')+f'inputs_{b}.root')\n",
    "                        fr_data, fr_mc = fr['data_obs'].values, fr['flvC'].values+fr['flvB'].values+fr['flvL'].values  ## nominal data & MC hist for 0.95 or 0.85 (depends on Up or Down)\n",
    "                        \n",
    "                        ## For each bins, migrate the confidence level of MC yield F0 given data yield D0 to the target data yield D => F\n",
    "                        hv_mc = []\n",
    "                        for D, D0, F0 in zip(hv_data, fr_data, fr_mc):\n",
    "                            ## The precise calculation\n",
    "                            F = 0.5*chi2.ppf(chi2.cdf(2*F0, 2*D0+2), 2*D+2) if F0>D0 else 0.5*chi2.ppf(chi2.cdf(2*F0, 2*D0), 2*D)\n",
    "                            if F == np.inf: ## in case the formula results in inf (may occur if F0 >> D0)\n",
    "                                assert F0 > D0\n",
    "                                sigD0 = 0.5 * chi2.ppf(1-(1-0.682689492)/2, 2*D0+2) - D0\n",
    "                                sigD = 0.5 * chi2.ppf(1-(1-0.682689492)/2, 2*D+2) - D\n",
    "                                F = D + sigD/sigD0*(F0-D0)\n",
    "                            hv_mc.append(F)\n",
    "                        \n",
    "                        ## Obtain flavor template based on the flavor proportion in 0.95 or 0.85 region\n",
    "                        hv[cat].value = np.nan_to_num(hv_mc * fr[cat].values / fr_mc, nan=0)\n",
    "                    \n",
    "                    ## Modify hv[cat] based on extracted pass+fail histogram\n",
    "                    if 'fitVarRwgt' in sys_name:\n",
    "                        if sys_name == 'fitVarRwgtUp':\n",
    "                            hv[cat].value = hv[cat].value * g_hist_fitvar_rwgt['fitVarRwgtUp']\n",
    "                        else:\n",
    "                            hv[cat].value = 2 * hv[cat].value - hv[cat].value * g_hist_fitvar_rwgt['fitVarRwgtUp']\n",
    "                    \n",
    "                    ## Use bflav qcd samples to stitch the final bflav template\n",
    "                    if 'use_bflav' in args and args['use_bflav'] and cat == 'flvB' and not all([s in sys_name for s in ['qcd','Syst']]):\n",
    "                        # print('---', hv[cat])\n",
    "                        ## Get the MC hist from the new b-enriched sample\n",
    "                        df_mc_bflav = pd.concat([df3[sam].query(f'({catMap[b]}) & ({config_dm[cat]})') for sam in args['args_bflav']['sl_dm_bflav']])\n",
    "                        hv_bflav = get_hist(df_mc_bflav[vname].values, bins=edges, weights=df_mc_bflav.eval(args['args_bflav']['wgtstropt_bflav'](wgtstr_dm_sys)).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                        df_mc_bflav_og = pd.concat([df3[sam].query(f'({catMap[b]}) & ({config_dm[cat]})') for sam in args['args_bflav']['sl_dm_bflav_orig']])\n",
    "                        hv_bflav_og = get_hist(df_mc_bflav_og[vname].values, bins=edges, weights=df_mc_bflav_og.eval(wgtstr_dm_sys).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                        ## Combine histogram\n",
    "                        hv_bflav_og.variance[hv_bflav_og.variance==0] = 1e20\n",
    "                        hv_bflav.variance[hv_bflav.variance==0] = 1e20\n",
    "                        hv_bflav_comb = hv[cat].copy()\n",
    "                        hv_bflav_comb.value = (hv_bflav_og.value*(1/hv_bflav_og.variance) + hv_bflav.value*(1/hv_bflav.variance)) / (1/hv_bflav_og.variance + 1/hv_bflav.variance)\n",
    "                        hv_bflav_comb.variance = 1 / (1/hv_bflav_og.variance + 1/hv_bflav.variance)\n",
    "                        ## Further combine with the non no-QCD contribution\n",
    "                        hv_bflav_nonsubst = hv[cat].copy() # histogram constitution not to be combined (i.e. no-QCD contribution)\n",
    "                        hv_bflav_nonsubst.value -= hv_bflav_og.value\n",
    "                        hv_bflav_nonsubst.variance -= hv_bflav_og.variance\n",
    "                        hv[cat] = hv_bflav_comb + hv_bflav_nonsubst\n",
    "                        # print('+++', hv_bflav_og, hv_bflav, hv_bflav_comb, hv_bflav_nonsubst, hv[cat])\n",
    "                    \n",
    "            ## Fill the hv[cat] (for qcd*, fill hv[cat+'_herwig.value']) into TH1D and save into ROOT\n",
    "            for cat in hist.keys():\n",
    "                ## Special handling for qcdSyst / qcdKdeSyst\n",
    "                if 'qcd' in sys_name and 'SystUp' in sys_name:\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, hv[cat+'_herwig.value'][i])\n",
    "                elif 'qcd' in sys_name and 'SystDown' in sys_name:\n",
    "                    hv[cat+'_herwig.value'] = g_hist_qcdsyst[(sys_name.replace('Down','Up'), b, cat)]\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, 2 * hv[cat].value[i] - hv[cat+'_herwig.value'][i])\n",
    "                    g_hist_qcdsyst[(sys_name.replace('Down','Up'), b, cat)] = None\n",
    "\n",
    "                ## Normal routine\n",
    "                else:\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, hv[cat].value[i])\n",
    "                        hist[cat].SetBinError(i+1, np.sqrt(hv[cat].variance[i]))\n",
    "                \n",
    "                ## Fix some buggy points\n",
    "                if cat!='data':\n",
    "                    for i in range(nbin):\n",
    "                        if hist[cat].GetBinContent(i+1) <= 1e-3:\n",
    "                            hist[cat].SetBinContent(i+1, 1e-3)\n",
    "                            hist[cat].SetBinError(i+1, 1e-3)\n",
    "                        elif hist[cat].GetBinError(i+1) > hist[cat].GetBinContent(i+1):\n",
    "                            hist[cat].SetBinError(i+1, hist[cat].GetBinContent(i+1))\n",
    "\n",
    "                if not g_dryrun:\n",
    "                    hist[cat].Write()\n",
    "        ## Close the ROOT file if error occurs (otherwise the notebook is easily corrupted)\n",
    "        finally:\n",
    "            if not g_dryrun:\n",
    "                fw.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we launch the template maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====================================================================================================\n",
    "## Main fit routine: launch all sfBDT values, only run on 1st variable\n",
    "g_dryrun = False\n",
    "g_make_template_mode = 'main'; g_mode_bdt_runlist = 'all'\n",
    "g_mode_psWeight_run_templ = None\n",
    "g_do_fit_for_var = [1] # only run the first fit variable (2, 3 are for validation fit)\n",
    "launch_maker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For year 2018**: you need to run the following block to provide psWeight templates for year 2016 and 2017 (otherwise 2016 and 2017 will report errors)\n",
    "\n",
    "However, you need to first run the same pre-processing for the corresponding 2016 and 2017 util step 3-2 to extract the sfBDT sequence in that year condition. The sequence will be stored to the file e.g. `plots/prep-pd/bdt_seq_2016.pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====================================================================================================\n",
    "## For year 2018, extract necessary psWeight templates for 2016/2017\n",
    "if year == 2018:\n",
    "    for ext_year in [2016, 2017]:\n",
    "        g_make_template_mode = 'main'; g_mode_bdt_runlist = 'all'\n",
    "        g_mode_psWeight_run_templ = ext_year\n",
    "        g_do_fit_for_var = [1] # only run the first fit variable (2, 3 are for validation fit)\n",
    "        launch_maker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are optional routines for the validation fit. No need to launch during the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ====================================================================================================\n",
    "# ## Validation on other variables\n",
    "# g_make_template_mode = 'main'; g_mode_bdt_runlist = 'all'\n",
    "# g_mode_psWeight_run_templ = None\n",
    "# g_do_fit_for_var = [2, 3]\n",
    "# launch_maker()\n",
    "\n",
    "# ## ====================================================================================================\n",
    "# ## Multiple validations modes: only run the central sfBDT cut point is fine\n",
    "# for mode in ['val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21', 'val_crop_bin']:\n",
    "#     g_make_template_mode = mode; g_mode_bdt_runlist = 'central'\n",
    "#     g_mode_psWeight_run_templ = None\n",
    "#     g_do_fit_for_var = [1]\n",
    "#     launch_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data/MC comparison plots\n",
    "\n",
    "Based on the DataFrame `df1`, this section aims to make data and MC plots, while MC is categorized into three flavors: C/B/L.\n",
    "With the universial make_data_mc_plots function, one can make specify any final selection, any sample list to produce the standard hist+ratio plot.\n",
    "\n",
    "The below recipe can make a default set of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ configuration  ===================\n",
    "\n",
    "def make_config_dm(sl_dm, wgtstr_dm):\n",
    "    return {\n",
    "        'data':  ('Data',       'jetht-noht',      '1.0',    ''      ),\n",
    "        'flvB':  ('MC (flvB)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons>=1'  ),\n",
    "        'flvC':  ('MC (flvC)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons==0 & fj_x_nchadrons>=1'  ),\n",
    "        'flvL':  ('MC (flvL)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons==0 & fj_x_nchadrons==0'  ),\n",
    "    }\n",
    "\n",
    "categories_dm = ['flvL', 'flvB', 'flvC', 'data']\n",
    "\n",
    "bininfo_dm = [ #(savename, vname, nbin, xmin, xmax, label)\n",
    "    ('ht', 50, 0, 2000, r'$H_{T}$ [GeV]'),\n",
    "#     ('fj_x_pt', 20, 200, 800, r'$p_{T}(AK15)$ [GeV]'),\n",
    "#     ('fj_x_eta', 20, -2.5, 2.5, r'$\\eta(AK15)$'),\n",
    "#     ('fj_x_sdmass', 15, 50, 200, r'$m_{SD}(AK15)$ [GeV]'),\n",
    "#     ('fj_x_sfBDT', 50, 0.5, 1, r'$sfBDT(AK15)$'),\n",
    "\n",
    "#     ('fj_x_ParticleNetMD_XccVsQCD', 100, 0, 1, r'ParticleNetMD_XccVsQCD(AK15)'),\n",
    "#     ('fj_x_origParticleNetMD_XccVsQCD', 50, 0, 1, r'ParticleNetMD_XccVsQCD(AK15) orig'),\n",
    "#     (('fj_x_ParticleNetMD_XccVsQCD_08', 'fj_x_ParticleNetMD_XccVsQCD'), 40, 0.8, 1, r'ParticleNetMD_XccVsQCD(AK15)-u'),\n",
    "    \n",
    "#     ('fj_x_btagcsvv2', [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, r'$CSVv2$'),\n",
    "#     ('mSV12_ptmax_log', [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, r'$log(m_{SV1,p_{T}\\,max}\\; /GeV)$'),\n",
    "#     ('mSV12_dxysig_log', [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, r'$log(m_{SV1,d_{xy}sig\\,max}\\; /GeV)$'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ slim on cc-tagger, sfBDT, then make data/MC plots ===================\n",
    "\n",
    "import seaborn as sns\n",
    "def set_sns_color(*args):\n",
    "    sns.palplot(sns.color_palette(*args))\n",
    "    sns.set_palette(*args)\n",
    "    \n",
    "def make_data_mc_plots(sl_dm, config_dm, finsel, prefix, **kwargs):\n",
    "    r\"\"\"To make standard hist+ratio plots based on the sample list and the final selection\n",
    "    Arguments:\n",
    "        sl_dm: sample list\n",
    "        config_dm: configuration set for each categories in the plots, in the dict format. name: (label, sample/sample list, weight string, cat selection)\n",
    "        finsel: final selections made to produce the plots\n",
    "        prefix: prefix string used in the output plot title\n",
    "        kwargs: includes further KDE-related variables\n",
    "    \"\"\"\n",
    "    \n",
    "    df2 = {}\n",
    "    for sam in sl_dm:\n",
    "        df2[sam] = df1[sam].query(finsel)\n",
    "\n",
    "    for vname, nbin, xmin, xmax, vlabel in bininfo_dm:\n",
    "        if not isinstance(vname, str): ## savename is specified other then the variable name\n",
    "            savename, vname = vname\n",
    "        else:\n",
    "            savename = vname\n",
    "        if 'plot_vars' in kwargs and savename not in kwargs['plot_vars']:\n",
    "            continue\n",
    "        if not isinstance(nbin, int):\n",
    "            edges, xmin, xmax, nbin = nbin, min(nbin), max(nbin), len(nbin)\n",
    "        else:\n",
    "            edges = np.linspace(xmin, xmax, nbin+1)\n",
    "\n",
    "        label, hdm = {}, {}\n",
    "        underflow = False if vlabel[-2:] in ['-u','-a'] else True\n",
    "        overflow  = False if vlabel[-2:] in ['-o','-a'] else True\n",
    "        if vlabel[-2:] in ['-u','-o','-a']:\n",
    "            vlabel = vlabel[:-2]\n",
    "        \n",
    "        if 'g_do_kde_vars' in kwargs and savename in kwargs['g_do_kde_vars'] and kwargs['g_do_kde_vars'][savename]==True:\n",
    "            g_do_kde_vars = True\n",
    "            kde = {}\n",
    "        else:\n",
    "            g_do_kde_vars = False\n",
    "        \n",
    "        ## Loop over categories to extract the hist for each flavor and data\n",
    "        for cat in categories_dm:\n",
    "            lab, sam, wgt, sel = config_dm[cat]\n",
    "            label[cat] = lab\n",
    "            if cat != 'data':\n",
    "                if not isinstance(sam, list):\n",
    "                    df2tmp = df2[sam].query(sel) if sel not in ['','1==1'] else df2[sam]\n",
    "                else:\n",
    "                    df2tmp = []\n",
    "                    for s in sam:\n",
    "                        df2tmp.append(df2[s].query(sel) if sel not in ['','1==1'] else df2[s])\n",
    "                    df2tmp = pd.concat(df2tmp, ignore_index=True)\n",
    "                hdm[cat] = get_hist(df2tmp[vname].values, bins=edges, weights=df2tmp.eval(wgt).values, underflow=underflow, overflow=overflow)\n",
    "                if g_do_kde_vars:\n",
    "                    from scipy.stats import gaussian_kde\n",
    "                    from scipy import integrate\n",
    "                    import multiprocessing\n",
    "                    if 'custom_kde' in kwargs.keys() and savename in kwargs['custom_kde']:\n",
    "                        kde[cat] = kwargs['custom_kde'][savename][cat]\n",
    "                        kde_int_res = [\n",
    "                                integrate.quad(kde[cat][0], -np.inf if (i==0 and underflow) else edges[i], \n",
    "                                                  +np.inf if (i==len(edges)-1 and overflow) else edges[i+1]) for i in range(len(edges)-1)]\n",
    "                    else:\n",
    "                        kdetmp = gaussian_kde(df2tmp[vname].values, weights=np.clip(df2tmp.eval(wgt).values, 0, np.inf))\n",
    "                        if 'g_custom_kde_bw' in kwargs.keys() and savename in kwargs['g_custom_kde_bw']:\n",
    "                            kdetmp = gaussian_kde(df2tmp[vname].values, weights=np.clip(df2tmp.eval(wgt).values, 0, np.inf), bw_method=kdetmp.factor/kwargs['g_custom_kde_bw'][savename])\n",
    "                        kde[cat] = (kdetmp, df2tmp.eval(wgt).sum())\n",
    "                        kde_int_res = [(kde[cat][0].integrate_box_1d(-np.inf if (i==0 and underflow) else edges[i], +np.inf if (i==len(edges)-1 and overflow) else edges[i+1]), 0.) for i in range(len(edges)-1)]\n",
    "                    hdm[cat+'_kde'] = hdm[cat].copy()\n",
    "                    hdm[cat+'_kde'].view(flow=True).value = np.array([kde_int_res[i][0] for i in range(len(edges)-1)]) * kde[cat][1]\n",
    "                    hdm[cat+'_kde'].view(flow=True).variance = np.zeros(len(edges)-1)\n",
    "                        \n",
    "            else: ## is data: no sel, weight=1\n",
    "                hdm[cat] = get_hist(df2[sam][vname].values, bins=edges, weights=np.ones(df2[sam].shape[0]), underflow=underflow, overflow=overflow)\n",
    "        \n",
    "        cat_sufs = ['']\n",
    "        if g_do_kde_vars:\n",
    "            cat_sufs += ['_kde']\n",
    "        for cat_suf in cat_sufs:\n",
    "            ## Draw the standard hist_ratio plot\n",
    "            set_sns_color('cubehelix_r', 3) ## set the color palette\n",
    "            f = plt.figure(figsize=(12,12))\n",
    "            gs = mpl.gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.05) \n",
    "            \n",
    "            ## Upper histogram panel\n",
    "            ax = f.add_subplot(gs[0])\n",
    "            hep.cms.label(data=True, paper=False, year=2016, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "            ax.set_xlim(xmin, xmax); ax.set_xticklabels([]); ax.set_ylabel('Events / bin', ha='right', y=1.0)\n",
    "\n",
    "            plot_hist([hdm[cat+cat_suf] for cat in categories_dm if cat!='data'], bins=edges, label=[label[cat] for cat in categories_dm if cat!='data'], histtype='fill', edgecolor='k', linewidth=1, stack=True) ## draw stacked bkg\n",
    "            cats_mc = list(set(categories_dm) - set(['data']))\n",
    "            hdm_add = hdm[cats_mc[0]+cat_suf].copy()\n",
    "            for cat in cats_mc[1:]:\n",
    "                hdm_add += hdm[cat+cat_suf]\n",
    "            bkgtot, bkgtot_err = hdm_add.view(flow=True).value, np.sqrt(hdm_add.view(flow=True).variance)\n",
    "            ax.fill_between(edges, (bkgtot-bkgtot_err).tolist()+[0], (bkgtot+bkgtot_err).tolist()+[0], label='BKG unce.', step='post', hatch='///', edgecolor='darkblue', facecolor='none', linewidth=0) ## draw bkg unce.\n",
    "            plot_hist(hdm['data'], bins=edges, label='Data', histtype='errorbar', color='k', markersize=15, elinewidth=1.5) ## draw data\n",
    "#             ax.set_yscale('log')\n",
    "            \n",
    "            ax.legend()\n",
    "            # ax.legend(loc='upper left'); ax.set_ylim(0, 1.4*ax.get_ylim()[1])\n",
    "            \n",
    "            ## Ratio panel\n",
    "            ax1 = f.add_subplot(gs[1]); ax1.set_xlim(xmin, xmax); ax1.set_ylim(0.001, 1.999)\n",
    "            ax1.set_xlabel(vlabel, ha='right', x=1.0); ax1.set_ylabel('Data / MC', ha='center')\n",
    "            ax1.plot([xmin,xmax], [1,1], 'k'); ax1.plot([xmin,xmax], [0.5,0.5], 'k:'); ax1.plot([xmin,xmax], [1.5,1.5], 'k:')\n",
    "\n",
    "            hr = hdm['data'].view(flow=True).value / hdm_add.view(flow=True).value\n",
    "            # hr_err = hr * np.sqrt(hdm['data'].view(flow=True).variance/(hdm['data'].view(flow=True).value**2) + hdm_add.view(flow=True).variance/(hdm_add.view(flow=True).value**2))\n",
    "            hr_dataerr = hr * np.sqrt(hdm['data'].view(flow=True).variance/(hdm['data'].view(flow=True).value**2))\n",
    "            ax1.fill_between(edges, ((bkgtot-bkgtot_err)/bkgtot).tolist()+[0], ((bkgtot+bkgtot_err)/bkgtot).tolist()+[0], step='post', hatch='///', edgecolor='darkblue', facecolor='none', linewidth=0) ## draw bkg unce.\n",
    "            hep.histplot(np.nan_to_num(hr, nan=-1), bins=edges, yerr=np.nan_to_num(hr_dataerr), histtype='errorbar', color='k', markersize=15, elinewidth=1) ## draw data in ratio plot\n",
    "\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}_pd/{prefix}__{finsel}__{savename}{cat_suf}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}_pd/{prefix}__{finsel}__{savename}{cat_suf}.pdf')\n",
    "            pickle.dump(hdm['data'], open(f'plots/{g_dirname}_{year}_pd/{prefix}__{finsel}__{savename}{cat_suf}.pickle', 'wb'))\n",
    "\n",
    "        ## kde/orig comparison plots\n",
    "        if g_do_kde_vars:\n",
    "            mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green'])\n",
    "            f, ax = plt.subplots(figsize=(12,12))\n",
    "            hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "            x_contin = np.linspace(xmin, xmax, 201)\n",
    "            bin_width = edges[int(nbin/2)+1] - edges[int(nbin/2)]\n",
    "            for cat, color in zip(['flvC', 'flvB', 'flvL'], ['blue', 'red', 'green']):\n",
    "                lab, sam, wgt, sel = config_dm[cat]\n",
    "                ax.plot(x_contin, kde[cat][0](x_contin) * kde[cat][1] * bin_width, label=lab+' KDE', linestyle=':', color=color)\n",
    "            for cat, color in zip(['flvC', 'flvB', 'flvL'], ['blue', 'red', 'green']):\n",
    "                lab, sam, wgt, sel = config_dm[cat]\n",
    "                hep.histplot(hdm[cat+'_kde'].view(flow=True).value, bins=edges, label=lab+' KDE integral', linestyle='--', color=color)\n",
    "                plot_hist(hdm[cat], bins=edges, label=lab, normed=False, color=color)\n",
    "            ax.set_xlim(xmin, xmax); ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.', ha='right', y=1.0); ax.legend()\n",
    "            \n",
    "            plt.savefig(f'plots/{g_dirname}_{year}_pd/{prefix}:kde_shape__{finsel}__{savename}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}_pd/{prefix}:kde_shape__{finsel}__{savename}.pdf')\n",
    "            \n",
    "\n",
    "g_do_kde_vars = {'fj_x_btagcsvv2':True, 'mSV12_ptmax_log':True, 'mSV12_dxysig_log':True}\n",
    "g_custom_kde_bw = {'fj_x_btagcsvv2':15, 'mSV12_ptmax_log':4, 'mSV12_dxysig_log':4}\n",
    "\n",
    "g_dirname = 'test_datamc' ## config me\n",
    "if not os.path.exists(f'plots/{g_dirname}_{year}_pd'):\n",
    "    os.makedirs(f'plots/{g_dirname}_{year}_pd')\n",
    "\n",
    "for ptrange in config['pt_range']['range']:\n",
    "    ptmin, ptmax = ptrange\n",
    "    bdt_seq = df1[f\"bdt_seq_{config['pt_range']['name']}\"][(ptrange[0], ptrange[1])]\n",
    "    bdt_cent = bdt_seq[int((len(bdt_seq)-1)/2)]\n",
    "    tagger_wp = sorted([rg[0] for rg in config['tagger']['working_points']['range'].values()])\n",
    "    \n",
    "    ## 1. With MadGraph sample list\n",
    "#     wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt'\n",
    "#     sl_dm = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.5', prefix='mg')\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>{bdt_cent:.3f}', prefix='mg')\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f\"fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>{bdt_cent:.3f} & {config['tagger']['var']}>{tagger_wp[-1]}\", prefix='mg')\n",
    "\n",
    "#     ## 2. With MadGraph sample list, while using the optional MC-to-data reweight scheme (on pT)\n",
    "#     wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*ad_ptwgt'\n",
    "#     sl_dm = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>{bdt_cent:.3f}', prefix='mg_ptwgt')\n",
    "    \n",
    "#     ## 3. With Herwig sample list\n",
    "#     wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt_herwig'\n",
    "#     sl_dm = ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.5', prefix='herwig')\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>{bdt_cent:.3f}', prefix='herwig')\n",
    "#     make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f\"fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>{bdt_cent:.3f} & {config['tagger']['var']}>{tagger_wp[-1]}\", prefix='herwig', \n",
    "#                        g_do_kde_vars=g_do_kde_vars, g_custom_kde_bw=g_custom_kde_bw) ## also make the KDE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal/proxy comparison plots\n",
    "\n",
    "Based on the DataFrame `df1`, The below recipe creates the proxy jet (from MC) and h->cc signal jet comparison plots on various jet observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the hcc signal tree\n",
    "if 'df_comp' not in globals():  \n",
    "    import re\n",
    "    _df0['vhcc-2L'] = uproot.open(f\"{re.search('^.+/trees', sample_prefix)[0]}/20210117_VH_extjetvar_{year}_2L/mc/vhcc_tree.root\")['Events'].pandas.df()\n",
    "\n",
    "    boosted = \"v_pt>200 & ak15_pt>200 & dphi_V_ak15>2.5 & ak15_sdmass>50 & ak15_sdmass<200\"\n",
    "    basecut = f\"fj_x_pt>200 & fj_x_sdmass>50 & fj_x_sdmass<200 & passmetfilters & fj_x_nbhadrons==0 & fj_x_nchadrons>=1\"\n",
    "    basecut_vhcc_2L = \"v_mass>75 & v_mass<105 & ((abs(lep1_pdgId)==11 & passTrigEl) | (abs(lep1_pdgId)==13 & passTrigMu)) & \" + boosted + \" & n_ak4<3\"\n",
    "    df_comp = {}\n",
    "    df_comp['proxy'] = pd.concat([df1[sam].query(basecut) for sam in ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht']])\n",
    "    df_comp['vhcc-2L'] = _df0['vhcc-2L'].query(basecut_vhcc_2L)\n",
    "\n",
    "wgtstr = 'genWeight*xsecWeight*puWeight*htwgt'\n",
    "wgtstr_vhcc_2L = 'genWeight*xsecWeight*puWeight'\n",
    "basesel = { # name: cut, label\n",
    "    'sv': (\"fj_x_sj1_nsv>=1 & fj_x_sj2_nsv>=1\", r'$N_{SV}^{match}\\geq 1$'),\n",
    "    'tightsv': (\"(fj_x_sj1_sv1_ntracks>2 & abs(fj_x_sj1_sv1_dxy)<3 & fj_x_sj1_sv1_dlensig>4 & fj_x_sj2_sv1_ntracks>2 & abs(fj_x_sj2_sv1_dxy)<3 & fj_x_sj2_sv1_dlensig>4)\", r'$N_{SV,tight}^{match}\\geq 1$'),\n",
    "}\n",
    "def func_basesel(name):\n",
    "    if name in basesel.keys():\n",
    "        return basesel[name]\n",
    "    elif name[:5]=='sfbdt':\n",
    "        x = float(name[5:])/1000.\n",
    "        return ('fj_x_sfBDT>%.3f'%x, r'$sfBDT>%.3f$'%x)\n",
    "    else:\n",
    "        raise RuntimeError('Baseline cut name not recognized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bininfo = [ #(vname, nbin, xmin, xmax, label, *vname for nominal*, xlim)   \n",
    "#     ('fj_x_ParticleNetMD_XccVsQCD', 20, 0, 1, 'ParticleNetMD_XccVsQCD (AK15)', 'ak15_ParticleNetMD_HccVsQCD', None),\n",
    "    (('fj_x_ParticleNetMD_XccVsQCD_3WP', 'fj_x_ParticleNetMD_XccVsQCD'), [0,0.5,0.8,0.9,0.96,0.99,1], None, None, 'ParticleNetMD_XccVsQCD (AK15)', 'ak15_ParticleNetMD_HccVsQCD', (0.9,1)),\n",
    "#     ('fj_x_sdmass', 15, 50, 200, r'$m_{SD}$ (AK15)', 'ak15_sdmass', None),\n",
    "#     ('fj_x_tau21', 20, 0, 1, r'$\\tau_{21}$ (AK15)', 'ak15_tau21', None), ##avaliable\n",
    "    \n",
    "#     ('fj_x_deltaR_sj12', 40, 0, 1.5, r'$\\Delta R_{sj_{1},sj_{2}}$ (AK15)', 'ak15_deltaR_sj12', None),\n",
    "#     ('fj_x_pt', 40, 0, 1000, r'$p_{T}$ (AK15)', 'ak15_pt', None),\n",
    "#     ('fj_x_sj1_pt', 40, 0, 1000, r'$p_{T,sj_{1}}$ (AK15)', 'ak15_sj1_pt', None),\n",
    "#     ('fj_x_sj1_rawmass', 40, 0, 200, r'$m_{sj_{1},raw}$ (AK15)', 'ak15_sj1_rawmass', None), ##avaliable\n",
    "#     ('fj_x_sj2_pt', 40, 0, 1000, r'$p_{T,sj_{2}}$ (AK15)', 'ak15_sj2_pt', None),\n",
    "#     ('fj_x_sj2_rawmass', 40, 0, 200, r'$m_{sj_{2},raw}$ (AK15)', 'ak15_sj2_rawmass', None), ##avaliable\n",
    "    \n",
    "#     ('fj_x_nsv', 10, 0, 10, r'$N_{SV}$ (AK15)', 'ak15_nlooseSV', None), ##avaliable\n",
    "#     ('fj_x_nsv_ptgt25', 8, 0, 8, r'$N_{SV,p_{T}\\geq 25}$ (AK15)', 'ak15_nlooseSV_ptgt25', None), ##avaliable\n",
    "#     ('fj_x_nsv_ptgt50', 8, 0, 8, r'$N_{SV,p_{T}\\geq 50}$ (AK15)', 'ak15_nlooseSV_ptgt50', None), ##avaliable\n",
    "#     ('fj_x_ntracks', 20, 0, 20, r'$N_{tracks}$ (AK15)', 'ak15_nlooseSV_ntracks', None), ##avaliable\n",
    "#     ('fj_x_ntracks_sv12', 20, 0, 20, r'$N_{tracks\\;for\\;SV_{1,2}}$ (AK15)', 'ak15_nlooseSV_ntracks_sv12', None), ##avaliable\n",
    "#     ('fj_x_sj1_nsv', 20, 0, 20, r'$N_{SV\\;from\\;sj_{1}}$ (AK15)', 'ak15_sj1_nlooseSV', None), ##avaliable\n",
    "#     ('fj_x_sj1_ntracks', 20, 0, 20, r'$N_{tracks\\;from\\;sj_{1}}$ (AK15)', 'ak15_sj1_nlooseSV_ntracks', None), ##avaliable\n",
    "#     ('fj_x_sj1_sv1_pt', 20, 0, 200, r'$p_{T,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_pt', None),\n",
    "#     ('fj_x_sj1_sv1_mass', 20, 0, 50, r'$m_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_mass', None), ##avaliable\n",
    "#     ('fj_x_sj1_sv1_masscor', 20, 0, 50, r'$m_{cor\\;for\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_masscor', None),\n",
    "#     ('fj_x_sj1_sv1_ntracks', 20, 0, 20, r'$N_{tracks\\;from\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_ntracks', None),\n",
    "#     ('fj_x_sj1_sv1_dxy', 20, 0, 5, r'$d_{xy,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dxy', None),\n",
    "#     ('fj_x_sj1_sv1_dxysig', 20, 0, 20, r'$\\sigma_{d_{xy},\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dxysig', None),\n",
    "#     ('fj_x_sj1_sv1_dlen', 20, 0, 5, r'$d_{z,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dlen', None),\n",
    "#     ('fj_x_sj1_sv1_dlensig', 20, 0, 20, r'$\\sigma_{d_{z},\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dlensig', None),\n",
    "#     ('fj_x_sj1_sv1_chi2ndof', 20, 0, 5, r'$\\chi^2 / Ndof_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_chi2ndof', None),\n",
    "#     ('fj_x_sj1_sv1_pangle', 40, 0, 5, r'$pAngle_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_pangle', None),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dirname = 'test_sigpxy' ## config me\n",
    "if not os.path.exists(f'plots/{g_dirname}_{year}_pd'):\n",
    "    os.makedirs(f'plots/{g_dirname}_{year}_pd')\n",
    "\n",
    "## Make comparison plots for normal weight (MC adopt the same weight as in the fit), or for additional mass / pT / tau21 weight\n",
    "# for wgtfac, pfwgt in zip(['1','massdatamcwgt','ptdatamcwgt'], ['nom', 'massdatamcwgt', 'ptdatamcwgt']):\n",
    "for wgtfac, pfwgt in zip(['1'], ['nom']):\n",
    "\n",
    "    wgtstr = f'genWeight*xsecWeight*puWeight*htwgt*{wgtfac}'\n",
    "    wgtstr_vhcc_2L = 'genWeight*xsecWeight*puWeight'\n",
    "\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "    do_rwgt = 0\n",
    "    for ptmin, ptmax in config['pt_range']['range']:\n",
    "        presel, presel1 = f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax}', f'ak15_pt>{ptmin} & ak15_pt<{ptmax}'\n",
    "        label = {'proxy': r'g(cc)', 'vhcc-2L':r'$Z(\\ell\\ell)H(cc)$'}\n",
    "\n",
    "        for vname, nbin, xmin, xmax, vlabel, vname1, xlim in bininfo:\n",
    "            if not isinstance(vname, str): ## savename is specified other then the variable name\n",
    "                savename, vname = vname\n",
    "            else:\n",
    "                savename = vname\n",
    "            if not isinstance(nbin, int):\n",
    "                edges, xmin, xmax, nbin = nbin, min(nbin), max(nbin), len(nbin)\n",
    "            else:\n",
    "                edges = np.linspace(xmin, xmax, nbin+1)\n",
    "\n",
    "            f, ax = plt.subplots(figsize=(12,12))\n",
    "            hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "\n",
    "            for sam in ['vhcc-2L']:\n",
    "                dftmp = df_comp[sam] if presel1=='' else df_comp[sam].query(presel1)\n",
    "                h = get_hist(dftmp[vname1].values, bins=edges, weights=dftmp.eval(wgtstr_vhcc_2L).values)\n",
    "                plot_hist(h, bins=edges, label=label[sam]+' $N_{SV}^{match}\\geq 1$' if sam=='qcd-mg' else label[sam], normed=True)\n",
    "\n",
    "            for sam in ['proxy']:\n",
    "                if (ptmin,ptmax) == (200,100000):\n",
    "                    selclist, suf_label = ['sv+sfbdt500', 'sv+sfbdt800', 'sv+sfbdt900', 'sv+sfbdt950'], ['','','','']\n",
    "                else:\n",
    "                    bdt_seq = df1[f\"bdt_seq_{config['pt_range']['name']}\"][(ptmin,ptmax)]\n",
    "                    selclist = ['sv+sfbdt500'] + [f'sv+sfbdt{int(b*1000)}' for b in [bdt_seq[0], bdt_seq[int((len(bdt_seq)-1)/2)], bdt_seq[-1]]]\n",
    "                    suf_label = ['', ' (lower)', ' (central)', ' (upper)']\n",
    "                for ext, slb in zip(selclist, suf_label):\n",
    "                    cutstr = ' & '.join(list(filter(None, [presel]+[func_basesel(cname)[0] for cname in ext.split('+')]))) ## join the cut string\n",
    "                    if 'qcd-mg' in sam:  print (cutstr)\n",
    "                    dftmp = df_comp[sam].query(cutstr)\n",
    "                    h = get_hist(dftmp[vname].values, bins=edges, weights=dftmp.eval(wgtstr))\n",
    "                    plot_hist(h, bins=edges, label=label[sam]+' '+(rwgt_ext_label if do_rwgt else '')+' & '.join([func_basesel(cname)[1] for cname in ext.split('+')])+slb, normed=True)\n",
    "\n",
    "            ax.legend()\n",
    "            ax.set_xlim((xmin, xmax) if xlim is None else xlim)\n",
    "            ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.', ha='right', y=1.0); \n",
    "            plt.savefig(f'plots/{g_dirname}_{year}_pd/{pfwgt}_{presel}__{savename}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}_pd/{pfwgt}_{presel}__{savename}.pdf')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other comparisons\n",
    "\n",
    "The below function enables one to make a simple comparison with the given sample lists, weight strings, pre-selection strings, and labels.\n",
    "\n",
    "## Standard vs. extra b-enriched sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_comp_plot(df, bininfo, sam_list, wgtstr, presel, label, isnormed=True):\n",
    "    for i in range(len(sam_list)):\n",
    "        if isinstance(sam_list[i], str):\n",
    "            sam_list[i] = [sam_list[i]]\n",
    "\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "    for vname, nbin, xmin, xmax, vlabel in bininfo:\n",
    "        if not isinstance(vname, str): ## savename is specified other then the variable name\n",
    "            savename, vname = vname\n",
    "        else:\n",
    "            savename = vname\n",
    "        if not isinstance(nbin, int):\n",
    "            edges, xmin, xmax, nbin = nbin, min(nbin), max(nbin), len(nbin)\n",
    "        else:\n",
    "            edges = np.linspace(xmin, xmax, nbin+1)\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(12,12))\n",
    "        hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "\n",
    "        for sl, wgt, sel, lab in zip(sam_list, wgtstr, presel, label):\n",
    "            print(sl, wgt, sel, lab)\n",
    "            _df = df[sl[0]].query(sel) if len(sl)==1 else pd.concat([df[sam].query(sel) for sam in sl])\n",
    "            h = get_hist(_df[vname].values, bins=edges, weights=_df.eval(wgt).values if wgt!='1' else np.ones(_df.shape[0]))\n",
    "            plot_hist(h, bins=edges, label=lab, normed=isnormed)\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.' if isnormed else 'Events / bin', ha='right', y=1.0); \n",
    "\n",
    "bininfo = [ #(savename, vname, nbin, xmin, xmax, label)\n",
    "    ('fj_x_btagcsvv2', [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, r'$CSVv2$'),\n",
    "    ('mSV12_ptmax_log', [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, r'$log(m_{SV1,p_{T}\\,max}\\; /GeV)$'),\n",
    "    ('mSV12_dxysig_log', [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, r'$log(m_{SV1,d_{xy}sig\\,max}\\; /GeV)$'),\n",
    "]\n",
    "ptmin, ptmax = 250, 350\n",
    "simple_comp_plot(\n",
    "    df=df1, bininfo=bininfo,\n",
    "    sam_list=[['subst_qcd-mg-noht'],['subst_qcd-mg-bflav-noht']],\n",
    "    wgtstr=['genWeight*xsecWeight*puWeight*htwgt', 'genWeight*xsecWeight*puWeight*htwgt*bflav_htwgt'],\n",
    "    presel=[f'fj_x_nbhadrons>=1 & fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.9']*2,\n",
    "    label=['standard','b-flavor'],\n",
    "    isnormed=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
