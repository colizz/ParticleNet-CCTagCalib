{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook for ParticleNet AK15 cc-tagger SF derivation\n",
    "\n",
    "The notebook aims to\n",
    " - Make the ROOT-format **templates** for fit\n",
    " - Produce **data/MC comparison plots** under some given event selection\n",
    " - Produce **H->cc signal and g->cc proxy jets comparison plots** on various jet observables\n",
    " \n",
    "We adopt the `uproot`+`pandas`* workflow in this notebook, illustrated as follows:\n",
    "\n",
    "    Input files (flat ROOT-tuples derived from analysis NanoAOD)\n",
    "    -> load as `pandas` DataFrame (by `uproot`)\n",
    "    -> manipulate the dataframe\n",
    "    -> produce histograms (`boost_histogram`)\n",
    "    -> (1) convert to TH1D for ROOT template; or (2) plot with `mplhep` using `matplotlib` as backend\n",
    "    \n",
    " \n",
    "(*) Note: this workflow suffers from large RAM usage in the runtime. It may consume 10-30 GB of RAM if dealing with large datasets, hence set requirement to the machine. \n",
    "A smarter workflow would be `coffea` (with `uproot` lazy dataframe as backend) which the future framework is planned to be migrated on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make templates for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boost_histogram as bh\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import pandas as pd\n",
    "use_helvet = True  ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "\n",
    "def get_hist(array, bins=10, xmin=None, xmax=None, underflow=False, overflow=False, mergeflowbin=True, normed=False,\n",
    "            weights=None, **kwargs):\n",
    "    r\"\"\"Plot histogram from input array.\n",
    "\n",
    "    Arguments:\n",
    "        array (np.ndarray): input array.\n",
    "        bins (int, list or tuple of numbers, np.ndarray, bh.axis): bins\n",
    "        weights (None, or np.ndarray): weights\n",
    "        # normed (bool): deprecated.\n",
    "\n",
    "    Returns:\n",
    "        hist (boost_histogram.Histogram)\n",
    "    \"\"\"\n",
    "    if isinstance(bins, int):\n",
    "        if xmin is None:\n",
    "            xmin = array.min()\n",
    "        if xmax is None:\n",
    "            xmax = array.max()\n",
    "        width = 1.*(xmax-xmin)/bins\n",
    "        if mergeflowbin and underflow:\n",
    "            xmin += width\n",
    "            bins -= 1\n",
    "        if mergeflowbin and underflow:\n",
    "            xmax -= width\n",
    "            bins -= 1\n",
    "        bins = bh.axis.Regular(bins, xmin, xmax, underflow=underflow, overflow=overflow)\n",
    "    elif isinstance(bins, (list, tuple, np.ndarray)):\n",
    "        if mergeflowbin and underflow:\n",
    "            bins = bins[1:]\n",
    "        if mergeflowbin and overflow:\n",
    "            bins = bins[:-1]\n",
    "        bins = bh.axis.Variable(bins, underflow=underflow, overflow=overflow)\n",
    "\n",
    "    hist = bh.Histogram(bins, storage=bh.storage.Weight())\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(array)\n",
    "    hist.fill(array, weight=weights)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def plot_hist(hists, normed=False, **kwargs):\n",
    "    r\"\"\"Plot the histogram in the type of boost_histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(hists, (list, tuple)):\n",
    "        hists = [hists]\n",
    "    content = [h.view(flow=True).value for h in hists]\n",
    "    bins = hists[0].axes[0].edges\n",
    "    if 'bins' in kwargs:\n",
    "        bins = kwargs.pop('bins')\n",
    "    if 'yerr' in kwargs:\n",
    "        yerr = kwargs.pop('yerr')\n",
    "    else:\n",
    "        yerr = [np.sqrt(h.view(flow=True).variance) for h in hists]\n",
    "    if normed:\n",
    "        for i in range(len(content)):\n",
    "            contsum = sum(content[i])\n",
    "            content[i] /= contsum\n",
    "            yerr[i] /= contsum\n",
    "    if len(hists) == 1:\n",
    "        content, yerr = content[0], yerr[0]\n",
    "    hep.histplot(content, bins=bins, yerr=yerr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "from uproot_methods import TLorentzVectorArray, TLorentzVector\n",
    "import ROOT\n",
    "import array\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load files\n",
    "\n",
    "Load the ROOT files into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016  ## config me! options: 2016, 2017, 2018\n",
    "\n",
    "lumi = {2016: 35.92, 2017: 41.53, 2018: 59.74}\n",
    "\n",
    "minimal_branches = [  ## minimal set of branches read into the notebook\n",
    "    \"run\", \"luminosityBlock\", \"event\", \"genWeight\", \"jetR\", \"passmetfilters\", \"n_fatjet\", \"fj_1_ParticleNetMD_XbbVsQCD\", \"fj_1_ParticleNetMD_XccVsQCD\", \"fj_1_dr_H\", \"fj_1_dr_Z\", \"fj_1_pt\", \"fj_1_eta\", \"fj_1_phi\", \"fj_1_energy\", \"fj_1_rawmass\", \"fj_1_sdmass\", \"fj_1_tau21\", \"fj_1_btagcsvv2\", \"fj_1_btagjp\", \"fj_1_nsv\", \"fj_1_nsv_ptgt25\", \"fj_1_nsv_ptgt50\", \"fj_1_ntracks\", \"fj_1_ntracks_sv12\", \"fj_1_deltaR_sj12\", \"fj_1_sj1_pt\", \"fj_1_sj1_eta\", \"fj_1_sj1_phi\", \"fj_1_sj1_rawmass\", \"fj_1_sj1_energy\", \"fj_1_sj1_btagdeepcsv\", \"fj_1_sj1_btagcsvv2\", \"fj_1_sj1_btagjp\", \"fj_1_sj1_ntracks\", \"fj_1_sj1_nsv\", \"fj_1_sj1_sv1_pt\", \"fj_1_sj1_sv1_mass\", \"fj_1_sj1_sv1_masscor\", \"fj_1_sj1_sv1_ntracks\", \"fj_1_sj1_sv1_dxy\", \"fj_1_sj1_sv1_dxysig\", \"fj_1_sj1_sv1_dlen\", \"fj_1_sj1_sv1_dlensig\", \"fj_1_sj1_sv1_chi2ndof\", \"fj_1_sj1_sv1_pangle\", \"fj_1_sj2_pt\", \"fj_1_sj2_eta\", \"fj_1_sj2_phi\", \"fj_1_sj2_rawmass\", \"fj_1_sj2_energy\", \"fj_1_sj2_btagdeepcsv\", \"fj_1_sj2_btagcsvv2\", \"fj_1_sj2_btagjp\", \"fj_1_sj2_ntracks\", \"fj_1_sj2_nsv\", \"fj_1_sj2_sv1_pt\", \"fj_1_sj2_sv1_mass\", \"fj_1_sj2_sv1_masscor\", \"fj_1_sj2_sv1_ntracks\", \"fj_1_sj2_sv1_dxy\", \"fj_1_sj2_sv1_dxysig\", \"fj_1_sj2_sv1_dlen\", \"fj_1_sj2_sv1_dlensig\", \"fj_1_sj2_sv1_chi2ndof\", \"fj_1_sj2_sv1_pangle\", \"fj_1_sj12_masscor_dxysig\", \"fj_1_sfBDT\", \"fj_1_nbhadrons\", \"fj_1_nchadrons\", \"fj_1_sj1_nbhadrons\", \"fj_1_sj1_nchadrons\", \"fj_1_sj2_nbhadrons\", \"fj_1_sj2_nchadrons\", \"fj_2_ParticleNetMD_XbbVsQCD\", \"fj_2_ParticleNetMD_XccVsQCD\", \"fj_2_dr_H\", \"fj_2_dr_Z\", \"fj_2_pt\", \"fj_2_eta\", \"fj_2_phi\", \"fj_2_energy\", \"fj_2_rawmass\", \"fj_2_sdmass\", \"fj_2_tau21\", \"fj_2_btagcsvv2\", \"fj_2_btagjp\", \"fj_2_nsv\", \"fj_2_nsv_ptgt25\", \"fj_2_nsv_ptgt50\", \"fj_2_ntracks\", \"fj_2_ntracks_sv12\", \"fj_2_deltaR_sj12\", \"fj_2_sj1_pt\", \"fj_2_sj1_eta\", \"fj_2_sj1_phi\", \"fj_2_sj1_rawmass\", \"fj_2_sj1_energy\", \"fj_2_sj1_btagdeepcsv\", \"fj_2_sj1_btagcsvv2\", \"fj_2_sj1_btagjp\", \"fj_2_sj1_ntracks\", \"fj_2_sj1_nsv\", \"fj_2_sj1_sv1_pt\", \"fj_2_sj1_sv1_mass\", \"fj_2_sj1_sv1_masscor\", \"fj_2_sj1_sv1_ntracks\", \"fj_2_sj1_sv1_dxy\", \"fj_2_sj1_sv1_dxysig\", \"fj_2_sj1_sv1_dlen\", \"fj_2_sj1_sv1_dlensig\", \"fj_2_sj1_sv1_chi2ndof\", \"fj_2_sj1_sv1_pangle\", \"fj_2_sj2_pt\", \"fj_2_sj2_eta\", \"fj_2_sj2_phi\", \"fj_2_sj2_rawmass\", \"fj_2_sj2_energy\", \"fj_2_sj2_btagdeepcsv\", \"fj_2_sj2_btagcsvv2\", \"fj_2_sj2_btagjp\", \"fj_2_sj2_ntracks\", \"fj_2_sj2_nsv\", \"fj_2_sj2_sv1_pt\", \"fj_2_sj2_sv1_mass\", \"fj_2_sj2_sv1_masscor\", \"fj_2_sj2_sv1_ntracks\", \"fj_2_sj2_sv1_dxy\", \"fj_2_sj2_sv1_dxysig\", \"fj_2_sj2_sv1_dlen\", \"fj_2_sj2_sv1_dlensig\", \"fj_2_sj2_sv1_chi2ndof\", \"fj_2_sj2_sv1_pangle\", \"fj_2_sj12_masscor_dxysig\", \"fj_2_sfBDT\", \"fj_2_nbhadrons\", \"fj_2_nchadrons\", \"fj_2_sj1_nbhadrons\", \"fj_2_sj1_nchadrons\", \"fj_2_sj2_nbhadrons\", \"fj_2_sj2_nchadrons\", \"passHTTrig\", \"ht\", \"nlep\", \"fj_1_is_qualified\", \"fj_2_is_qualified\", \"puWeight\", \"puWeightUp\", \"puWeightDown\", \"xsecWeight\"\n",
    "]\n",
    "ext_hlt_branches = {  ## extra branches depend on year\n",
    "    2016: ['HLT_PFHT125', 'HLT_PFHT200', 'HLT_PFHT250', 'HLT_PFHT300', 'HLT_PFHT350', 'HLT_PFHT400', 'HLT_PFHT475', 'HLT_PFHT600', 'HLT_PFHT650', 'HLT_PFHT800', 'HLT_PFHT900'],\n",
    "    2017: ['HLT_PFHT180', 'HLT_PFHT250', 'HLT_PFHT370', 'HLT_PFHT430', 'HLT_PFHT510', 'HLT_PFHT590', 'HLT_PFHT680', 'HLT_PFHT780', 'HLT_PFHT890', 'HLT_PFHT1050', 'HLT_PFHT350'],\n",
    "    2018: ['HLT_PFHT180', 'HLT_PFHT250', 'HLT_PFHT370', 'HLT_PFHT430', 'HLT_PFHT510', 'HLT_PFHT590', 'HLT_PFHT680', 'HLT_PFHT780', 'HLT_PFHT890', 'HLT_PFHT1050', 'HLT_PFHT350'],\n",
    "}\n",
    "minimal_branches += ext_hlt_branches[year]\n",
    "minimal_branches += ['nPSWeight', 'PSWeight'] if year==2018 else []  ## extra PSWeight branches for 2018\n",
    "minimal_branches_for_data = set(minimal_branches) - set(['genWeight',\"puWeight\", \"puWeightUp\", \"puWeightDown\", \"xsecWeight\", 'nPSWeight', 'PSWeight',\n",
    "                                'fj_1_nchadrons', 'fj_1_nbhadrons','fj_2_nbhadrons','fj_1_sj1_nbhadrons','fj_2_sj1_nbhadrons','fj_1_sj2_nbhadrons','fj_2_sj2_nbhadrons',\n",
    "                                'fj_2_nchadrons','fj_1_sj1_nchadrons','fj_2_sj1_nchadrons','fj_1_sj2_nchadrons','fj_2_sj2_nchadrons'])\n",
    "\n",
    "## Read into pandas DataFrame\n",
    "_df0 = {}\n",
    "_df0['qcd-mg-noht'] = uproot.open(f\"samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/qcd-mg_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['qcd-herwig-noht'] = uproot.open(f\"samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/qcd-herwig_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['top-noht'] = uproot.open(f\"samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/top_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['v-qq-noht'] = uproot.open(f\"samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/v-qq_tree.root\")['Events'].pandas.df(minimal_branches, flatten=False)\n",
    "_df0['jetht-noht'] = uproot.open(f\"samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/data/jetht_tree.root\")['Events'].pandas.df(minimal_branches_for_data, flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing\n",
    "\n",
    "For data: apply OR of all HT trigger to enhance statistics.\n",
    "\n",
    "For MC: apply no HT trigger, based on the strategy we name it \"MC substitute\".\n",
    "\n",
    "The initial dataframe (`_df0`) is event-based, but for the purpose of fit we transform the dataframe to be jet-based. \n",
    "The new dataframe `df1` contains branches `fj_x_` that either come from `fj_1_` or `fj_2_` passing the corresponding jet-based creteria (pT>200, each subjet matched to >=1 SV, sfBDT>0.5) carried by `fj_?_is_qualified` (?=1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ Pre-processing for data  ===================\n",
    "\n",
    "## Baseline selection applied to data. \n",
    "## Note that we use the OR or all HT triggers (some are pre-scaled triggers)\n",
    "htcut_incl = '('+' | '.join(ext_hlt_branches[year])+')'\n",
    "basesel_noht_prep = f\"passmetfilters & {htcut_incl} & fj_x_pt>200 & fj_x_is_qualified\"\n",
    "sl_prep = ['jetht-noht']\n",
    "df1 = {}\n",
    "for sam in sl_prep:\n",
    "    assert 'noht' in sam\n",
    "    ## To concatenate event lists where either fj_1 is qualified OR fj_2 is qualified\n",
    "    fj_branches = [key.replace('fj_2', 'fj_x') for key in _df0[sam].keys() if (key.startswith('fj_2') and key!='fj_2_is_qualified')]  ## all fj_2_ branches expect fj_2_is_qualified\n",
    "    for i, i_inv in zip(['1','2'], ['2','1']):\n",
    "        df1[sam + i] = _df0[sam].query(basesel_noht_prep.replace('fj_x', f'fj_{i}'))  ## select events where fj_1/fj_2 is qualified\n",
    "        df1[sam + i].drop(columns=[key.replace('fj_x', f'fj_{i_inv}') for key in fj_branches], inplace=True)  ## drop fj branches for the other index\n",
    "        df1[sam + i].rename(columns={key.replace('fj_x', f'fj_{i}'): key for key in fj_branches}, inplace=True)  ## change branches name from fj_1/fj_2 to a unified name fj_x\n",
    "        df1[sam + i].loc[:, 'fj_idx'] = int(i)  ## label the jet index\n",
    "        df1[sam + i].loc[:, 'is_qcd'] = True if 'qcd' in sam else False\n",
    "    df1[sam] = pd.concat([df1[sam + '1'], df1[sam + '2']])\n",
    "    del df1[sam + '1'], df1[sam + '2']\n",
    "    del _df0[sam]  # to release memory usage if necessary\n",
    "\n",
    "## Produce new variables used for fit\n",
    "for sam in sl_prep:\n",
    "    df1[sam]['mSV12_ptmax'] = df1[sam].eval('(fj_x_sj1_sv1_pt>fj_x_sj2_sv1_pt)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_pt<=fj_x_sj2_sv1_pt)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_ptmax_log'] = df1[sam].eval('log(mSV12_ptmax)')\n",
    "    df1[sam]['mSV12_dxysig'] = df1[sam].eval('(fj_x_sj1_sv1_dxysig>fj_x_sj2_sv1_dxysig)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_dxysig<=fj_x_sj2_sv1_dxysig)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_dxysig_log'] = df1[sam].eval('log(mSV12_dxysig)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FOR TEST: to see data HT distributions passing different HT pre-scaled trigger\n",
    "# for hlt in ext_hlt_branches[year]:\n",
    "#     dftmp = _df0['jetht-noht'].query(hlt)\n",
    "#     h = get_hist(dftmp['ht'].values, bins=np.linspace(0, 2000, 201), weights=np.ones(dftmp.shape[0]))\n",
    "#     plot_hist(h, label=hlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR TEST: check the xsecWeight for MG samples & genWeight for Herwig sample (to avoid extremely large values) \n",
    "from collections import Counter\n",
    "print(Counter(_df0['qcd-mg-noht']['xsecWeight']),'\\n')\n",
    "for i in [0.96, 0.98, 0.99]:\n",
    "    print(_df0['qcd-herwig-noht']['genWeight'].quantile(q=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ Pre-processing for MC substitute  ===================\n",
    "\n",
    "## Baseline selection applied to MC.\n",
    "## No HT trigger is applied, based on the \"MC substitute\" strategy\n",
    "basesel_noht_prep_subst = \"passmetfilters & fj_x_pt>200 & fj_x_is_qualified\"\n",
    "sl_prep_subst = ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht']  ## mark sample name with \"subst_\" as a reminder of MC substitute\n",
    "for sam in sl_prep_subst:\n",
    "    assert 'noht' in sam\n",
    "    ## To concatenate event lists where fj_1 is qualified OR fj_2 is qualified. Same procedure here\n",
    "    fj_branches = [key.replace('fj_2', 'fj_x') for key in _df0[sam.replace('subst_','')].keys() if (key.startswith('fj_2') and key!='fj_2_is_qualified')]  ## all fj_2_ branches expect fj_2_is_qualified\n",
    "    for i, i_inv in zip(['1','2'], ['2','1']):\n",
    "        df1[sam + i] = _df0[sam.replace('subst_','')].query(basesel_noht_prep_subst.replace('fj_x', f'fj_{i}'))\n",
    "        df1[sam + i].drop(columns=[key.replace('fj_x', f'fj_{i_inv}') for key in fj_branches], inplace=True)\n",
    "        df1[sam + i].rename(columns={key.replace('fj_x', f'fj_{i}'): key for key in fj_branches}, inplace=True)\n",
    "        df1[sam + i].loc[:, 'fj_idx'] = int(i)\n",
    "        df1[sam + i].loc[:, 'is_qcd'] = True if 'qcd' in sam else False\n",
    "        if sam == 'subst_qcd-mg-noht':\n",
    "            df1[sam + i].query('xsecWeight<5.', inplace=True)  ## drop MG events with extremely large xsecWeight (coming from low HT sample in the HT-binned MG list)\n",
    "        if sam == 'subst_qcd-herwig-noht':\n",
    "            df1[sam + i].query('genWeight<{}'.format(_df0['qcd-herwig-noht']['genWeight'].quantile(q=0.96)), inplace=True)  ## drop Herwig events with extremely large genWeight\n",
    "        if year == 2016 and sam == 'subst_qcd-herwig-noht':\n",
    "            df1[sam + i].loc[:, 'xsecWeight'] = df1[sam + i]['xsecWeight'] * 2400.  ## fix a 2016 bug: Herwig sample xsec is mistaken\n",
    "    df1[sam] = pd.concat([df1[sam + '1'], df1[sam + '2']])\n",
    "    del df1[sam + '1'], df1[sam + '2']\n",
    "    del _df0[sam.replace('subst_','')]  # to release memory usage if necessary\n",
    "\n",
    "## Produce new variables used for fit\n",
    "for sam in sl_prep_subst:\n",
    "    df1[sam]['mSV12_ptmax'] = df1[sam].eval('(fj_x_sj1_sv1_pt>fj_x_sj2_sv1_pt)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_pt<=fj_x_sj2_sv1_pt)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_ptmax_log'] = df1[sam].eval('log(mSV12_ptmax)')\n",
    "    df1[sam]['mSV12_dxysig'] = df1[sam].eval('(fj_x_sj1_sv1_dxysig>fj_x_sj2_sv1_dxysig)*fj_x_sj1_sv1_masscor + (fj_x_sj1_sv1_dxysig<=fj_x_sj2_sv1_dxysig)*fj_x_sj2_sv1_masscor')\n",
    "    df1[sam]['mSV12_dxysig_log'] = df1[sam].eval('log(mSV12_dxysig)')\n",
    "\n",
    "    ## PSWeight variables exclusive to 2018 datasets\n",
    "    if year==2018:\n",
    "        if df1[sam]['nPSWeight'].iloc[0] == 1:\n",
    "            df1[sam]['PSWeight1'] = df1[sam]['PSWeight2'] = df1[sam]['PSWeight3'] = df1[sam]['PSWeight4'] = df1[sam]['PSWeight']\n",
    "        else:\n",
    "            assert all(df1[sam]['nPSWeight'] == 4)\n",
    "            for i in range(4):\n",
    "                df1[sam][f'PSWeight{i+1}'] = df1[sam]['PSWeight'].map(lambda x: x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain reweight factors\n",
    "\n",
    "We extract the following reweight factors. The first two sets are used in the nominal fit. The other two are for validation.\n",
    " 1. **MC substitute-to-data reweight factor**: on the HT variable based on (pT, jet index) bins. The goal is to bring the shape of MC substitute back to the data shape in the inclusive region. Remember that the raw MC substitute yield is always much larger than data, because most HT triggers applied to data are pre-scaled triggers. New variables have the name `htwgt_(|herwig)`.\n",
    " 2. **sfBDT reweight factor**: based on (pT, jet index) bins, to further reweight MC substitute back to data shape on the sfBDT variable. Since sfBDT>0.9 is imposed in the final fit region, the sfBDT shape discrepancy between the \"reweighted MC substitute\" and data may again cause $N_{total}$ difference for MC and data, after setting sfBDT>0.9 in the fit region. Therefore, we calculate the overall factor `sfbdtwgt_g90_(|herwig)_incl` in each (pT, jet index) bin, used in the nominal shape template; and the binned factor `sfbdtwgt_g90_(|herwig)_binned` used in the shape uncertainty extraction brought by the sfBDT shape mismodeling\n",
    " 3. **Additional MC substitute-to-data reweight factor on $p_{T}$ only**: A possible replacement of the first two factors combined. This factor is only used in the validation fit. The goal for this validation is to check if different reweighting schemes may affect the SF fit results. New variables have the name `ad_ptwgt_(|herwig)`.\n",
    " 4. **Proxy-to-signal reweight factor on $m_{SD}$ / $p_{T}$ / $\\tau_{21}$**: based on the shape of \"reweighted MC substitute (after the first two steps)\" and the H->cc signal jet shape in the inclusive region. The factor is only used in the validation fit, in which we apply such reweight factor to both MC substitute and data to check if the SF results are affected. New variables have the name `(mass|pt|tau21)datamcwgt_(|herwig)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 1. Reweight MC subsitute to data: stored as variable \"htwgt\", \"htwgt_herwig\") ===================\n",
    "\n",
    "## True: if the block has run before, we can obtain the reweight factor from the previously stored pickle output\n",
    "is_read_from_pickel = False\n",
    "\n",
    "def extract_mc_to_data_ht_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor on HT based on (pT, jet index) bins\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    rwgt_var = 'ht'\n",
    "    ## The binning info for (pT, HT) grid. Note that 2016 is different from 2017/18. The adopted HT grid is based on MC shape in each pT bin\n",
    "    rwgt_edge_dic = {}\n",
    "    rwgt_edge_dic[2016] = {\n",
    "        'pt200to250': [300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100],\n",
    "        'pt250to300': [350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1300],\n",
    "        'pt300to350': [450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1350],\n",
    "        'pt350to400': [550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "        'pt400to500': [600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "        'pt500toInf': [800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "    }\n",
    "    rwgt_edge_dic[2017] = rwgt_edge_dic[2018] = {\n",
    "#         'pt200to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200], # deprecated\n",
    "#         'pt300to400': [500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1600], # deprecated\n",
    "        'pt200to250': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 900, 1000],\n",
    "        'pt250to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200],\n",
    "        'pt300to350': [450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "        'pt350to400': [550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1600],\n",
    "        'pt400to500': [700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1800],\n",
    "        'pt500toInf': [900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "    }\n",
    "    for sam in sl_rwgt:\n",
    "        df1[sam][wgtname] = np.nan  ## initially fill the output column with NaN\n",
    "\n",
    "    if is_read_from_pickel: ## restore info from a previously stored pickle\n",
    "        import pickle\n",
    "        with open(f'plots/wgtv5/htwgt_{year}.pickle', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            res = res[0] if 'herwig' not in wgtname else res[1]\n",
    "            ent_data, ent_mc, rwgt = res['ent_data'], res['ent_mc'], res['rwgt']\n",
    "    else:\n",
    "        ent_data, ent_mc, rwgt = {}, {}, {}\n",
    "        \n",
    "    ## Rewight separately on jet pT bins\n",
    "    for ptsel, ptlab in zip(['fj_x_pt>=200 & fj_x_pt<250', 'fj_x_pt>=250 & fj_x_pt<300', 'fj_x_pt>=300 & fj_x_pt<350', 'fj_x_pt>=350 & fj_x_pt<400', 'fj_x_pt>=400 & fj_x_pt<500', 'fj_x_pt>=500'], \n",
    "                            ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf']):\n",
    "        ## Reweight separately for 1st or 2nd jet\n",
    "        for sel, lab in zip(['fj_idx==1', 'fj_idx==2'], ['jet1', 'jet2']):\n",
    "            print (' -- ', ptsel, sel)\n",
    "            rwgt_edge = rwgt_edge_dic[year][ptlab]\n",
    "            if not is_read_from_pickel:\n",
    "                ## Calculate the rwgt for the first time\n",
    "                _dffdata = df1['jetht-noht'].query(f'{ptsel} & {sel}')\n",
    "                _dffmc =  pd.concat([df1[sam].query(f'{ptsel} & {sel}') for sam in sl_rwgt])  ## concat all MC substitute sample\n",
    "                \n",
    "                ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "                ent_data[ptlab+lab] = get_hist(_dffdata[rwgt_var].values, bins=rwgt_edge, weights=np.ones(_dffdata.shape[0]), underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "                ent_mc[ptlab+lab]  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc.eval(wgtstr_rwgt).values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "                ## Calculate the reweight factor\n",
    "                rwgt[ptlab+lab] = ent_data[ptlab+lab] / ent_mc[ptlab+lab] # len=nbin+2\n",
    "            print(ent_data[ptlab+lab], '\\n', rwgt[ptlab+lab])\n",
    "            \n",
    "            ## assign the reweight factor to the new column\n",
    "            for sam in sl_rwgt:\n",
    "                df1sel = df1[sam].eval(f'{ptsel} & {sel}')\n",
    "                df1[sam].loc[df1sel, wgtname] = df1[sam].loc[df1sel, rwgt_var].map(lambda val: rwgt[ptlab+lab][sum(np.array(rwgt_edge)<=val)] )\n",
    "    \n",
    "    ## check all entries are filled with valid factors\n",
    "    assert any([any(df1[sam][wgtname] == np.nan) for sam in sl_rwgt]) == False\n",
    "\n",
    "    # =========== plot ===========\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "    for ptlab in ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf']:\n",
    "        f, ax = plt.subplots(figsize=(11,11))\n",
    "        hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "        for lab in ['jet1', 'jet2']:\n",
    "            hep.histplot(ent_data[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][ptlab])+[2500], label=f'Data ({lab})')\n",
    "            hep.histplot(ent_mc[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][ptlab])+[2500], label=f'MC subst. ({lab})')\n",
    "        ax.set_xlim(0, 2500); ax.set_xlabel('$H_{T}$ [GeV]', ha='right', x=1.0); ax.set_ylabel('Events / bin', ha='right', y=1.0); ax.legend()\n",
    "        if not os.path.exists('plots/wgtv5'):\n",
    "            os.makedirs('plots/wgtv5')\n",
    "        plt.savefig(f'plots/wgtv5/{year}_{ptlab}__{wgtname}.pdf')\n",
    "        plt.savefig(f'plots/wgtv5/{year}_{ptlab}__{wgtname}.png')\n",
    "    # ============================\n",
    "    \n",
    "    return {'ent_data':ent_data, 'ent_mc':ent_mc, 'rwgt':rwgt}\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "htwgt = extract_mc_to_data_ht_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'],     wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='htwgt')\n",
    "htwgt_herwig = extract_mc_to_data_ht_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='htwgt_herwig')\n",
    "\n",
    "if not is_read_from_pickel: ## store the info for the first run\n",
    "    import pickle\n",
    "    with open(f'plots/wgtv5/htwgt_{year}.pickle', 'wb') as fw:\n",
    "        pickle.dump([htwgt, htwgt_herwig], fw)\n",
    "\n",
    "df1['subst_qcd-mg-noht'][['ht', 'fj_x_pt', 'fj_idx', 'htwgt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 2. Extract the sfBDT>0.9 overall factor and binned fractor: stored as variable \"sfbdtwgt_g90_incl\", \"sfbdtwgt_g90_binned\"; similar for herwig ===================\n",
    "\n",
    "def extract_further_sfbdt_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname_binned, wgtname_incl):\n",
    "    r\"\"\"Extract the \"MC substitute to data\" reweight factor (both overall and binned factor) further on sfBDT variable, after a sfBDT>0.9 selection\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitute in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname_binned: the reweight name (the binned factors) stored as a new column\n",
    "        wgtname_incl: the reweight name (the overall factor) stored as a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    for sam in sl_rwgt:\n",
    "        df1[sam][wgtname_binned] = np.nan  ## initially fill the output column with NaN\n",
    "        df1[sam][wgtname_incl] = np.nan\n",
    "    \n",
    "    ## Reweight based on the sfBDT variable\n",
    "    rwgt_var, nbin, xmin, xmax  = 'fj_x_sfBDT', 5, 0.9, 1.\n",
    "    print('rwgt sfBDT bins: ', rwgt_var, nbin, xmin, xmax)\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Rewight separately on jet pT bins\n",
    "    for pt_range in [(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000)]:\n",
    "        ## Requires the selection sfBDT>0.9 which is used in the fit region\n",
    "        rwgt_sel = f'fj_x_sfBDT>0.9 & fj_x_pt>={pt_range[0]} & fj_x_pt<{pt_range[1]}'; print(rwgt_sel)\n",
    "        _dffdata = df1['jetht-noht'].query(rwgt_sel)\n",
    "        _dffmc =  pd.concat([df1[sam].query(rwgt_sel) for sam in sl_rwgt])\n",
    "        \n",
    "        ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "        ent_data = get_hist(_dffdata[rwgt_var].values, bins=rwgt_edge, weights=np.ones(_dffdata.shape[0]), underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ent_mc  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc.eval(wgtstr_rwgt).values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ## Calculate the reweight factor\n",
    "        rwgt = ent_data / ent_mc # len=nbin+2\n",
    "        \n",
    "        ## assign the reweight factor to the new column\n",
    "        for sam in sl_rwgt:\n",
    "            df1[sam].loc[df1[sam].eval(rwgt_sel), wgtname_binned] = df1[sam].query(rwgt_sel)[rwgt_var].map(lambda val: rwgt[sum(np.array(rwgt_edge)<=val)] )\n",
    "            rwgt_sel_nobdt = f'fj_x_pt>={pt_range[0]} & fj_x_pt<{pt_range[1]}'\n",
    "            df1[sam].loc[df1[sam].eval(rwgt_sel_nobdt), wgtname_incl] = sum(ent_data) / sum(ent_mc)\n",
    "        print (ent_data, rwgt, 'incl:', sum(ent_data) / sum(ent_mc))\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_further_sfbdt_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt\",\n",
    "                             wgtname_binned='sfbdtwgt_g90_binned', wgtname_incl='sfbdtwgt_g90_incl')\n",
    "extract_further_sfbdt_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig\",\n",
    "                             wgtname_binned='sfbdtwgt_g90_herwig_binned', wgtname_incl='sfbdtwgt_g90_herwig_incl')\n",
    "\n",
    "assert any([any(np.isnan(df1[sam].query(f'fj_x_sfBDT>0.9')[['sfbdtwgt_g90_binned','sfbdtwgt_g90_incl']].values.flatten())) for sam in ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht']]) == False\n",
    "assert any([any(np.isnan(df1[sam].query(f'fj_x_sfBDT>0.9')[['sfbdtwgt_g90_herwig_binned','sfbdtwgt_g90_herwig_incl']].values.flatten())) for sam in ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht']]) == False\n",
    "\n",
    "df1['subst_qcd-mg-noht'][['fj_x_pt', 'fj_idx', 'fj_x_sfBDT', 'sfbdtwgt_g90_incl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3. [additional] Reweight MC subsitute to data on pT: stored as variable \"ad_ptwgt\", \"ad_ptwgt_herwig\" ===================\n",
    "\n",
    "def extract_mc_to_data_pt_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor on pT as a optional choice\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply simple 1D reweight to pT\n",
    "    rwgt_var, nbin, xmin, xmax  = 'fj_x_pt', 20, 200., 1200.\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Rewight separately on 1st/2nd jet\n",
    "    for sel, lab in zip(['fj_idx==1', 'fj_idx==2'], ['jet1', 'jet2']):\n",
    "        _dffdata = df1['jetht-noht'].query(f'fj_x_sfBDT>0.9 & {sel}')\n",
    "        _dffmc =  pd.concat([df1[sam].query(f'fj_x_sfBDT>0.9 & {sel}') for sam in sl_rwgt])\n",
    "        \n",
    "        ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "        ent_data = get_hist(_dffdata[rwgt_var].values, bins=rwgt_edge, weights=np.ones(_dffdata.shape[0]), underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ent_mc  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc.eval(wgtstr_rwgt).values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ## Calculate the reweight factor\n",
    "        rwgt = ent_data / ent_mc # len=nbin+2\n",
    "        \n",
    "        ## assign the reweight factor to the new column\n",
    "        for sam in sl_rwgt:\n",
    "            df1sel = df1[sam].eval(sel)\n",
    "            df1[sam].loc[df1sel, wgtname] = df1[sam].loc[df1sel, rwgt_var].map(lambda val: rwgt[int(max(0, min(nbin+1, np.floor((val-1.*xmin)/(1.*xmax-xmin)*nbin) +1 )))] )\n",
    "        print (ent_data, rwgt)\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_mc_to_data_pt_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'],     wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\",        wgtname='ad_ptwgt')\n",
    "extract_mc_to_data_pt_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='ad_ptwgt_herwig')\n",
    "\n",
    "df1['subst_qcd-mg-noht'][['ht', 'fj_x_pt', 'fj_idx', 'htwgt', 'sfbdtwgt_g90_incl', 'ad_ptwgt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 4. [additional] Reweight MC (proxy jet) to H->cc signal jet on either mass/pT/tau21: stored as variable \"(mass|pt|tau21)datamcwgt\"; similar for herwig  ===================\n",
    "\n",
    "# First load the h->cc signal ntuple. Adopt the selction used in the analysis\n",
    "_df0['vhcc-2L'] = uproot.open(\"samples/trees/20200906_VH_extfillsv_2016_2L/mc/vhcc_tree.root\")['Events'].pandas.df()\n",
    "\n",
    "boosted = \"v_pt>200 & ak15_pt>200 & dphi_V_ak15>2.5 & ak15_sdmass>50 & ak15_sdmass<200\"\n",
    "basecut_vhcc_2L = \"v_mass>75 & v_mass<105 & ((abs(lep1_pdgId)==11 & passTrigEl) | (abs(lep1_pdgId)==13 & passTrigMu)) & \" + boosted + \" & n_ak4<3\"\n",
    "df_comp = {}\n",
    "df_comp['vhcc-2L'] = _df0['vhcc-2L'].query(basecut_vhcc_2L)\n",
    "\n",
    "def extract_mc_to_signal_weight(df1, sl_rwgt, wgtstr_rwgt, wgtname, rwgt_info):\n",
    "    r\"\"\"Extract the \"MC subsisute (proxy) to H->cc signal jet\" reweight factor on possible variable\n",
    "    \n",
    "    Arguments:\n",
    "        df1: DataFrame as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "        rwgt_info: variable and binning info for this reweighting routine\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reweight info extracted from the function argument\n",
    "    rwgt_var, nbin, xmin, xmax, rwgt_var_nom  = rwgt_info\n",
    "    print('rwgt info: ', rwgt_var, nbin, xmin, xmax)\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Requires the selection sfBDT>0.9 which is used in the fit region\n",
    "    rwgt_sel = 'fj_x_sfBDT>0.9'\n",
    "    \n",
    "    ## Get MC and h->cc signal histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "    _dffmc =  pd.concat([df1[sam].query(rwgt_sel) for sam in sl_rwgt])\n",
    "    _dffmc_wgt = _dffmc.eval(wgtstr_rwgt)\n",
    "    ent_mc  = get_hist(_dffmc[rwgt_var].values, bins=rwgt_edge, weights=_dffmc_wgt.values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "    yield_mc = _dffmc_wgt.sum()\n",
    "    _dffhcc_wgt = df_comp['vhcc-2L'].eval('genWeight*xsecWeight*puWeight')\n",
    "    ent_hcc  = get_hist(df_comp['vhcc-2L'][rwgt_var_nom].values, bins=rwgt_edge, weights=_dffhcc_wgt.values, underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "    yield_hcc = _dffhcc_wgt.sum()\n",
    "    \n",
    "    ## Calculate the reweight factor, and clip to (0, 50)\n",
    "    rwgt = (ent_hcc/yield_hcc) / (ent_mc/yield_mc) # len=nbin+2\n",
    "    rwgt = np.clip(rwgt, 0, 50)\n",
    "    \n",
    "    ## assign the reweight factor to the new column (to both MC and data)\n",
    "    for sam in sl_rwgt + ['jetht-noht']:\n",
    "        df1[sam][wgtname] = df1[sam][rwgt_var].map(lambda val: rwgt[int(max(0, min(nbin+1, np.floor((val-1.*xmin)/(1.*xmax-xmin)*nbin) +1 )))] )\n",
    "    print (ent_hcc, rwgt)\n",
    "\n",
    "## For each reweight variable, calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt*sfbdtwgt_g90_incl\",\n",
    "                            wgtname='massdatamcwgt', rwgt_info=('fj_x_sdmass', 15, 50, 200, 'ak15_sdmass'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig*sfbdtwgt_g90_herwig_incl\",\n",
    "                            wgtname='massdatamcwgt_herwig', rwgt_info=('fj_x_sdmass', 15, 50, 200, 'ak15_sdmass'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt*sfbdtwgt_g90_incl\",\n",
    "                            wgtname='ptdatamcwgt', rwgt_info=('fj_x_pt', 20, 200, 1200, 'ak15_pt'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig*sfbdtwgt_g90_herwig_incl\",\n",
    "                            wgtname='ptdatamcwgt_herwig', rwgt_info=('fj_x_pt', 20, 200, 1200, 'ak15_pt'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt*sfbdtwgt_g90_incl\",\n",
    "                            wgtname='tau21datamcwgt', rwgt_info=('fj_x_tau21', 20, 0, 1, 'ak15_tau21'))\n",
    "extract_mc_to_signal_weight(df1, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*htwgt_herwig*sfbdtwgt_g90_herwig_incl\",\n",
    "                            wgtname='tau21datamcwgt_herwig', rwgt_info=('fj_x_tau21', 20, 0, 1, 'ak15_tau21'))\n",
    "\n",
    "df1['jetht-noht'][['fj_x_sdmass', 'massdatamcwgt', 'fj_x_pt', 'ptdatamcwgt', 'fj_x_tau21', 'tau21datamcwgt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make ROOT templates\n",
    "\n",
    "We produce the ROOT templates using the DataFrame in this step. The outputs are ROOT files with neat structure. After the further reorganization, they can be used as the Higgs Combine input to implement the fit.\n",
    "\n",
    "As a reference, we provide an example of the output files and their structure. \n",
    "E.g., for a **given fit variable**, **given tagger WP** and a **certain jet-pT bin** for **a single fit**, the output ROOT templates should include the pass and fail MC template in the B/C/L flavors, the data template, and the MC systematics for all specified shape uncertainties. The files are organized in the following structure:\n",
    "```\n",
    "─── 20201115_SF2017_AK15_qcd_subst_pst_ptw50_TP_msv12_dxysig_log_var22binsv2  [use variable: msv12_dxysig_log, Tight WP]\n",
    "    └── Cards\n",
    "        └── bdt900\n",
    "            ├── pt200to250                 [given pT bin]\n",
    "            │   ├── nominal                    [the nominal histograms]\n",
    "            │   │   ├── inputs_fail.root           [include four TH1D: flvC, flvB, flvL, data_obs]\n",
    "            │   │   └── inputs_pass.root           [..]\n",
    "            │   ├── fracBBDown                 [shape uncertainty plots]\n",
    "            │   │   ├── inputs_fail.root           [include three TH1D: flvC_fracBBDown, flvB_fracBBDown, flvL_fracBBDown]\n",
    "            │   │   └── inputs_pass.root           [..]\n",
    "            │   ├── fracBBUp                   [..]\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracCCDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracCCUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracLightDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracLightUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightFsrDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightFsrUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightIsrDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightIsrUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── puDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── puUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdKdeSystDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdKdeSystUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdSystDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdSystUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── sfBDTFloAroundDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── sfBDTFloAroundUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── sfBDTRwgtDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   └── sfBDTRwgtUp\n",
    "            │       ├── inputs_fail.root\n",
    "            │       └── inputs_pass.root\n",
    "            ├── pt250to300\n",
    "            │   ├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template making is organized in three nested functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ================================ Global parameters: config me! ================================ ####\n",
    "g_make_template_mode = 'main'\n",
    "r\"\"\"Options:\n",
    "        main           : the main fit\n",
    "        val_pt         : the validation fit -- to use an optional MC subsitute-to-data strategy, i.e. on pT variable only\n",
    "        val_tosig_mass : the validation fit -- additionally reweight MC & data to h->cc signal jet on mass\n",
    "        val_tosig_pt   : the validation fit -- additionally reweight MC & data to h->cc signal jet on pt  \n",
    "        val_tosig_tau21: the validation fit -- additionally reweight MC & data to h->cc signal jet on tau21\n",
    "        val_vary_sfbdt : the validation fit -- varying sfBDT cut value and drop sfBDT* uncertaint\n",
    "        val_crop_bin   : the validation fit -- cropping the marginal bins for fit\n",
    "\"\"\"\n",
    "\n",
    "g_outdir_prefix = f'20201115_SF{year}_AK15_qcd_subst_pst_ptw50'\n",
    "r\"\"\"Prefix for the output dir name \"\"\"\n",
    "\n",
    "g_make_unce_types = {'nominal':True, 'pu':True, 'fracBB':True, 'fracCC':True, 'fracLight':True, 'psWeightIsr':False, 'psWeightFsr':False, 'sfBDTRwgt':True, 'sfBDTFloAround':True}\n",
    "r\"\"\"The uncertainty types used in the fit. Use False or remove the key to disable an certain unce type\n",
    "    Note: \"qcdSyst\" and \"qcdKdeSyst\" is not used in this verision. \"psWeightIsr\" and \"psWeightFsr\" works fine in 2018 while in 2016/17 one need to first garantee the 2018 histograms exist\n",
    "          so the unce can be transferred.\n",
    "\"\"\" # for test, we disable psWeightIsr/Fsr\n",
    "\n",
    "g_do_fit_for = { # for test, we launch the main fit var (1) only\n",
    "    1: ['TP', 'MP', 'LP'],\n",
    "#     2: ['TP', 'MP', 'LP'],\n",
    "#     3: ['TP', 'MP', 'LP'],\n",
    "}\n",
    "r\"\"\" Do fit for which variable and which WPs\"\"\"\n",
    "#### =============================================================================================== ####\n",
    "\n",
    "## Consistency check for gloal params\n",
    "if g_make_template_mode not in ['main', 'val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21', 'val_vary_sfbdt', 'val_crop_bin']:\n",
    "    raise RuntimeError('Specified mode cannot be recognized.')\n",
    "if g_make_template_mode in ['val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21', 'val_vary_sfbdt'] and list(g_do_fit_for.keys()) != [1]:\n",
    "    print('Warning: for validation fit, set the fit information to the main variable (1) only')\n",
    "    g_do_fit_for = {1: ['TP', 'MP', 'LP']}\n",
    "if g_make_template_mode == 'val_crop_bin' and list(g_do_fit_for.keys()) != [901]:\n",
    "    print('Warning: for validation fit on cropping the marginal bins, set the fit information to the cropped main variable (901) only')\n",
    "    g_do_fit_for = {901: ['TP', 'MP', 'LP']}\n",
    "if g_make_template_mode == 'val_vary_sfbdt':\n",
    "    g_make_unce_types.pop('sfBDTRwgt', None)\n",
    "    g_make_unce_types.pop('sfBDTFloAround', None)\n",
    "    \n",
    "## The sfBDT varing list. \n",
    "## Note: to implement sfBDTFloAround unce, one must first obtain the nominal hist for the cut value 0.85, 0.95\n",
    "if g_make_template_mode != 'val_vary_sfbdt':\n",
    "    g_sfBDT_val_list = [0.85, 0.95, 0.9]\n",
    "else:\n",
    "    g_sfBDT_val_list = [0.84, 0.86, 0.88, 0.90, 0.92, 0.94] ## for validation: varying sfBDT\n",
    "    \n",
    "\n",
    "## Fit info: in the format of [ (fit var, nbins/edges, xmin/None, xmax/None, (underflow, overflow), label), outputdir lambda func ]\n",
    "g_fitinfo = {\n",
    "    1: [ ##  main fit var\n",
    "        ('mSV12_dxysig_log', [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, (True, True), 'mSV12_dxysig_log'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_msv12_dxysig_log_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "    2: [ ## the other var for validation\n",
    "        ('mSV12_ptmax_log', [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, (True, True), 'mSV12_ptmax_log'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_msv12_ptmax_log_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "    3: [ ## the other var for validation\n",
    "        ('fj_x_btagcsvv2', [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, (True, True), 'CSVv2'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_csvv2_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "    901: [ ## crop the marginal bins for the main var as a validation\n",
    "        ('mSV12_dxysig_log', [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8], None, None, (False, False), 'mSV12_dxysig_log'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_msv12_dxysig_log_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "}\n",
    "g_hist_qcdsyst = {}\n",
    "\n",
    "\n",
    "## Tagger values in use\n",
    "g_map_tagger_val = {'TP':0.95, 'MP':0.90, 'LP':0.80}\n",
    "\n",
    "    \n",
    "## Necessary KDE parameters used in qcdKdeSyst unce\n",
    "g_custom_kde_bw = {'fj_x_btagcsvv2':15, 'mSV12_ptmax_log':4, 'mSV12_dxysig_log':4}\n",
    "g_custom_kde_binmask = {'fj_x_btagcsvv2':[0], 'mSV12_ptmax_log':[-0.4,1.8,2.5,3.2], 'mSV12_dxysig_log':[-0.8,-0.4,1.8,2.5]}\n",
    "\n",
    "def launch_maker():\n",
    "    r\"\"\"Depth 0: Main function to launch the fit given the global parameters\n",
    "    \"\"\"\n",
    "    for _ifit in g_do_fit_for:\n",
    "        for _wp in g_do_fit_for[_ifit]:\n",
    "            \n",
    "            ## Real tagger range with the given WP\n",
    "            tagger_range = {'TP': (g_map_tagger_val['TP'], 1.0), 'MP': (g_map_tagger_val['MP'], g_map_tagger_val['TP']), 'LP': (g_map_tagger_val['LP'], g_map_tagger_val['MP'])}\n",
    "\n",
    "            ## Get fit info and output lambda func\n",
    "            fitinfo, outdir_func = g_fitinfo[_ifit]\n",
    "\n",
    "            ## Loop over BDT varing list \n",
    "            for sfBDT_val in g_sfBDT_val_list:\n",
    "                ## The default args in the main fit\n",
    "                args = {\n",
    "                    'wgtstr_dm': f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*sfbdtwgt_g90_incl', 'wgtstr_dm_data': None,\n",
    "                    'sl_dm': ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'],\n",
    "                    'sl_dm_herwig': ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'],\n",
    "                    'config_dm': {\n",
    "                        'data':  '',\n",
    "                        'flvB':  'fj_x_nbhadrons>=1',\n",
    "                        'flvC':  'fj_x_nbhadrons==0 & fj_x_nchadrons>=1',\n",
    "                        'flvL':  'fj_x_nbhadrons==0 & fj_x_nchadrons==0',\n",
    "                    },\n",
    "                    'categories_dm': ['flvL', 'flvB', 'flvC', 'data'],\n",
    "                    'catMap': {\n",
    "                        'pass': 'fj_x_ParticleNetMD_XccVsQCD>%.3f & fj_x_ParticleNetMD_XccVsQCD<=%.3f' % (tagger_range[_wp][0], tagger_range[_wp][1]),\n",
    "                        'fail': 'fj_x_ParticleNetMD_XccVsQCD<=%.3f | fj_x_ParticleNetMD_XccVsQCD>%.3f' % (tagger_range[_wp][0], tagger_range[_wp][1]),\n",
    "                    },\n",
    "                }\n",
    "                ## Modify args according to specified global param\n",
    "                if g_make_template_mode == 'val_pt':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*ad_ptwgt', None\n",
    "                elif g_make_template_mode == 'val_tosig_mass':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*sfbdtwgt_g90_incl*massdatamcwgt', 'massdatamcwgt'\n",
    "                elif g_make_template_mode == 'val_tosig_pt':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*sfbdtwgt_g90_incl*ptdatamcwgt', 'ptdatamcwgt'\n",
    "                elif g_make_template_mode == 'val_tosig_tau21':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*sfbdtwgt_g90_incl*tau21datamcwgt', 'tau21datamcwgt'\n",
    "\n",
    "                ## df1->df2: apply sfBDT cut first\n",
    "                df2 = {}\n",
    "                for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']:\n",
    "                    df2[sam] = df1[sam].query(f'fj_x_sfBDT>{sfBDT_val}')\n",
    "\n",
    "                wrapperPt(df2, fitinfo, lambda pt_range, sys_name: outdir_func(_wp, sfBDT_val, pt_range, sys_name), sfBDT_val, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapperPt(df2, fitinfo, outdir_func, sfBDT_val, args):\n",
    "    r\"\"\"Depth 1: Process the pT cut and wrap all other following steps\n",
    "    \"\"\"\n",
    "    \n",
    "    for pt_range in [(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000)]:\n",
    "        print ('pt range:', pt_range)\n",
    "        \n",
    "        ## df2->df3: apply the pT cut\n",
    "        df3 = {}\n",
    "        for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']:\n",
    "            df3[sam] = df2[sam].query(f'fj_x_pt>={pt_range[0]} & fj_x_pt<{pt_range[1]}')\n",
    "        \n",
    "        makeTemplatesWrapper(df3, fitinfo, lambda sys_name: outdir_func(pt_range, sys_name), sfBDT_val, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTemplatesWrapper(df3, fitinfo, outdir_func, sfBDT_val, args):\n",
    "    r\"\"\"Depth 2: Specify which template (nominal or any shape uncertainty) to make in this step\n",
    "    \"\"\"\n",
    "    \n",
    "    wgtstr_dm = args['wgtstr_dm']\n",
    "    if 'nominal' in g_make_unce_types.keys() and g_make_unce_types['nominal']:\n",
    "        sys_name = 'nominal'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "    \n",
    "    ## Below we extract hists for all unce type. Note: we only need such procedure in sfBDT>0.9 case (except for the validaiton when varying the sfBDT)\n",
    "    if sfBDT_val==g_sfBDT_val_list[-1] or g_make_template_mode=='val_vary_sfbdt':\n",
    "        if 'pu' in g_make_unce_types.keys() and g_make_unce_types['pu']: \n",
    "            sys_name = 'puUp'; wgtstr_dm_sys = wgtstr_dm.replace('puWeight','puWeightUp'); makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = 'puDown'; wgtstr_dm_sys = wgtstr_dm.replace('puWeight','puWeightDown'); makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        \n",
    "        if 'fracBB' in g_make_unce_types.keys() and g_make_unce_types['fracBB']: \n",
    "            sys_name = \"fracBBUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons>1) + 1.0*(fj_x_nbhadrons<=1))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"fracBBDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons>1) + 1.0*(fj_x_nbhadrons<=1))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        if 'fracCC' in g_make_unce_types.keys() and g_make_unce_types['fracCC']: \n",
    "            sys_name = \"fracCCUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons==0 & fj_x_nchadrons>1) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons>1)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"fracCCDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons==0 & fj_x_nchadrons>1) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons>1)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        if 'fracLight' in g_make_unce_types.keys() and g_make_unce_types['fracLight']: \n",
    "            sys_name = \"fracLightUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons==0 & fj_x_nchadrons==0) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons==0)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"fracLightDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons==0 & fj_x_nchadrons==0) + 1.0*(not(fj_x_nbhadrons==0 & fj_x_nchadrons==0)))'; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        \n",
    "        ## Below unce is not as easily extracted as above by specifying a different weight string. They may need *special treatment* implemented in the depth-3 function\n",
    "        if 'qcdSyst' in g_make_unce_types.keys() and g_make_unce_types['qcdSyst']: \n",
    "            sys_name = \"qcdSystUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"qcdSystDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        if 'qcdKdeSyst' in g_make_unce_types.keys() and g_make_unce_types['qcdKdeSyst']: \n",
    "            sys_name = \"qcdKdeSystUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"qcdKdeSystDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        if 'psWeightIsr' in g_make_unce_types.keys() and g_make_unce_types['psWeightIsr']: \n",
    "            sys_name = \"psWeightIsrUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"psWeightIsrDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        if 'psWeightFsr' in g_make_unce_types.keys() and g_make_unce_types['psWeightFsr']: \n",
    "            sys_name = \"psWeightFsrUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = \"psWeightFsrDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "\n",
    "        if 'sfBDTRwgt' in g_make_unce_types.keys() and g_make_unce_types['sfBDTRwgt']: \n",
    "            sys_name = 'sfBDTRwgtUp'; wgtstr_dm_sys = wgtstr_dm.replace('sfbdtwgt_g90_incl','sfbdtwgt_g90_binned'); makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = 'sfBDTRwgtDown'; wgtstr_dm_sys = wgtstr_dm.replace('sfbdtwgt_g90_incl','(2*sfbdtwgt_g90_incl-sfbdtwgt_g90_binned)'); makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "        if 'sfBDTFloAround' in g_make_unce_types.keys() and g_make_unce_types['sfBDTFloAround']: \n",
    "            sys_name = 'sfBDTFloAroundUp'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)\n",
    "            sys_name = 'sfBDTFloAroundDown'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(df3, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTemplates(df3, fitinfo, outputdir, sys_name, wgtstr_dm_sys, args):\n",
    "    r\"\"\"Depth 3: The very base implementation that apply the final pass/fail cut and make the template\n",
    "    \"\"\"\n",
    "    \n",
    "    wgtstr_dm, wgtstr_dm_data, sl_dm, sl_dm_herwig, config_dm, categories_dm, catMap = args['wgtstr_dm'], args['wgtstr_dm_data'], args['sl_dm'], args['sl_dm_herwig'], args['config_dm'], args['categories_dm'], args['catMap']\n",
    "    \n",
    "    if not os.path.exists(outputdir):\n",
    "        os.makedirs(outputdir)\n",
    "\n",
    "    ## Create the output root file\n",
    "    print (fitinfo, outputdir, sys_name, wgtstr_dm_sys)\n",
    "    \n",
    "    ## Loop over pass and fail region\n",
    "    for b in ['pass', 'fail']:\n",
    "        try:\n",
    "            fw = ROOT.TFile(outputdir+f'inputs_{b}.root', 'recreate')\n",
    "            vname, nbin, xmin, xmax, (underflow, overflow), vlabel = fitinfo\n",
    "            \n",
    "            ## Tranfer the {nbin, xmin, xmax} set to the real bin edge if necessary\n",
    "            if not isinstance(nbin, int):\n",
    "                edges = nbin\n",
    "                nbin = len(edges)-1 # reset nbin to \"real\" nbin\n",
    "                edges_inroot = (len(edges)-1, array.array('f', edges))\n",
    "            else:\n",
    "                edges = np.linspace(xmin, xmax, nbin+1)\n",
    "                edges_inroot = (nbin, xmin, xmax)\n",
    "\n",
    "            hv, hist = {}, {}\n",
    "            hname_suf = '_'+sys_name if sys_name!='nominal' else ''  ## suffix to the hist name (the Higgs Combine syntax)\n",
    "            print (' -- ', catMap[b])\n",
    "            \n",
    "            ## MC and data dataframe after applying the final selection\n",
    "            df_mc = pd.concat([df3[sam].query(catMap[b]) for sam in sl_dm[:-1]])\n",
    "            df_data = df3[sl_dm[-1]].query(catMap[b])\n",
    "            \n",
    "            ## Preprocessing for herwig related dataframe if we mean to calculate qcdSyst / qcdKdeSyst unce in this iteration\n",
    "            if 'qcdSyst' in sys_name or 'qcdKdeSyst' in sys_name:\n",
    "                df_mc_herwig = pd.concat([df3[sam].query(catMap[b]) for sam in sl_dm_herwig[:-1]])\n",
    "\n",
    "            # Loop over categories: flvC/flvB/flvL/data\n",
    "            for cat in config_dm:\n",
    "                ## hv[] holds the boosted-histogram type derived from the dataframe, hist[] holds the TH1D type to be stored in ROOT\n",
    "                if cat=='data' and sys_name == 'nominal':\n",
    "                    ## Get the data hist\n",
    "                    hv['data'] = get_hist(df_data[vname].values, bins=edges, weights=np.ones(df_data.shape[0]) if wgtstr_dm_data==None else df_data.eval(wgtstr_dm_data).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                    # Initialize the TH1D hist\n",
    "                    hist['data'] = ROOT.TH1D('data_obs', 'data_obs;'+vname, *edges_inroot) \n",
    "                if cat!='data':\n",
    "                    df_mc_tmp = df_mc.query(config_dm[cat]) ## category selection based on flavor\n",
    "                    ## Get the MC hist for certain flavor\n",
    "                    hv[cat] = get_hist(df_mc_tmp[vname].values, bins=edges, weights=df_mc_tmp.eval(wgtstr_dm_sys).values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                    # Initialize the TH1D hist\n",
    "                    hist[cat] = ROOT.TH1D(cat+hname_suf, cat+hname_suf+';'+vname, *edges_inroot) # init TH1 hist\n",
    "                    hist[cat].Sumw2()\n",
    "            \n",
    "                    ## For qcdSyst / qcdKdeSyst unce that is actually related to Herwig, hv[cat] is dummy here, \n",
    "                    ## and we mean to obtain hv[cat+'_herwig.value'] that will be later filled into hist[cat]\n",
    "                    if sys_name=='qcdSystUp':\n",
    "                        ## Get the Herwig fit for certain flavor\n",
    "                        df_mc_herwig_tmp = df_mc_herwig.query(config_dm[cat]) ## cat selection\n",
    "                        wgtstr_dm_sys_herwig = wgtstr_dm_sys.replace('htwgt','htwgt_herwig').replace('sfbdtwgt_g90','sfbdtwgt_g90_herwig').replace('ad_ptwgt','ad_ptwgt_herwig').replace('datamcwgt','datamcwgt_herwig')\n",
    "                        hv[cat+'_herwig.value'] = get_hist(df_mc_herwig_tmp[vname].values, bins=edges, \n",
    "                                                     weights=df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, \n",
    "                                                     underflow=underflow, overflow=overflow).view(flow=True).value\n",
    "                        ## Store the histogram into global var so we can recycle the same hist in the \"Down\" routine\n",
    "                        g_hist_qcdsyst[(sys_name, b, cat)] = hv[cat+'_herwig.value']\n",
    "                    \n",
    "                    ## Extract the KDE shape directly from herwig shape\n",
    "                    if sys_name=='qcdKdeSystUp':\n",
    "                        df_mc_herwig_tmp = df_mc_herwig.query(config_dm[cat])\n",
    "                        wgtstr_dm_sys_herwig = wgtstr_dm_sys.replace('htwgt','htwgt_herwig').replace('sfbdtwgt_g90','sfbdtwgt_g90_herwig').replace('ad_ptwgt','ad_ptwgt_herwig').replace('datamcwgt','datamcwgt_herwig')\n",
    "                        hv_herwig_orig_value = get_hist(df_mc_herwig_tmp[vname].values, bins=edges, \n",
    "                                                     weights=df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, \n",
    "                                                     underflow=underflow, overflow=overflow).view(flow=True).value\n",
    "                        \n",
    "                        ## Calculate KDE shape, apply two times so that we specify a finer KDE bindwidth based on the first result\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(df_mc_herwig_tmp[vname].values, weights=np.clip(df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, 0, +np.inf))\n",
    "                        kde = gaussian_kde(df_mc_herwig_tmp[vname].values, weights=np.clip(df_mc_herwig_tmp.eval(wgtstr_dm_sys_herwig).values, 0, +np.inf), bw_method=kde.factor/g_custom_kde_bw[vname])\n",
    "                        kde_int = np.zeros([nbin, 2])\n",
    "                        \n",
    "                        ## Integrate the KDE function to obtain KDE histogram\n",
    "                        for i, (low, high) in enumerate(zip(edges[:-1], edges[1:])):\n",
    "                            if low in g_custom_kde_binmask[vname]:\n",
    "                                continue\n",
    "                            kde_int[i] = [kde.integrate_box_1d(low, high), hv_herwig_orig_value[i]]\n",
    "                        # print('rescale kde sum to original herwig sum: ', kde_int[:,1].sum() / kde_int[:,0].sum())\n",
    "                        kde_int[:,0] *= kde_int[:,1].sum() / kde_int[:,0].sum()\n",
    "                        \n",
    "                        ## Fill with original madgraph hist if we plan to mask the bin for KDE. \n",
    "                        ## This is based on the fact that KDE cannot model the hist well in the marginal bins\n",
    "                        hv[cat+'_herwig.value'] = np.array([kde_int[i][0] if kde_int[i][0]!=0 else hv[cat].value[i] for i in range(nbin)])\n",
    "                        \n",
    "                        ## Store the histogram into global var so we can recycle the same hist in the \"Down\" routine\n",
    "                        g_hist_qcdsyst[(sys_name, b, cat)] = hv[cat+'_herwig.value']\n",
    "            \n",
    "                    ## Extract the PSWeight histogram\n",
    "                    if 'psWeight' in sys_name:\n",
    "                        if year==2018:  ## for 2018, calculate the hist by PSWeight vars \n",
    "                            ps_idx = {'psWeightIsrUp':2, 'psWeightIsrDown':0, 'psWeightFsrUp':3, 'psWeightFsrDown':1}\n",
    "                            hv[cat] = get_hist(df_mc_tmp[vname].values, bins=edges, weights=df_mc_tmp.eval(wgtstr_dm_sys+f'*PSWeight{ps_idx[sys_name]+1}').values, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                        else:  ## for 2016/17 extract the PSWeight hist based on 2018 result (transfer the ratio for PSWeight/nominal)\n",
    "                            import re\n",
    "                            outputdir_ps_18 = outputdir.replace(f'_SF{year}_', '_SF2018_')\n",
    "                            hv_nom_18 = uproot.open(outputdir_ps_18.replace(sys_name, 'nominal')+f'inputs_{b}.root')[cat]\n",
    "                            hv_ps_18 = uproot.open(outputdir_ps_18+f'inputs_{b}.root')[cat+'_'+sys_name]\n",
    "                            hv[cat].value *= hv_ps_18.values / hv_nom_18.values\n",
    "                        # print (hv[cat].value)\n",
    "                    \n",
    "                    ## Extract the sfBDTFloAround histogram.\n",
    "                    ## Method: to utilize the nominal hist for sfbdt>0.95 or 0.85 and migrate the MC-to-data confidence level in the 0.90 case\n",
    "                    if 'sfBDTFloAround' in sys_name:\n",
    "                        from scipy.stats import chi2\n",
    "                        hv_data = uproot.open(outputdir.replace(sys_name, 'nominal')+f'inputs_{b}.root')['data_obs'].values  ## nominal data hist for 0.90\n",
    "                        _bdtname = '95' if 'Up' in sys_name else '85'\n",
    "                        fr = uproot.open(outputdir.replace(sys_name, 'nominal').replace(f'/bdt{int(g_sfBDT_val_list[-1]*1000)}/',f'/bdt{_bdtname}0/')+f'inputs_{b}.root')\n",
    "                        fr_data, fr_mc = fr['data_obs'].values, fr['flvC'].values+fr['flvB'].values+fr['flvL'].values  ## nominal data & MC hist for 0.95 or 0.85 (depends on Up or Down)\n",
    "                        \n",
    "                        ## For each bins, migrate the confidence level of MC yield F0 given data yield D0 to the target data yield D => F\n",
    "                        hv_mc = []\n",
    "                        for D, D0, F0 in zip(hv_data, fr_data, fr_mc):\n",
    "                            ## The precise calculation\n",
    "                            F = 0.5*chi2.ppf(chi2.cdf(2*F0, 2*D0+2), 2*D+2) if F0>D0 else 0.5*chi2.ppf(chi2.cdf(2*F0, 2*D0), 2*D)\n",
    "                            if F == np.inf: ## in case the formula results in inf (may occur if F0 >> D0)\n",
    "                                assert F0 > D0\n",
    "                                sigD0 = 0.5 * chi2.ppf(1-(1-0.682689492)/2, 2*D0+2) - D0\n",
    "                                sigD = 0.5 * chi2.ppf(1-(1-0.682689492)/2, 2*D+2) - D\n",
    "                                F = D + sigD/sigD0*(F0-D0)\n",
    "                            hv_mc.append(F)\n",
    "                        \n",
    "                        ## Obtain flavor template based on the flavor proportion in 0.95 or 0.85 region\n",
    "                        hv[cat].value = np.nan_to_num(hv_mc * fr[cat].values / fr_mc, nan=0)\n",
    "                        \n",
    "            ## Fill the hv[cat] (for qcd*, fill hv[cat+'_herwig.value']) into TH1D and save into ROOT\n",
    "            for cat in hist.keys():\n",
    "                ## Special handling for qcdSyst / qcdKdeSyst\n",
    "                if 'qcd' in sys_name and 'SystUp' in sys_name:\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, hv[cat+'_herwig.value'][i])\n",
    "                elif 'qcd' in sys_name and 'SystDown' in sys_name:\n",
    "                    hv[cat+'_herwig.value'] = g_hist_qcdsyst[(sys_name.replace('Down','Up'), b, cat)]\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, 2 * hv[cat].value[i] - hv[cat+'_herwig.value'][i])\n",
    "                    g_hist_qcdsyst[(sys_name.replace('Down','Up'), b, cat)] = None\n",
    "\n",
    "                ## Normal routine\n",
    "                else:\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, hv[cat].value[i])\n",
    "                        hist[cat].SetBinError(i+1, np.sqrt(hv[cat].variance[i]))\n",
    "                \n",
    "                ## Fix some buggy points\n",
    "                if cat!='data':\n",
    "                    for i in range(nbin):\n",
    "                        if hist[cat].GetBinContent(i+1) <= 1e-3:\n",
    "                            hist[cat].SetBinContent(i+1, 1e-3)\n",
    "                            hist[cat].SetBinError(i+1, 1e-3)\n",
    "                        elif hist[cat].GetBinError(i+1) > hist[cat].GetBinContent(i+1):\n",
    "                            hist[cat].SetBinError(i+1, hist[cat].GetBinContent(i+1))\n",
    "\n",
    "                hist[cat].Write()\n",
    "        ## Close the ROOT file if error occurs (otherwise the notebook is easily corrupted)\n",
    "        finally:\n",
    "            fw.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we launch the template maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data/MC comparison plots\n",
    "\n",
    "Based on the DataFrame `df1`, this section aims to make data and MC plots, while MC is categorized into three flavors: C/B/L.\n",
    "With the universial make_data_mc_plots function, one can make specify any final selection, any sample list to produce the standard hist+ratio plot.\n",
    "\n",
    "The below recipe can make a default set of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ configuration  ===================\n",
    "\n",
    "def make_config_dm(sl_dm, wgtstr_dm):\n",
    "    return {\n",
    "        'data':  ('Data',       'jetht-noht',      '1.0',    ''      ),\n",
    "        'flvB':  ('QCD (flvB)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons>=1'  ),\n",
    "        'flvC':  ('QCD (flvC)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons==0 & fj_x_nchadrons>=1'  ),\n",
    "        'flvL':  ('QCD (flvL)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons==0 & fj_x_nchadrons==0'  ),\n",
    "    }\n",
    "\n",
    "categories_dm = ['flvL', 'flvB', 'flvC', 'data']\n",
    "\n",
    "bininfo_dm = [ #(savename, vname, nbin, xmin, xmax, label)\n",
    "    ('ht', 'ht', 50, 0, 2000, r'$H_{T}$ [GeV]'),\n",
    "    ('fj_x_pt', 'fj_x_pt', 20, 200, 800, r'$p_{T}(AK15)$ [GeV]'),\n",
    "    ('fj_x_eta', 'fj_x_eta', 20, -2.5, 2.5, r'$\\eta(AK15)$'),\n",
    "    ('fj_x_sdmass', 'fj_x_sdmass', 15, 50, 200, r'$m_{SD}(AK15)$ [GeV]'),\n",
    "    ('fj_x_sfBDT', 'fj_x_sfBDT', 50, 0.5, 1, r'$sfBDT(AK15)$'),\n",
    "\n",
    "    ('fj_x_ParticleNetMD_XccVsQCD', 'fj_x_ParticleNetMD_XccVsQCD', 40, 0, 1, r'ParticleNetMD_XccVsQCD(AK15)'),\n",
    "    ('fj_x_ParticleNetMD_XccVsQCD_08', 'fj_x_ParticleNetMD_XccVsQCD', 40, 0.8, 1, r'ParticleNetMD_XccVsQCD(AK15)-u'),\n",
    "    \n",
    "    (\"fj_x_btagcsvv2\", \"fj_x_btagcsvv2\", [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, r'$CSVv2$'),\n",
    "    (\"mSV12_ptmax_log\", \"mSV12_ptmax_log\", [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, r'$log(m_{SV1,p_{T}\\,max}\\; /GeV)$'),\n",
    "    (\"mSV12_dxysig_log\", \"mSV12_dxysig_log\", [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, r'$log(m_{SV1,d_{xy}sig\\,max}\\; /GeV)$'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ slim on cc-tagger, sfBDT, then make data/MC plots ===================\n",
    "\n",
    "import seaborn as sns\n",
    "def set_sns_color(*args):\n",
    "    sns.palplot(sns.color_palette(*args))\n",
    "    sns.set_palette(*args)\n",
    "    \n",
    "def make_data_mc_plots(sl_dm, config_dm, finsel, prefix, **kwargs):\n",
    "    r\"\"\"To make standard hist+ratio plots based on the sample list and the final selection\n",
    "    Arguments:\n",
    "        sl_dm: sample list\n",
    "        config_dm: configuration set for each categories in the plots, in the dict format. name: (label, sample/sample list, weight string, cat selection)\n",
    "        finsel: final selections made to produce the plots\n",
    "        prefix: prefix string used in the output plot title\n",
    "        kwargs: includes further KDE-related variables\n",
    "    \"\"\"\n",
    "    \n",
    "    df2 = {}\n",
    "    for sam in sl_dm:\n",
    "        df2[sam] = df1[sam].query(finsel)\n",
    "\n",
    "    result_dic = {savename: {} for savename, _, _, _, _, _ in bininfo_dm}\n",
    "    for savename, vname, nbin, xmin, xmax, vlabel in bininfo_dm:\n",
    "        if 'plot_vars' in kwargs and savename not in kwargs['plot_vars']:\n",
    "            continue\n",
    "        if not isinstance(nbin, int):\n",
    "            edges, xmin, xmax, nbin = nbin, min(nbin), max(nbin), len(nbin)\n",
    "        else:\n",
    "            edges = np.linspace(xmin, xmax, nbin+1)\n",
    "\n",
    "        label, hdm = {}, {}\n",
    "        underflow = False if vlabel[-2:] in ['-u','-a'] else True\n",
    "        overflow  = False if vlabel[-2:] in ['-o','-a'] else True\n",
    "        if vlabel[-2:] in ['-u','-o','-a']:\n",
    "            vlabel = vlabel[:-2]\n",
    "        \n",
    "        if 'g_do_kde_vars' in kwargs and savename in kwargs['g_do_kde_vars'] and kwargs['g_do_kde_vars'][savename]==True:\n",
    "            g_do_kde_vars = True\n",
    "            kde = {}\n",
    "        else:\n",
    "            g_do_kde_vars = False\n",
    "        \n",
    "        ## Loop over categories to extract the hist for each flavor and data\n",
    "        for cat in categories_dm:\n",
    "            lab, sam, wgt, sel = config_dm[cat]\n",
    "            label[cat] = lab\n",
    "            if cat != 'data':\n",
    "                if not isinstance(sam, list):\n",
    "                    df2tmp = df2[sam].query(sel) if sel not in ['','1==1'] else df2[sam]\n",
    "                else:\n",
    "                    df2tmp = []\n",
    "                    for s in sam:\n",
    "                        df2tmp.append(df2[s].query(sel) if sel not in ['','1==1'] else df2[s])\n",
    "                    df2tmp = pd.concat(df2tmp, ignore_index=True)\n",
    "                hdm[cat] = get_hist(df2tmp[vname].values, bins=edges, weights=df2tmp.eval(wgt).values, underflow=underflow, overflow=overflow)\n",
    "                if g_do_kde_vars:\n",
    "                    from scipy.stats import gaussian_kde\n",
    "                    from scipy import integrate\n",
    "                    import multiprocessing\n",
    "                    if 'custom_kde' in kwargs.keys() and savename in kwargs['custom_kde']:\n",
    "                        kde[cat] = kwargs['custom_kde'][savename][cat]\n",
    "                        kde_int_res = [\n",
    "                                integrate.quad(kde[cat][0], -np.inf if (i==0 and underflow) else edges[i], \n",
    "                                                  +np.inf if (i==len(edges)-1 and overflow) else edges[i+1]) for i in range(len(edges)-1)]\n",
    "                    else:\n",
    "                        kdetmp = gaussian_kde(df2tmp[vname].values, weights=np.clip(df2tmp.eval(wgt).values, 0, np.inf))\n",
    "                        if 'g_custom_kde_bw' in kwargs.keys() and savename in kwargs['g_custom_kde_bw']:\n",
    "                            kdetmp = gaussian_kde(df2tmp[vname].values, weights=np.clip(df2tmp.eval(wgt).values, 0, np.inf), bw_method=kdetmp.factor/kwargs['g_custom_kde_bw'][savename])\n",
    "                        kde[cat] = (kdetmp, df2tmp.eval(wgt).sum())\n",
    "                        kde_int_res = [(kde[cat][0].integrate_box_1d(-np.inf if (i==0 and underflow) else edges[i], +np.inf if (i==len(edges)-1 and overflow) else edges[i+1]), 0.) for i in range(len(edges)-1)]\n",
    "                    hdm[cat+'_kde'] = hdm[cat].copy()\n",
    "                    hdm[cat+'_kde'].view(flow=True).value = np.array([kde_int_res[i][0] for i in range(len(edges)-1)]) * kde[cat][1]\n",
    "                    hdm[cat+'_kde'].view(flow=True).variance = np.zeros(len(edges)-1)\n",
    "                        \n",
    "            else: ## is data: no sel, weight=1\n",
    "                hdm[cat] = get_hist(df2[sam][vname].values, bins=edges, weights=np.ones(df2[sam].shape[0]), underflow=underflow, overflow=overflow)\n",
    "        \n",
    "        cat_sufs = ['']\n",
    "        if g_do_kde_vars:\n",
    "            cat_sufs += ['_kde']\n",
    "        for cat_suf in cat_sufs:\n",
    "            ## Draw the standard hist_ratio plot\n",
    "            set_sns_color('cubehelix_r', 3) ## set the color palette\n",
    "            f = plt.figure(figsize=(12,12))\n",
    "            gs = mpl.gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.05) \n",
    "            \n",
    "            ## Upper histogram panel\n",
    "            ax = f.add_subplot(gs[0])\n",
    "            hep.cms.label(data=True, paper=False, year=2016, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "            ax.set_xlim(xmin, xmax); ax.set_xticklabels([]); ax.set_ylabel('Events / bin', ha='right', y=1.0)\n",
    "\n",
    "            plot_hist([hdm[cat+cat_suf] for cat in categories_dm if cat!='data'], bins=edges, label=[label[cat] for cat in categories_dm if cat!='data'], histtype='fill', edgecolor='k', linewidth=1, stack=True) ## draw stacked bkg\n",
    "            cats_mc = list(set(categories_dm) - set(['data']))\n",
    "            hdm_add = hdm[cats_mc[0]+cat_suf].copy()\n",
    "            for cat in cats_mc[1:]:\n",
    "                hdm_add += hdm[cat+cat_suf]\n",
    "            bkgtot, bkgtot_err = hdm_add.view(flow=True).value, np.sqrt(hdm_add.view(flow=True).variance)\n",
    "            ax.fill_between(edges, (bkgtot-bkgtot_err).tolist()+[0], (bkgtot+bkgtot_err).tolist()+[0], label='BKG unce.', step='post', hatch='///', edgecolor='darkblue', facecolor='none', linewidth=0) ## draw bkg unce.\n",
    "            plot_hist(hdm['data'], bins=edges, label='Data', histtype='errorbar', color='k', markersize=15, elinewidth=1.5) ## draw data\n",
    "            # ax.set_yscale('log')\n",
    "\n",
    "            ax.legend()\n",
    "            # ax.legend(loc='upper left'); ax.set_ylim(0, 1.4*ax.get_ylim()[1])\n",
    "            \n",
    "            ## Ratio panel\n",
    "            ax1 = f.add_subplot(gs[1]); ax1.set_xlim(xmin, xmax); ax1.set_ylim(0.001, 1.999)\n",
    "            ax1.set_xlabel(vlabel, ha='right', x=1.0); ax1.set_ylabel('Data / MC', ha='center')\n",
    "            ax1.plot([xmin,xmax], [1,1], 'k'); ax1.plot([xmin,xmax], [0.5,0.5], 'k:'); ax1.plot([xmin,xmax], [1.5,1.5], 'k:')\n",
    "\n",
    "            hr = hdm['data'].view(flow=True).value / hdm_add.view(flow=True).value\n",
    "            # hr_err = hr * np.sqrt(hdm['data'].view(flow=True).variance/(hdm['data'].view(flow=True).value**2) + hdm_add.view(flow=True).variance/(hdm_add.view(flow=True).value**2))\n",
    "            hr_dataerr = hr * np.sqrt(hdm['data'].view(flow=True).variance/(hdm['data'].view(flow=True).value**2))\n",
    "            ax1.fill_between(edges, ((bkgtot-bkgtot_err)/bkgtot).tolist()+[0], ((bkgtot+bkgtot_err)/bkgtot).tolist()+[0], step='post', hatch='///', edgecolor='darkblue', facecolor='none', linewidth=0) ## draw bkg unce.\n",
    "            hep.histplot(np.nan_to_num(hr, nan=-1), bins=edges, yerr=np.nan_to_num(hr_dataerr), histtype='errorbar', color='k', markersize=15, elinewidth=1) ## draw data in ratio plot\n",
    "\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}__{finsel}__{savename}{cat_suf}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}__{finsel}__{savename}{cat_suf}.pdf')\n",
    "\n",
    "        ## kde/orig comparison plots\n",
    "        if g_do_kde_vars:\n",
    "            mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green'])\n",
    "            f, ax = plt.subplots(figsize=(12,12))\n",
    "            hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "            x_contin = np.linspace(xmin, xmax, 201)\n",
    "            bin_width = edges[int(nbin/2)+1] - edges[int(nbin/2)]\n",
    "            for cat, color in zip(['flvC', 'flvB', 'flvL'], ['blue', 'red', 'green']):\n",
    "                lab, sam, wgt, sel = config_dm[cat]\n",
    "                ax.plot(x_contin, kde[cat][0](x_contin) * kde[cat][1] * bin_width, label=lab+' KDE', linestyle=':', color=color)\n",
    "            for cat, color in zip(['flvC', 'flvB', 'flvL'], ['blue', 'red', 'green']):\n",
    "                lab, sam, wgt, sel = config_dm[cat]\n",
    "                hep.histplot(hdm[cat+'_kde'].view(flow=True).value, bins=edges, label=lab+' KDE integral', linestyle='--', color=color)\n",
    "                plot_hist(hdm[cat], bins=edges, label=lab, normed=False, color=color)\n",
    "            ax.set_xlim(xmin, xmax); ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.', ha='right', y=1.0); ax.legend()\n",
    "\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}:kde_shape__{finsel}__{savename}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}:kde_shape__{finsel}__{savename}.pdf')\n",
    "            \n",
    "\n",
    "g_do_kde_vars = {'fj_x_btagcsvv2':True, 'mSV12_ptmax_log':True, 'mSV12_dxysig_log':True}\n",
    "g_custom_kde_bw = {'fj_x_btagcsvv2':15, 'mSV12_ptmax_log':4, 'mSV12_dxysig_log':4}\n",
    "\n",
    "g_dirname = 'test_datamc' ## config me\n",
    "if not os.path.exists(f'plots/{g_dirname}_{year}'):\n",
    "    os.makedirs(f'plots/{g_dirname}_{year}')\n",
    "\n",
    "for ptmin, ptmax in [(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000), (200, 100000)]:\n",
    "    ## 1. With MadGraph sample list\n",
    "    wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt*sfbdtwgt_g90_incl'\n",
    "    sl_dm = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.5', prefix='mg')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.9', prefix='mg')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.9 & fj_x_ParticleNetMD_XccVsQCD>0.95', prefix='mg')\n",
    "\n",
    "    ## 2. With MadGraph sample list, while using the optional MC-to-data reweight scheme (on pT)\n",
    "    wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*ad_ptwgt'\n",
    "    sl_dm = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.9', prefix='mg_ptwgt')\n",
    "    \n",
    "    ## 3. With Herwig sample list\n",
    "    wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*htwgt_herwig*sfbdtwgt_g90_herwig_incl'\n",
    "    sl_dm = ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.5', prefix='herwig')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.9', prefix='herwig')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), finsel=f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax} & fj_x_sfBDT>0.9 & fj_x_ParticleNetMD_XccVsQCD>0.95', prefix='herwig', \n",
    "                       g_do_kde_vars=g_do_kde_vars, g_custom_kde_bw=g_custom_kde_bw) ## also make the KDE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal/proxy comparison plots\n",
    "\n",
    "Based on the DataFrame `df1`, The below recipe creates the proxy jet (from MC) and h->cc signal jet comparison plots on various jet observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the hcc signal tree\n",
    "_df0['vhcc-2L'] = uproot.open(f\"samples/trees/20200906_VH_extfillsv_2016_2L/mc/vhcc_tree.root\")['Events'].pandas.df()\n",
    "\n",
    "boosted = \"v_pt>200 & ak15_pt>200 & dphi_V_ak15>2.5 & ak15_sdmass>50 & ak15_sdmass<200\"\n",
    "basecut = f\"fj_x_pt>200 & fj_x_sdmass>50 & fj_x_sdmass<200 & passmetfilters & fj_x_nbhadrons==0 & fj_x_nchadrons>=1\"\n",
    "basecut_vhcc_2L = \"v_mass>75 & v_mass<105 & ((abs(lep1_pdgId)==11 & passTrigEl) | (abs(lep1_pdgId)==13 & passTrigMu)) & \" + boosted + \" & n_ak4<3\"\n",
    "df_comp = {}\n",
    "df_comp['subst_qcd-mg-noht'] = df1['subst_qcd-mg-noht'].query(basecut)\n",
    "df_comp['vhcc-2L'] = _df0['vhcc-2L'].query(basecut_vhcc_2L)\n",
    "\n",
    "wgtstr = 'genWeight*xsecWeight*puWeight*htwgt'\n",
    "wgtstr_vhcc_2L = 'genWeight*xsecWeight*puWeight'\n",
    "basesel = { # name: cut, label\n",
    "    'sv': (\"fj_x_sj1_nsv>=1 & fj_x_sj2_nsv>=1\", r'$N_{SV}^{match}\\geq 1$'),\n",
    "    'tightsv': (\"(fj_x_sj1_sv1_ntracks>2 & abs(fj_x_sj1_sv1_dxy)<3 & fj_x_sj1_sv1_dlensig>4 & fj_x_sj2_sv1_ntracks>2 & abs(fj_x_sj2_sv1_dxy)<3 & fj_x_sj2_sv1_dlensig>4)\", r'$N_{SV,tight}^{match}\\geq 1$'),\n",
    "}\n",
    "def func_basesel(name):\n",
    "    if name in basesel.keys():\n",
    "        return basesel[name]\n",
    "    elif name[:5]=='sfbdt':\n",
    "        x = float(name[5:])/1000.\n",
    "        return ('fj_x_sfBDT>%.3f'%x, r'$sfBDT>%.2f$'%x)\n",
    "    else:\n",
    "        raise RuntimeError('Baseline cut name not recognized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bininfo = [ #(vname, nbin, xmin, xmax, label, *vname for nominal*)   \n",
    "    ('fj_x_ParticleNetMD_XccVsQCD', 20, 0, 1, 'ParticleNetMD_XccVsQCD (AK15)', 'ak15_ParticleNetMD_HccVsQCD'),\n",
    "    ('fj_x_sdmass', 15, 50, 200, r'$m_{SD}$ (AK15)', 'ak15_sdmass'),\n",
    "    ('fj_x_tau21', 20, 0, 1, r'$\\tau_{21}$ (AK15)', 'ak15_tau21'), ##avaliable\n",
    "    \n",
    "    ('fj_x_deltaR_sj12', 40, 0, 1.5, r'$\\Delta R_{sj_{1},sj_{2}}$ (AK15)', 'ak15_deltaR_sj12'),\n",
    "    ('fj_x_pt', 40, 0, 1000, r'$p_{T}$ (AK15)', 'ak15_pt'),\n",
    "    ('fj_x_sj1_pt', 40, 0, 1000, r'$p_{T,sj_{1}}$ (AK15)', 'ak15_sj1_pt'),\n",
    "    ('fj_x_sj1_rawmass', 40, 0, 200, r'$m_{sj_{1},raw}$ (AK15)', 'ak15_sj1_rawmass'), ##avaliable\n",
    "    ('fj_x_sj2_pt', 40, 0, 1000, r'$p_{T,sj_{2}}$ (AK15)', 'ak15_sj2_pt'),\n",
    "    ('fj_x_sj2_rawmass', 40, 0, 200, r'$m_{sj_{2},raw}$ (AK15)', 'ak15_sj2_rawmass'), ##avaliable\n",
    "    \n",
    "    ('fj_x_nsv', 10, 0, 10, r'$N_{SV}$ (AK15)', 'ak15_nlooseSV'), ##avaliable\n",
    "    ('fj_x_nsv_ptgt25', 8, 0, 8, r'$N_{SV,p_{T}\\geq 25}$ (AK15)', 'ak15_nlooseSV_ptgt25'), ##avaliable\n",
    "    ('fj_x_nsv_ptgt50', 8, 0, 8, r'$N_{SV,p_{T}\\geq 50}$ (AK15)', 'ak15_nlooseSV_ptgt50'), ##avaliable\n",
    "    ('fj_x_ntracks', 20, 0, 20, r'$N_{tracks}$ (AK15)', 'ak15_nlooseSV_ntracks'), ##avaliable\n",
    "    ('fj_x_ntracks_sv12', 20, 0, 20, r'$N_{tracks\\;for\\;SV_{1,2}}$ (AK15)', 'ak15_nlooseSV_ntracks_sv12'), ##avaliable\n",
    "    ('fj_x_sj1_nsv', 20, 0, 20, r'$N_{SV\\;from\\;sj_{1}}$ (AK15)', 'ak15_sj1_nlooseSV'), ##avaliable\n",
    "    ('fj_x_sj1_ntracks', 20, 0, 20, r'$N_{tracks\\;from\\;sj_{1}}$ (AK15)', 'ak15_sj1_nlooseSV_ntracks'), ##avaliable\n",
    "    ('fj_x_sj1_sv1_pt', 20, 0, 200, r'$p_{T,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_pt'),\n",
    "    ('fj_x_sj1_sv1_mass', 20, 0, 50, r'$m_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_mass'), ##avaliable\n",
    "    ('fj_x_sj1_sv1_masscor', 20, 0, 50, r'$m_{cor\\;for\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_masscor'),\n",
    "    ('fj_x_sj1_sv1_ntracks', 20, 0, 20, r'$N_{tracks\\;from\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_ntracks'),\n",
    "    ('fj_x_sj1_sv1_dxy', 20, 0, 5, r'$d_{xy,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dxy'),\n",
    "    ('fj_x_sj1_sv1_dxysig', 20, 0, 20, r'$\\sigma_{d_{xy},\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dxysig'),\n",
    "    ('fj_x_sj1_sv1_dlen', 20, 0, 5, r'$d_{z,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dlen'),\n",
    "    ('fj_x_sj1_sv1_dlensig', 20, 0, 20, r'$\\sigma_{d_{z},\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dlensig'),\n",
    "    ('fj_x_sj1_sv1_chi2ndof', 20, 0, 5, r'$\\chi^2 / Ndof_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_chi2ndof'),\n",
    "    ('fj_x_sj1_sv1_pangle', 40, 0, 5, r'$pAngle_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_pangle'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dirname = 'test_sigpxy' ## config me\n",
    "if not os.path.exists(f'plots/{g_dirname}_{year}'):\n",
    "    os.makedirs(f'plots/{g_dirname}_{year}')\n",
    "\n",
    "## Make comparison plots for normal weight (MC adopt the same weight as in the fit), or for additional mass / pT / tau21 weight\n",
    "for wgtfac, pfwgt in zip(['1','massdatamcwgt','ptdatamcwgt'], ['nom', 'massdatamcwgt', 'ptdatamcwgt']):\n",
    "\n",
    "    wgtstr = f'genWeight*xsecWeight*puWeight*htwgt*sfbdtwgt_g90_incl*{wgtfac}'\n",
    "    wgtstr_vhcc_2L = 'genWeight*xsecWeight*puWeight'\n",
    "\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "    do_rwgt = 0\n",
    "    for ptmin, ptmax in [(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000), (200, 100000)]:\n",
    "        presel, presel1 = f'fj_x_pt>{ptmin} & fj_x_pt<{ptmax}', f'ak15_pt>{ptmin} & ak15_pt<{ptmax}'\n",
    "        label = {'subst_qcd-mg-noht': r'g(cc) (subst.)', 'vhcc-2L':r'$Z(\\ell\\ell)H(cc)$'}\n",
    "\n",
    "        for vname, nbin, xmin, xmax, vlabel, vname1 in bininfo:\n",
    "            f, ax = plt.subplots(figsize=(12,12))\n",
    "            hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "\n",
    "            for sam in ['vhcc-2L']:\n",
    "                dftmp = df_comp[sam] if presel1=='' else df_comp[sam].query(presel1)\n",
    "                h = get_hist(dftmp[vname1].values, bins=np.linspace(xmin, xmax, nbin+1), weights=dftmp.eval(wgtstr_vhcc_2L).values)\n",
    "                plot_hist(h, label=label[sam]+' $N_{SV}^{match}\\geq 1$' if sam=='qcd-mg' else label[sam], normed=True)\n",
    "\n",
    "            for sam in ['subst_qcd-mg-noht']:\n",
    "                for ext in ['sv+sfbdt500', 'sv+sfbdt850', 'sv+sfbdt900', 'sv+sfbdt950']:\n",
    "                    cutstr = ' & '.join(list(filter(None, [presel]+[func_basesel(cname)[0] for cname in ext.split('+')]))) ## join the cut string\n",
    "                    if 'qcd-mg' in sam:  print (cutstr)\n",
    "                    dftmp = df_comp[sam].query(cutstr)\n",
    "                    h = get_hist(dftmp[vname].values, bins=np.linspace(xmin, xmax, nbin+1), weights=dftmp.eval(wgtstr))\n",
    "                    plot_hist(h, label=label[sam]+' '+(rwgt_ext_label if do_rwgt else '')+' & '.join([func_basesel(cname)[1] for cname in ext.split('+')]), normed=True)\n",
    "\n",
    "            ax.legend()\n",
    "            ax.set_xlim(xmin, xmax)\n",
    "            ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.', ha='right', y=1.0); \n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{pfwgt}_{presel}__{vname}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{pfwgt}_{presel}__{vname}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
