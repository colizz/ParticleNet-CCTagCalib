{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook for ParticleNet AK15 cc-tagger SF derivation \n",
    "## (`coffea`+`awkward` workflow)\n",
    "\n",
    "The notebook aims to\n",
    " - Make the ROOT-format **templates** for fit\n",
    " - Produce **data/MC comparison plots** under some given event selection\n",
    " - Produce **H->cc signal and g->cc proxy jets comparison plots** on various jet observables\n",
    " \n",
    "We adopt the `coffea`+`awkward` non-processor workflow in this notebook, illustrated as follows:\n",
    "\n",
    "    Input files (flat ROOT-tuples derived from analysis NanoAOD)\n",
    "    -> use the `coffea` event factory to load the branches as awkward arrays (in the lazy way)\n",
    "    -> manipulate the awkward arrays\n",
    "    -> produce histograms (`boost_histogram`)\n",
    "    -> (1) convert to TH1D for ROOT template; or (2) plot with `mplhep` using `matplotlib` as backend\n",
    "    \n",
    "An earlier notebook using `uproot`+`panda` workflow is given in `ak15_sf_main_pd.ipynb`. The two notebooks have exactly the same goal and have block-to-block correspondence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make templates for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, TreeMakerSchema, BaseSchema\n",
    "import awkward1 as ak\n",
    "import uproot4 as uproot\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boost_histogram as bh\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "use_helvet = True  ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "\n",
    "def get_hist(array, bins=10, xmin=None, xmax=None, underflow=False, overflow=False, mergeflowbin=True, normed=False,\n",
    "            weights=None, **kwargs):\n",
    "    r\"\"\"Plot histogram from input array.\n",
    "\n",
    "    Arguments:\n",
    "        array (np.ndarray): input array.\n",
    "        bins (int, list or tuple of numbers, np.ndarray, bh.axis): bins\n",
    "        weights (None, or np.ndarray): weights\n",
    "        # normed (bool): deprecated.\n",
    "\n",
    "    Returns:\n",
    "        hist (boost_histogram.Histogram)\n",
    "    \"\"\"\n",
    "    if isinstance(bins, int):\n",
    "        if xmin is None:\n",
    "            xmin = array.min()\n",
    "        if xmax is None:\n",
    "            xmax = array.max()\n",
    "        width = 1.*(xmax-xmin)/bins\n",
    "        if mergeflowbin and underflow:\n",
    "            xmin += width\n",
    "            bins -= 1\n",
    "        if mergeflowbin and underflow:\n",
    "            xmax -= width\n",
    "            bins -= 1\n",
    "        bins = bh.axis.Regular(bins, xmin, xmax, underflow=underflow, overflow=overflow)\n",
    "    elif isinstance(bins, (list, tuple, np.ndarray)):\n",
    "        if mergeflowbin and underflow:\n",
    "            bins = bins[1:]\n",
    "        if mergeflowbin and overflow:\n",
    "            bins = bins[:-1]\n",
    "        bins = bh.axis.Variable(bins, underflow=underflow, overflow=overflow)\n",
    "\n",
    "    hist = bh.Histogram(bins, storage=bh.storage.Weight())\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(array)\n",
    "    hist.fill(array, weight=weights)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def plot_hist(hists, normed=False, **kwargs):\n",
    "    r\"\"\"Plot the histogram in the type of boost_histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(hists, (list, tuple)):\n",
    "        hists = [hists]\n",
    "    content = [h.view(flow=True).value for h in hists]\n",
    "    bins = hists[0].axes[0].edges\n",
    "    if 'bins' in kwargs:\n",
    "        bins = kwargs.pop('bins')\n",
    "    if 'yerr' in kwargs:\n",
    "        yerr = kwargs.pop('yerr')\n",
    "    else:\n",
    "        yerr = [np.sqrt(h.view(flow=True).variance) for h in hists]\n",
    "    if normed:\n",
    "        for i in range(len(content)):\n",
    "            contsum = sum(content[i])\n",
    "            content[i] /= contsum\n",
    "            yerr[i] /= contsum\n",
    "    if len(hists) == 1:\n",
    "        content, yerr = content[0], yerr[0]\n",
    "    hep.histplot(content, bins=bins, yerr=yerr, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load files\n",
    "\n",
    "Load the ROOT files into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018  ## config me! options: 2016, 2017, 2018\n",
    "lumi = {2016: 35.92, 2017: 41.53, 2018: 59.74}\n",
    "\n",
    "## Read the root file into lazy awkward arrays\n",
    "arr = {}\n",
    "arr['qcd-mg-noht'] = NanoEventsFactory.from_file(f'samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/qcd-mg_tree.root', schemaclass=BaseSchema).events()\n",
    "arr['qcd-herwig-noht'] = NanoEventsFactory.from_file(f'samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/qcd-herwig_tree.root', schemaclass=BaseSchema).events()\n",
    "arr['top-noht'] = NanoEventsFactory.from_file(f'samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/top_tree.root', schemaclass=BaseSchema).events()\n",
    "arr['v-qq-noht'] = NanoEventsFactory.from_file(f'samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/mc/v-qq_tree.root', schemaclass=BaseSchema).events()\n",
    "arr['jetht-noht'] = NanoEventsFactory.from_file(f'samples/trees_sf/20201028_nohtwbdt_v2_ak15_qcd_{year}/data/jetht_tree.root', schemaclass=BaseSchema).events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing\n",
    "\n",
    "For data: apply OR of all HT trigger to enhance statistics.\n",
    "\n",
    "For MC: apply no HT trigger, based on the strategy we name it \"MC substitute\".\n",
    "\n",
    "We define an attribute `maskdict` in each sample that stores masks corresponding to different selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_expr(ak_array, expr, mask=None):\n",
    "    \"\"\"A function that can do `eval` to the awkward array, immitating the behavior of `eval` in pandas.\"\"\"\n",
    "    \n",
    "    def get_variable_names(expr, exclude=['awkward', 'ak', 'np', 'numpy', 'math']):\n",
    "        \"\"\"Extract variables in the expr\"\"\"\n",
    "        import ast\n",
    "        root = ast.parse(expr)\n",
    "        return sorted({node.id for node in ast.walk(root) if isinstance(node, ast.Name) and not node.id.startswith('_')} - set(exclude))\n",
    "\n",
    "    tmp = {k:ak_array[k] if mask is None else ak_array[k].mask[mask] for k in get_variable_names(expr)}\n",
    "    tmp.update({'math': math, 'numpy': np, 'np': np, 'awkward': ak, 'ak': ak})\n",
    "#     print('eval expr: ', expr, '\\nvars', get_variable_names(expr))\n",
    "    return eval(expr, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ Pre-processing for data  ===================\n",
    "\n",
    "## Baseline selection applied to data. \n",
    "## Note that we use the OR or all HT triggers (some are pre-scaled triggers)\n",
    "\n",
    "hlt_branches = {  ## used HLT_PFHT* branches depend on year\n",
    "    2016: ['HLT_PFHT125', 'HLT_PFHT200', 'HLT_PFHT250', 'HLT_PFHT300', 'HLT_PFHT350', 'HLT_PFHT400', 'HLT_PFHT475', 'HLT_PFHT600', 'HLT_PFHT650', 'HLT_PFHT800', 'HLT_PFHT900'],\n",
    "    2017: ['HLT_PFHT180', 'HLT_PFHT250', 'HLT_PFHT370', 'HLT_PFHT430', 'HLT_PFHT510', 'HLT_PFHT590', 'HLT_PFHT680', 'HLT_PFHT780', 'HLT_PFHT890', 'HLT_PFHT1050', 'HLT_PFHT350'],\n",
    "    2018: ['HLT_PFHT180', 'HLT_PFHT250', 'HLT_PFHT370', 'HLT_PFHT430', 'HLT_PFHT510', 'HLT_PFHT590', 'HLT_PFHT680', 'HLT_PFHT780', 'HLT_PFHT890', 'HLT_PFHT1050', 'HLT_PFHT350'],\n",
    "}\n",
    "htcut_incl = '('+' | '.join(hlt_branches[year])+')'\n",
    "basesel_ext_noht_prep = f\"passmetfilters & (fj_x_pt>200) & fj_x_is_qualified\"\n",
    "sl_prep = ['jetht-noht']\n",
    "\n",
    "for sam in sl_prep:\n",
    "    assert 'noht' in sam\n",
    "    arr[sam].maskdict = {}\n",
    "    arr[sam].maskdict['hlt'] = eval_expr(arr[sam], htcut_incl)\n",
    "    for i in ['1','2']:\n",
    "        ## The baseline selection for data\n",
    "        print('baseline selection for data: ', sam, f'jet{i}')\n",
    "        arr[sam].maskdict[f'fj_{i}_base'] = arr[sam].maskdict['hlt'] & eval_expr(arr[sam], basesel_ext_noht_prep.replace('fj_x', f'fj_{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR TEST: check the xsecWeight for MG samples & genWeight for Herwig sample (to avoid extremely large values) \n",
    "from collections import Counter\n",
    "print(Counter(np.array(arr['qcd-mg-noht'].xsecWeight)),'\\n')\n",
    "for i in [0.96, 0.98, 0.99]:\n",
    "    print(np.quantile(np.array(arr['qcd-herwig-noht'].genWeight), q=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ Pre-processing for MC substitute  ===================\n",
    "\n",
    "## Baseline selection applied to MC.\n",
    "## No HT trigger is applied, based on the \"MC substitute\" strategy\n",
    "basesel_noht_prep_subst = \"passmetfilters & (fj_x_pt>200) & fj_x_is_qualified\"\n",
    "sl_prep_subst = ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht']  ## mark sample name with \"subst_\" as a reminder of MC substitute\n",
    "for sam in sl_prep_subst:\n",
    "    assert 'noht' in sam\n",
    "    arr[sam] = arr[sam.replace('subst_','')]  ## use the name subst_ as a ref\n",
    "    arr[sam].maskdict = {}\n",
    "    for i in ['1','2']:\n",
    "        print('baseline selection for: ', sam, f'jet{i}')\n",
    "        arr[sam].maskdict[f'fj_{i}_base_subst'] = eval_expr(arr[sam], basesel_noht_prep_subst.replace('fj_x', f'fj_{i}'))\n",
    "        ## Drop MG events with extremely large xsecWeight (coming from low HT sample in the HT-binned MG list)\n",
    "        if sam == 'subst_qcd-mg-noht':\n",
    "            arr[sam].maskdict[f'fj_{i}_base_subst'] = arr[sam].maskdict[f'fj_{i}_base_subst'] & eval_expr(arr[sam], 'xsecWeight<5.')\n",
    "        ## Drop Herwig events with extremely large genWeight\n",
    "        if sam == 'subst_qcd-herwig-noht':\n",
    "            arr[sam].maskdict[f'fj_{i}_base_subst'] = arr[sam].maskdict[f'fj_{i}_base_subst'] & eval_expr(arr[sam], 'genWeight<{}'.format(np.quantile(np.array(arr[sam].genWeight), q=0.96)))\n",
    "    ## Fix a 2016 bug: Herwig sample xsec is mistaken\n",
    "    if year == 2016 and sam == 'subst_qcd-herwig-noht' and not hasattr(arr[sam], 'xsecWeight_is_normed'):\n",
    "        arr[sam]['xsecWeight'] = arr[sam]['xsecWeight'] * 2400.\n",
    "        arr[sam]['xsecWeight_is_normed'] = True\n",
    "\n",
    "## Produce new variables used for fit\n",
    "for sam in sl_prep + sl_prep_subst:\n",
    "    for i in ['1','2']:\n",
    "        _mask = arr[sam].maskdict[f'fj_{i}_base'] if sam in sl_prep else arr[sam].maskdict[f'fj_{i}_base_subst']\n",
    "        print('calculating new vars for: ', sam, f'jet{i}')\n",
    "        arr[sam][f'fj_{i}_mSV12_ptmax'] = eval_expr(arr[sam], f'(fj_{i}_sj1_sv1_pt>fj_{i}_sj2_sv1_pt)*fj_{i}_sj1_sv1_masscor + (fj_{i}_sj1_sv1_pt<=fj_{i}_sj2_sv1_pt)*fj_{i}_sj2_sv1_masscor', mask=_mask)\n",
    "        arr[sam][f'fj_{i}_mSV12_ptmax_log'] = eval_expr(arr[sam], f'np.log(fj_{i}_mSV12_ptmax)', mask=_mask)\n",
    "        arr[sam][f'fj_{i}_mSV12_dxysig'] = eval_expr(arr[sam], f'(fj_{i}_sj1_sv1_dxysig>fj_{i}_sj2_sv1_dxysig)*fj_{i}_sj1_sv1_masscor + (fj_{i}_sj1_sv1_dxysig<=fj_{i}_sj2_sv1_dxysig)*fj_{i}_sj2_sv1_masscor', mask=_mask)\n",
    "        arr[sam][f'fj_{i}_mSV12_dxysig_log'] = eval_expr(arr[sam], f'np.log(fj_{i}_mSV12_dxysig)', mask=_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain reweight factors\n",
    "\n",
    "We extract the following reweight factors. The first two sets are used in the nominal fit. The other two are for validation.\n",
    "\n",
    " 1. **MC substitute-to-data reweight factor**: on the HT variable based on (pT, jet index) bins. The goal is to bring the shape of MC substitute back to the data shape in the inclusive region. Remember that the raw MC substitute yield is always much larger than data, because most HT triggers applied to data are pre-scaled triggers. New variables have the name `htwgt_(|herwig)`.\n",
    "\n",
    " 2. **sfBDT reweight factor**: based on (pT, jet index) bins, to further reweight MC substitute back to data shape on the sfBDT variable. Since sfBDT>0.9 is imposed in the final fit region, the sfBDT shape discrepancy between the \"reweighted MC substitute\" and data may again cause $N_{total}$ difference for MC and data, after setting sfBDT>0.9 in the fit region. Therefore, we calculate the overall factor `sfbdtwgt_g90_(|herwig)_incl` in each (pT, jet index) bin, used in the nominal shape template; and the binned factor `sfbdtwgt_g90_(|herwig)_binned` used in the shape uncertainty extraction brought by the sfBDT shape mismodeling\n",
    "\n",
    " 3. **Additional MC substitute-to-data reweight factor on $p_{T}$ only**: A possible replacement of the first two factors combined. This factor is only used in the validation fit. The goal for this validation is to check if different reweighting schemes may affect the SF fit results. New variables have the name `ad_ptwgt_(|herwig)`.\n",
    " \n",
    " 4. **Proxy-to-signal reweight factor on $m_{SD}$ / $p_{T}$ / $\\tau_{21}$**: based on the shape of \"reweighted MC substitute (after the first two steps)\" and the H->cc signal jet shape in the inclusive region. The factor is only used in the validation fit, in which we apply such reweight factor to both MC substitute and data to check if the SF results are affected. New variables have the name `(mass|pt|tau21)datamcwgt_(|herwig)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and(arr, mask_list):\n",
    "    \"\"\"Calculate AND of given mask list\"\"\"\n",
    "    return np.logical_and.reduce([arr.maskdict[mask] for mask in mask_list])\n",
    "\n",
    "def concat_array(arrdict, expr, sam_list, filter_list):\n",
    "    \"\"\"Concatenate the awkward arrays passing the given filter list\"\"\"\n",
    "    if not isinstance(sam_list, list):\n",
    "        sam_list = [sam_list]\n",
    "    return np.concatenate([\n",
    "        np.array(eval_expr(arrdict[sam], expr)[mask_and(arrdict[sam], filter_list)]) for sam in sam_list\n",
    "    ])\n",
    "\n",
    "def mask_and_fj12(arr, mask_list):\n",
    "    \"\"\"Comibne `mask_and` result for fj_1 and fj_2\"\"\"\n",
    "    mask_list_fj1 = [ele.replace('fj_x', 'fj_1') for ele in mask_list]\n",
    "    mask_list_fj2 = [ele.replace('fj_x', 'fj_2') for ele in mask_list]\n",
    "    return np.concatenate([mask_and(arr, mask_list_fj1), mask_and(arr, mask_list_fj2)])\n",
    "\n",
    "def concat_array_fj12(arrdict, expr, sam_list, filter_list):\n",
    "    \"\"\"Comibne `concat_array` result for fj_1 and fj_2\"\"\"\n",
    "    filter_list_fj1 = [ele.replace('fj_x', 'fj_1') for ele in filter_list]\n",
    "    filter_list_fj2 = [ele.replace('fj_x', 'fj_2') for ele in filter_list]\n",
    "    return np.concatenate([concat_array(arrdict, expr.replace('fj_x', 'fj_1'), sam_list, filter_list_fj1), \n",
    "                           concat_array(arrdict, expr.replace('fj_x', 'fj_2'), sam_list, filter_list_fj2)])\n",
    "\n",
    "def calc_rwgt_akarray(arr, rwgt_edge, rwgt):\n",
    "    \"\"\"Calculate the weight ak-array based on the value ak-array of the reweight variable\"\"\"\n",
    "    arr_out = (arr<rwgt_edge[0])*rwgt[0]\n",
    "    for i in range(len(rwgt_edge)-1):\n",
    "        arr_out = arr_out + ((arr>=rwgt_edge[i]) & (arr<rwgt_edge[i+1]))*rwgt[i+1]\n",
    "    arr_out = arr_out + (arr>=rwgt_edge[-1])*rwgt[-1]\n",
    "    return arr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 1. Reweight MC subsitute to data: stored as variable \"fj_x_htwgt\", \"fj_x_htwgt_herwig\") ===================\n",
    "\n",
    "## True: if the block has run before, we can obtain the reweight factor from the previously stored pickle output\n",
    "is_read_from_pickel = False\n",
    "\n",
    "def extract_mc_to_data_ht_weight(arr, sl_rwgt, wgtstr_rwgt, wgtname):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor on HT based on (pT, jet index) bins\n",
    "    \n",
    "    Arguments:\n",
    "        arr: awkward array dict as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "    \"\"\"\n",
    "\n",
    "    rwgt_var = 'ht'\n",
    "    ## The binning info for (pT, HT) grid. Note that 2016 is different from 2017/18. The adopted HT grid is based on MC shape in each pT bin\n",
    "    rwgt_edge_dic = {}\n",
    "    rwgt_edge_dic[2016] = {\n",
    "        'pt200to250': [300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100],\n",
    "        'pt250to300': [350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1300],\n",
    "        'pt300to350': [450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1350],\n",
    "        'pt350to400': [550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "        'pt400to500': [600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "        'pt500toInf': [800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "    }\n",
    "    rwgt_edge_dic[2017] = rwgt_edge_dic[2018] = {\n",
    "    #         'pt200to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200], # deprecated\n",
    "    #         'pt300to400': [500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1600], # deprecated\n",
    "        'pt200to250': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 900, 1000],\n",
    "        'pt250to300': [250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200],\n",
    "        'pt300to350': [450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500],\n",
    "        'pt350to400': [550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1600],\n",
    "        'pt400to500': [700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1800],\n",
    "        'pt500toInf': [900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2200],\n",
    "    }\n",
    "    \n",
    "    ## Initially fill the output column with 0, since we will fill the column iteratively for each pT bin\n",
    "    for sam in sl_rwgt:\n",
    "        for i in ['1','2']:\n",
    "            arr[sam][wgtname.replace('fj_x', f'fj_{i}')] = ak.zeros_like(arr[sam][rwgt_var])\n",
    "\n",
    "    if is_read_from_pickel: ## restore info from a previously stored pickle\n",
    "        import pickle\n",
    "        with open(f'plots/wgtv5/htwgt_{year}.pickle', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            res = res[0] if 'herwig' not in wgtname else res[1]\n",
    "            ent_data, ent_mc, rwgt = res['ent_data'], res['ent_mc'], res['rwgt']\n",
    "    else:\n",
    "        ent_data, ent_mc, rwgt = {}, {}, {}\n",
    "\n",
    "    ## Rewight separately on jet pT bins\n",
    "    for ptsel, ptlab in zip(['(fj_x_pt>=200) & (fj_x_pt<250)', '(fj_x_pt>=250) & (fj_x_pt<300)', '(fj_x_pt>=300) & (fj_x_pt<350)', '(fj_x_pt>=350) & (fj_x_pt<400)', '(fj_x_pt>=400) & (fj_x_pt<500)', '(fj_x_pt>=500)'], \n",
    "                            ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf']):\n",
    "        ## Reweight separately for 1st or 2nd jet\n",
    "        for i, lab in zip(['1','2'], ['jet1','jet2']):\n",
    "            print (' -- ', ptsel, lab)\n",
    "            rwgt_edge = rwgt_edge_dic[year][ptlab]\n",
    "            ## Calculate the rwgt for the first time\n",
    "            if not is_read_from_pickel:\n",
    "                for sam in sl_rwgt+['jetht-noht']:\n",
    "                    arr[sam].maskdict[f'fj_{i}_{ptlab}'] = eval_expr(arr[sam], ptsel.replace('fj_x', f'fj_{i}'))\n",
    "\n",
    "                ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "                ent_data[ptlab+lab] = get_hist(concat_array(arr, expr=rwgt_var, sam_list=['jetht-noht'], filter_list=[f'fj_{i}_base', f'fj_{i}_{ptlab}']),\n",
    "                                               bins=rwgt_edge, \n",
    "                                               weights=np.ones(np.sum(mask_and(arr['jetht-noht'], mask_list=[f'fj_{i}_base', f'fj_{i}_{ptlab}']))), \n",
    "                                               underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "                ent_mc[ptlab+lab]   = get_hist(concat_array(arr, expr=rwgt_var, sam_list=sl_rwgt, filter_list=[f'fj_{i}_base_subst', f'fj_{i}_{ptlab}']),\n",
    "                                               bins=rwgt_edge,\n",
    "                                               weights=concat_array(arr, expr=wgtstr_rwgt, sam_list=sl_rwgt, filter_list=[f'fj_{i}_base_subst', f'fj_{i}_{ptlab}']),\n",
    "                                               underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "                ## Calculate the reweight factor\n",
    "                rwgt[ptlab+lab] = np.nan_to_num(ent_data[ptlab+lab] / ent_mc[ptlab+lab], nan=0) # len=nbin+2\n",
    "            print(ent_data[ptlab+lab], '\\n', rwgt[ptlab+lab])\n",
    "\n",
    "            ## Assign the reweight factor to the new column\n",
    "            ## We use pandas for easier implementation. Modifcation on pandas array can be directly transferred back to original ak array\n",
    "            for sam in sl_rwgt:\n",
    "                _var = rwgt_var\n",
    "                _wgtname = wgtname.replace('fj_x', f'fj_{i}')\n",
    "                _mask = mask_and(arr[sam], mask_list=[f'fj_{i}_base_subst', f'fj_{i}_{ptlab}'])\n",
    "                arr[sam][_wgtname] = arr[sam][_wgtname] + ak.fill_none(calc_rwgt_akarray(arr[sam][_var].mask[_mask], rwgt_edge, rwgt[ptlab+lab]), 0)\n",
    "                print('midpoint: ', sam, _wgtname, arr[sam][_wgtname])\n",
    "\n",
    "    # =========== plot ===========\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "    for ptlab in ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf']:\n",
    "        f, ax = plt.subplots(figsize=(11,11))\n",
    "        hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "        for lab in ['jet1', 'jet2']:\n",
    "            hep.histplot(ent_data[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][ptlab])+[2500], label=f'Data ({lab})')\n",
    "            hep.histplot(ent_mc[ptlab+lab], bins=[0]+list(rwgt_edge_dic[year][ptlab])+[2500], label=f'MC subst. ({lab})')\n",
    "        ax.set_xlim(0, 2500); ax.set_xlabel('$H_{T}$ [GeV]', ha='right', x=1.0); ax.set_ylabel('Events / bin', ha='right', y=1.0); ax.legend()\n",
    "        if not os.path.exists('plots/wgtv5'):\n",
    "            os.makedirs('plots/wgtv5')\n",
    "        plt.savefig(f'plots/wgtv5/{year}_{ptlab}__{wgtname}.pdf')\n",
    "        plt.savefig(f'plots/wgtv5/{year}_{ptlab}__{wgtname}.png')\n",
    "    # ============================\n",
    "    \n",
    "    return {'ent_data':ent_data, 'ent_mc':ent_mc, 'rwgt':rwgt}\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "htwgt = extract_mc_to_data_ht_weight(arr, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'],     wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='fj_x_htwgt')\n",
    "htwgt_herwig = extract_mc_to_data_ht_weight(arr, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='fj_x_htwgt_herwig')\n",
    "\n",
    "if not is_read_from_pickel: ## store the info for the first run\n",
    "    import pickle\n",
    "    with open(f'plots/wgtv5/htwgt_{year}.pickle', 'wb') as fw:\n",
    "        pickle.dump([htwgt, htwgt_herwig], fw)\n",
    "\n",
    "ak.to_pandas(arr['subst_qcd-mg-noht'][['ht', 'fj_1_pt', 'fj_1_htwgt']][arr['subst_qcd-mg-noht'].maskdict['fj_1_base_subst']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 2. Extract the sfBDT>0.9 overall factor and binned fractor: stored as variable \"sfbdtwgt_g90_incl\", \"sfbdtwgt_g90_binned\"; similar for herwig ===================\n",
    "\n",
    "def extract_further_sfbdt_weight(arr, sl_rwgt, wgtstr_rwgt, wgtname_binned, wgtname_incl):\n",
    "    r\"\"\"Extract the \"MC substitute to data\" reweight factor (both overall and binned factor) further on sfBDT variable, after a sfBDT>0.9 selection\n",
    "    \n",
    "    Arguments:\n",
    "        arr: awkward array dict as input\n",
    "        sl_rwgt: sample list for MC substitute in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname_binned: the reweight name (the binned factors) stored as a new column\n",
    "        wgtname_incl: the reweight name (the overall factor) stored as a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initially fill the output column with 0, since we will fill the column iteratively for each pT bin\n",
    "    for sam in sl_rwgt:\n",
    "        for i in ['1','2']:\n",
    "            arr[sam][wgtname_binned.replace('fj_x', f'fj_{i}')] = ak.zeros_like(arr[sam]['ht'])\n",
    "            arr[sam][wgtname_incl.replace('fj_x', f'fj_{i}')]   = ak.zeros_like(arr[sam]['ht'])\n",
    "    \n",
    "    ## Reweight based on the sfBDT variable\n",
    "    rwgt_var, nbin, xmin, xmax  = 'fj_x_sfBDT', 5, 0.9, 1.\n",
    "    print('rwgt sfBDT bins: ', rwgt_var, nbin, xmin, xmax)\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Rewight separately on jet pT bins\n",
    "    for pt_range, ptlab in zip([(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000)],\n",
    "                               ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf']):\n",
    "        ## Requires the selection sfBDT>0.9 which is used in the fit region\n",
    "        for sam in sl_rwgt+['jetht-noht']:\n",
    "            arr[sam].maskdict['fj_1_sfBDT'] = eval_expr(arr[sam], 'fj_1_sfBDT>0.9')\n",
    "            arr[sam].maskdict['fj_2_sfBDT'] = eval_expr(arr[sam], 'fj_2_sfBDT>0.9')\n",
    "\n",
    "        ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "        ## does not distinguish jet1 or jet2 on this reweighting\n",
    "        ent_data = get_hist(concat_array_fj12(arr, expr=rwgt_var, sam_list=['jetht-noht'], filter_list=['fj_x_base', f'fj_x_{ptlab}', 'fj_x_sfBDT']),\n",
    "                            bins=rwgt_edge, \n",
    "                            weights=np.ones(np.sum(mask_and_fj12(arr['jetht-noht'], mask_list=['fj_x_base', f'fj_x_{ptlab}', 'fj_x_sfBDT']))), \n",
    "                            underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ent_mc   = get_hist(concat_array_fj12(arr, expr=rwgt_var, sam_list=sl_rwgt, filter_list=['fj_x_base_subst', f'fj_x_{ptlab}', 'fj_x_sfBDT']),\n",
    "                            bins=rwgt_edge,\n",
    "                            weights=concat_array_fj12(arr, expr=wgtstr_rwgt, sam_list=sl_rwgt, filter_list=['fj_x_base_subst', f'fj_x_{ptlab}', 'fj_x_sfBDT']),\n",
    "                            underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ## Calculate the reweight factor\n",
    "        rwgt = np.nan_to_num(ent_data / ent_mc, nan=0) # len=nbin+2\n",
    "        print (ent_data, rwgt, 'incl:', sum(ent_data) / sum(ent_mc))\n",
    "\n",
    "        ## Assign the reweight factor to the new column\n",
    "        for sam in sl_rwgt:\n",
    "            for i in ['1','2']:\n",
    "                _var = rwgt_var.replace('fj_x', f'fj_{i}')\n",
    "                _wgtname = wgtname_binned.replace('fj_x', f'fj_{i}')\n",
    "                _mask = mask_and(arr[sam], mask_list=[f'fj_{i}_base_subst', f'fj_{i}_{ptlab}', f'fj_{i}_sfBDT'])\n",
    "                arr[sam][_wgtname] = arr[sam][_wgtname] + ak.fill_none(calc_rwgt_akarray(arr[sam][_var].mask[_mask], rwgt_edge, rwgt), 0)\n",
    "                \n",
    "                _wgtname = wgtname_incl.replace('fj_x', f'fj_{i}')\n",
    "                _mask = mask_and(arr[sam], mask_list=[f'fj_{i}_base_subst', f'fj_{i}_{ptlab}'])\n",
    "                arr[sam][_wgtname] = arr[sam][_wgtname] + ak.fill_none(_mask * sum(ent_data) / sum(ent_mc), 0)\n",
    "                print('midpoint: ', sam, _wgtname, arr[sam][_wgtname])\n",
    "\n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_further_sfbdt_weight(arr, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt\",\n",
    "                             wgtname_binned='fj_x_sfbdtwgt_g90_binned', wgtname_incl='fj_x_sfbdtwgt_g90_incl')\n",
    "extract_further_sfbdt_weight(arr, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt_herwig\",\n",
    "                             wgtname_binned='fj_x_sfbdtwgt_g90_herwig_binned', wgtname_incl='fj_x_sfbdtwgt_g90_herwig_incl')\n",
    "\n",
    "ak.to_pandas(arr['subst_qcd-mg-noht'][['fj_1_pt', 'fj_1_sfBDT', 'fj_1_sfbdtwgt_g90_incl']][arr['subst_qcd-mg-noht'].maskdict['fj_1_base_subst']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 3. [additional] Reweight MC subsitute to data on pT: stored as variable \"ad_ptwgt\", \"ad_ptwgt_herwig\" ===================\n",
    "\n",
    "def extract_mc_to_data_pt_weight(arr, sl_rwgt, wgtstr_rwgt, wgtname):\n",
    "    r\"\"\"Extract the \"MC subsisute to data\" reweight factor on pT as a optional choice\n",
    "    \n",
    "    Arguments:\n",
    "        arr: awkward array dict as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply simple 1D reweight to pT\n",
    "    rwgt_var, nbin, xmin, xmax  = 'fj_x_pt', 20, 200., 1200.\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Rewight separately on 1st/2nd jet\n",
    "    for i, lab in zip(['1','2'], ['jet1','jet2']):\n",
    "        ## Get data and MC histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "        ent_data = get_hist(concat_array(arr, expr=rwgt_var.replace('fj_x', f'fj_{i}'), sam_list=['jetht-noht'], filter_list=[f'fj_{i}_base', f'fj_{i}_sfBDT']),\n",
    "                            bins=rwgt_edge, \n",
    "                            weights=np.ones(np.sum(mask_and(arr['jetht-noht'], filter_list=[f'fj_{i}_base', f'fj_{i}_sfBDT']))), \n",
    "                            underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ent_mc   = get_hist(concat_array(arr, expr=rwgt_var.replace('fj_x', f'fj_{i}'), sam_list=sl_rwgt, filter_list=[f'fj_{i}_base_subst', f'fj_{i}_sfBDT']),\n",
    "                            bins=rwgt_edge,\n",
    "                            weights=concat_array(arr, expr=wgtstr_rwgt, sam_list=sl_rwgt, filter_list=[f'fj_{i}_base_subst', f'fj_{i}_sfBDT']),\n",
    "                            underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "        ## Calculate the reweight factor\n",
    "        rwgt = np.nan_to_num(ent_data / ent_mc, nan=0) # len=nbin+2\n",
    "        print (ent_data, rwgt)\n",
    "        \n",
    "        ## assign the reweight factor to the new column\n",
    "        for sam in sl_rwgt:\n",
    "            _var = rwgt_var.replace('fj_x', f'fj_{i}')\n",
    "            _wgtname = wgtname.replace('fj_x', f'fj_{i}')\n",
    "            _mask = mask_and(arr[sam], mask_list=[f'fj_{i}_base_subst'])\n",
    "            arr[sam][_wgtname] = calc_rwgt_akarray(arr[sam][_var].mask[_mask], rwgt_edge, rwgt)  ## fill the new column directly as a masked array\n",
    "            print('midpoint: ', sam, _wgtname, arr[sam][_wgtname])\n",
    "        \n",
    "## Calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_mc_to_data_pt_weight(arr, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'],     wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\",        wgtname='fj_x_ad_ptwgt')\n",
    "extract_mc_to_data_pt_weight(arr, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight\", wgtname='fj_x_ad_ptwgt_herwig')\n",
    "\n",
    "ak.to_pandas(arr['subst_qcd-mg-noht'][['ht', 'fj_1_pt', 'fj_1_htwgt', 'fj_1_sfbdtwgt_g90_incl', 'fj_1_ad_ptwgt']][arr['subst_qcd-mg-noht'].maskdict['fj_1_base_subst']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ 4. [additional] Reweight MC (proxy jet) to H->cc signal jet on either mass/pT/tau21: stored as variable \"(mass|pt|tau21)datamcwgt\"; similar for herwig  ===================\n",
    "\n",
    "# First load the h->cc signal ntuple. Adopt the selction used in the analysis\n",
    "arr['vhcc-2L'] = NanoEventsFactory.from_file(f'samples/trees/20200906_VH_extfillsv_2016_2L/mc/vhcc_tree.root', schemaclass=BaseSchema).events()\n",
    "\n",
    "boosted = \"(v_pt>200) & (ak15_pt>200) & (dphi_V_ak15>2.5) & (ak15_sdmass>50) & (ak15_sdmass<200)\"\n",
    "basecut_vhcc_2L = \"(v_mass>75) & (v_mass<105) & (((np.abs(lep1_pdgId)==11) & passTrigEl) | ((np.abs(lep1_pdgId)==13) & passTrigMu)) & \" + boosted + \" & (n_ak4<3)\"\n",
    "arr['vhcc-2L'].maskdict = {}\n",
    "arr['vhcc-2L'].maskdict['base'] = eval_expr(arr['vhcc-2L'], basecut_vhcc_2L)\n",
    "\n",
    "def extract_mc_to_signal_weight(arr, sl_rwgt, wgtstr_rwgt, wgtname, rwgt_info):\n",
    "    r\"\"\"Extract the \"MC subsisute (proxy) to H->cc signal jet\" reweight factor on possible variable\n",
    "    \n",
    "    Arguments:\n",
    "        arr: awkward array dict as input\n",
    "        sl_rwgt: sample list for MC substitue in this reweighting routine\n",
    "        wgtstr_rwgt: the weight string applied to MC to produce the histogram in this reweighting routine\n",
    "        wgtname: the reweight name stored as a new column\n",
    "        rwgt_info: variable and binning info for this reweighting routine\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reweight info extracted from the function argument\n",
    "    rwgt_var, nbin, xmin, xmax, rwgt_var_nom  = rwgt_info\n",
    "    print('rwgt info: ', rwgt_var, nbin, xmin, xmax)\n",
    "    rwgt_edge = np.linspace(xmin, xmax, nbin+1)\n",
    "    \n",
    "    ## Requires the selection sfBDT>0.9 which is used in the fit region\n",
    "    rwgt_sel = 'fj_x_sfBDT>0.9'\n",
    "    \n",
    "    ## Get MC and h->cc signal histogram. Note: consider underflow & overflow bins, hence len = nbins+2\n",
    "    wgt_mc = concat_array_fj12(arr, expr=wgtstr_rwgt, sam_list=sl_rwgt, filter_list=['fj_x_base_subst', 'fj_x_sfBDT'])\n",
    "    yield_mc = wgt_mc.sum()\n",
    "    ent_mc  = get_hist(concat_array_fj12(arr, expr=rwgt_var, sam_list=sl_rwgt, filter_list=['fj_x_base_subst', 'fj_x_sfBDT']),\n",
    "                       bins=rwgt_edge,\n",
    "                       weights=wgt_mc,\n",
    "                       underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value      \n",
    "    \n",
    "    wgt_hcc = concat_array(arr, expr='genWeight*xsecWeight*puWeight', sam_list=['vhcc-2L'], filter_list=['base'])\n",
    "    yield_hcc = wgt_hcc.sum()\n",
    "    ent_hcc = get_hist(concat_array(arr, expr=rwgt_var_nom, sam_list=['vhcc-2L'], filter_list=['base']),\n",
    "                       bins=rwgt_edge,\n",
    "                       weights=wgt_hcc,\n",
    "                       underflow=True, overflow=True, mergeflowbin=False).view(flow=True).value\n",
    "    \n",
    "    ## Calculate the reweight factors for the two normalized histograms, and clip to (0, 50)\n",
    "    rwgt = np.nan_to_num((ent_hcc/yield_hcc) / (ent_mc/yield_mc), nan=0) # len=nbin+2\n",
    "    rwgt = np.clip(rwgt, 0, 50)\n",
    "    print (ent_hcc, rwgt)\n",
    "\n",
    "    ## assign the reweight factor to the new column (to both MC and data)\n",
    "    for sam in sl_rwgt + ['jetht-noht']:\n",
    "        for i in ['1','2']:\n",
    "            _var = rwgt_var.replace('fj_x', f'fj_{i}')\n",
    "            _wgtname = wgtname.replace('fj_x', f'fj_{i}')\n",
    "            _mask = mask_and(arr[sam], mask_list=[f'fj_{i}_base_subst'] if sam!='jetht-noht' else [f'fj_{i}_base'])\n",
    "            arr[sam][_wgtname] = calc_rwgt_akarray(arr[sam][_var].mask[_mask], rwgt_edge, rwgt)  ## fill the new column directly as a masked array\n",
    "            print('midpoint: ', sam, _wgtname, arr[sam][_wgtname])\n",
    "    \n",
    "## For each reweight variable, calculate two sets of reweight factor: one for the MG sample list and another for Herwig sample list\n",
    "extract_mc_to_signal_weight(arr, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl\",\n",
    "                            wgtname='fj_x_massdatamcwgt', rwgt_info=('fj_x_sdmass', 15, 50, 200, 'ak15_sdmass'))\n",
    "extract_mc_to_signal_weight(arr, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt_herwig*fj_x_sfbdtwgt_g90_herwig_incl\",\n",
    "                            wgtname='fj_x_massdatamcwgt_herwig', rwgt_info=('fj_x_sdmass', 15, 50, 200, 'ak15_sdmass'))\n",
    "extract_mc_to_signal_weight(arr, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl\",\n",
    "                            wgtname='fj_x_ptdatamcwgt', rwgt_info=('fj_x_pt', 20, 200, 1200, 'ak15_pt'))\n",
    "extract_mc_to_signal_weight(arr, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt_herwig*fj_x_sfbdtwgt_g90_herwig_incl\",\n",
    "                            wgtname='fj_x_ptdatamcwgt_herwig', rwgt_info=('fj_x_pt', 20, 200, 1200, 'ak15_pt'))\n",
    "extract_mc_to_signal_weight(arr, sl_rwgt=['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl\",\n",
    "                            wgtname='fj_x_tau21datamcwgt', rwgt_info=('fj_x_tau21', 20, 0, 1, 'ak15_tau21'))\n",
    "extract_mc_to_signal_weight(arr, sl_rwgt=['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht'], wgtstr_rwgt = f\"{lumi[year]}*genWeight*xsecWeight*puWeight*fj_x_htwgt_herwig*fj_x_sfbdtwgt_g90_herwig_incl\",\n",
    "                            wgtname='fj_x_tau21datamcwgt_herwig', rwgt_info=('fj_x_tau21', 20, 0, 1, 'ak15_tau21'))\n",
    "\n",
    "ak.to_pandas(arr['jetht-noht'][['fj_1_sdmass', 'fj_1_massdatamcwgt', 'fj_1_pt', 'fj_1_ptdatamcwgt', 'fj_1_tau21', 'fj_1_tau21datamcwgt']][arr['jetht-noht'].maskdict['fj_1_base']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make ROOT templates\n",
    "\n",
    "We produce the ROOT templates using the DataFrame in this step. The outputs are ROOT files with neat structure. After the further reorganization, they can be used as the Higgs Combine input to implement the fit.\n",
    "\n",
    "As a reference, we provide an example of the output files and their structure. \n",
    "E.g., for a **given fit variable**, **given tagger WP** and a **certain jet-pT bin** for **a single fit**, the output ROOT templates should include the pass and fail MC template in the B/C/L flavors, the data template, and the MC systematics for all specified shape uncertainties. The files are organized in the following structure:\n",
    "```\n",
    "─── 20201115_SF2017_AK15_qcd_subst_pst_ptw50_TP_msv12_dxysig_log_var22binsv2  [use variable: msv12_dxysig_log, Tight WP]\n",
    "    └── Cards\n",
    "        └── bdt900\n",
    "            ├── pt200to250                 [given pT bin]\n",
    "            │   ├── nominal                    [the nominal histograms]\n",
    "            │   │   ├── inputs_fail.root           [include four TH1D: flvC, flvB, flvL, data_obs]\n",
    "            │   │   └── inputs_pass.root           [..]\n",
    "            │   ├── fracBBDown                 [shape uncertainty plots]\n",
    "            │   │   ├── inputs_fail.root           [include three TH1D: flvC_fracBBDown, flvB_fracBBDown, flvL_fracBBDown]\n",
    "            │   │   └── inputs_pass.root           [..]\n",
    "            │   ├── fracBBUp                   [..]\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracCCDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracCCUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracLightDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── fracLightUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightFsrDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightFsrUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightIsrDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── psWeightIsrUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── puDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── puUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdKdeSystDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdKdeSystUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdSystDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── qcdSystUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── sfBDTFloAroundDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── sfBDTFloAroundUp\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   ├── sfBDTRwgtDown\n",
    "            │   │   ├── inputs_fail.root\n",
    "            │   │   └── inputs_pass.root\n",
    "            │   └── sfBDTRwgtUp\n",
    "            │       ├── inputs_fail.root\n",
    "            │       └── inputs_pass.root\n",
    "            ├── pt250to300\n",
    "            │   ├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template making is organized in three nested functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ================================ Global parameters: config me! ================================ ####\n",
    "g_make_template_mode = 'main'\n",
    "r\"\"\"Options:\n",
    "        main           : the main fit\n",
    "        val_pt         : the validation fit -- to use an optional MC subsitute-to-data strategy, i.e. on pT variable only\n",
    "        val_tosig_mass : the validation fit -- additionally reweight MC & data to h->cc signal jet on mass\n",
    "        val_tosig_pt   : the validation fit -- additionally reweight MC & data to h->cc signal jet on pt  \n",
    "        val_tosig_tau21: the validation fit -- additionally reweight MC & data to h->cc signal jet on tau21\n",
    "        val_vary_sfbdt : the validation fit -- varying sfBDT cut value and drop sfBDT* uncertaint\n",
    "        val_crop_bin   : the validation fit -- cropping the marginal bins for fit\n",
    "\"\"\"\n",
    "\n",
    "g_outdir_prefix = f'20201115_SF{year}_AK15_qcd_subst_pst_ptw50_ak'\n",
    "r\"\"\"Prefix for the output dir name \"\"\"\n",
    "\n",
    "g_make_unce_types = {'nominal':True, 'pu':True, 'fracBB':True, 'fracCC':True, 'fracLight':True, 'psWeightIsr':False, 'psWeightFsr':False, 'sfBDTRwgt':True, 'sfBDTFloAround':True}\n",
    "r\"\"\"The uncertainty types used in the fit. Use False or remove the key to disable an certain unce type\n",
    "    Note: \"qcdSyst\" and \"qcdKdeSyst\" is not used in this verision. \"psWeightIsr\" and \"psWeightFsr\" works fine in 2018 while in 2016/17 one need to first garantee the 2018 histograms exist\n",
    "          so the unce can be transferred.\n",
    "\"\"\" # for test, we disable psWeightIsr/Fsr\n",
    "\n",
    "g_do_fit_for = { # for test, we launch the main fit var (1) only\n",
    "    1: ['TP', 'MP', 'LP'],\n",
    "#     2: ['TP', 'MP', 'LP'],\n",
    "#     3: ['TP', 'MP', 'LP'],\n",
    "}\n",
    "r\"\"\" Do fit for which variable and which WPs\"\"\"\n",
    "#### =============================================================================================== ####\n",
    "\n",
    "## Consistency check for gloal params\n",
    "if g_make_template_mode not in ['main', 'val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21', 'val_vary_sfbdt', 'val_crop_bin']:\n",
    "    raise RuntimeError('Specified mode cannot be recognized.')\n",
    "if g_make_template_mode in ['val_pt', 'val_tosig_mass', 'val_tosig_pt', 'val_tosig_tau21', 'val_vary_sfbdt'] and list(g_do_fit_for.keys()) != [1]:\n",
    "    print('Warning: for validation fit, set the fit information to the main variable (1) only')\n",
    "    g_do_fit_for = {1: ['TP', 'MP', 'LP']}\n",
    "if g_make_template_mode == 'val_crop_bin' and list(g_do_fit_for.keys()) != [901]:\n",
    "    print('Warning: for validation fit on cropping the marginal bins, set the fit information to the cropped main variable (901) only')\n",
    "    g_do_fit_for = {901: ['TP', 'MP', 'LP']}\n",
    "if g_make_template_mode == 'val_vary_sfbdt':\n",
    "    g_make_unce_types.pop('sfBDTRwgt', None)\n",
    "    g_make_unce_types.pop('sfBDTFloAround', None)\n",
    "    \n",
    "## The sfBDT varing list. \n",
    "## Note: to implement sfBDTFloAround unce, one must first obtain the nominal hist for the cut value 0.85, 0.95\n",
    "if g_make_template_mode != 'val_vary_sfbdt':\n",
    "    g_sfBDT_val_list = [0.85, 0.95, 0.9]\n",
    "else:\n",
    "    g_sfBDT_val_list = [0.84, 0.86, 0.88, 0.90, 0.92, 0.94] ## for validation: varying sfBDT\n",
    "    \n",
    "\n",
    "## Fit info: in the format of [ (fit var, nbins/edges, xmin/None, xmax/None, (underflow, overflow), label), outputdir lambda func ]\n",
    "g_fitinfo = {\n",
    "    1: [ ##  main fit var\n",
    "        ('fj_x_mSV12_dxysig_log', [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, (True, True), 'mSV12_dxysig_log'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_msv12_dxysig_log_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "    2: [ ## the other var for validation\n",
    "        ('fj_x_mSV12_ptmax_log', [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, (True, True), 'mSV12_ptmax_log'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_msv12_ptmax_log_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "    3: [ ## the other var for validation\n",
    "        ('fj_x_btagcsvv2', [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, (True, True), 'CSVv2'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_csvv2_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "    901: [ ## crop the marginal bins for the main var as a validation\n",
    "        ('fj_x_mSV12_dxysig_log', [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8], None, None, (False, False), 'mSV12_dxysig_log'), \n",
    "        lambda wp, bdt, pt_range, sys_name: f'results/{g_outdir_prefix}_{wp}_msv12_dxysig_log_var22binsv2/Cards/bdt{int(bdt*1000)}/pt{pt_range[0]}to{pt_range[1]}/{sys_name}/'\n",
    "    ],\n",
    "}\n",
    "g_hist_qcdsyst = {}\n",
    "\n",
    "\n",
    "## Tagger values in use\n",
    "g_map_tagger_val = {'TP':0.95, 'MP':0.90, 'LP':0.80}\n",
    "\n",
    "    \n",
    "## Necessary KDE parameters used in qcdKdeSyst unce\n",
    "g_custom_kde_bw = {'fj_x_btagcsvv2':15, 'mSV12_ptmax_log':4, 'mSV12_dxysig_log':4}\n",
    "g_custom_kde_binmask = {'fj_x_btagcsvv2':[0], 'mSV12_ptmax_log':[-0.4,1.8,2.5,3.2], 'mSV12_dxysig_log':[-0.8,-0.4,1.8,2.5]}\n",
    "\n",
    "def launch_maker():\n",
    "    r\"\"\"Depth 0: Main function to launch the fit given the global parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    ## flavor masks\n",
    "    for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht']:\n",
    "        for i in ['1','2']:\n",
    "            arr[sam].maskdict[f'fj_{i}_flvB'] = eval_expr(arr[sam], f'fj_{i}_nbhadrons>=1')\n",
    "            arr[sam].maskdict[f'fj_{i}_flvC'] = eval_expr(arr[sam], f'(fj_{i}_nbhadrons==0) & (fj_{i}_nchadrons>=1)')\n",
    "            arr[sam].maskdict[f'fj_{i}_flvL'] = eval_expr(arr[sam], f'(fj_{i}_nbhadrons==0) & (fj_{i}_nchadrons==0)')\n",
    "\n",
    "    for _ifit in g_do_fit_for:\n",
    "        for _wp in g_do_fit_for[_ifit]:\n",
    "            \n",
    "            ## Real tagger range with the given WP\n",
    "            tagger_range = {'TP': (g_map_tagger_val['TP'], 1.0), 'MP': (g_map_tagger_val['MP'], g_map_tagger_val['TP']), 'LP': (g_map_tagger_val['LP'], g_map_tagger_val['MP'])}\n",
    "\n",
    "            ## masks for applying the tagger\n",
    "            for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']:\n",
    "                for i in ['1','2']:\n",
    "                    arr[sam].maskdict[f'fj_{i}_tagger_pass'] = eval_expr(arr[sam], f'(fj_{i}_ParticleNetMD_XccVsQCD>{tagger_range[_wp][0]:.3f}) & (fj_{i}_ParticleNetMD_XccVsQCD<={tagger_range[_wp][1]:.3f})')\n",
    "                    arr[sam].maskdict[f'fj_{i}_tagger_fail'] = eval_expr(arr[sam], f'(fj_{i}_ParticleNetMD_XccVsQCD<={tagger_range[_wp][0]:.3f}) | (fj_{i}_ParticleNetMD_XccVsQCD>{tagger_range[_wp][1]:.3f})')\n",
    "\n",
    "            ## Get fit info and output lambda func\n",
    "            fitinfo, outdir_func = g_fitinfo[_ifit]\n",
    "\n",
    "            ## Loop over BDT varing list \n",
    "            for sfBDT_val in g_sfBDT_val_list:\n",
    "                ## The default args in the main fit\n",
    "                args = {\n",
    "                    'wgtstr_dm': f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl', 'wgtstr_dm_data': None,\n",
    "                    'sl_dm': ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'],\n",
    "                    'sl_dm_herwig': ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht'],\n",
    "                    'categories_dm': ['flvL', 'flvB', 'flvC', 'data'],\n",
    "                    'base_masks': {\n",
    "                        'data': ['fj_x_base'],\n",
    "                        'mc':   ['fj_x_base_subst'],\n",
    "                    }\n",
    "                }\n",
    "                ## Modify args according to specified global param\n",
    "                if g_make_template_mode == 'val_pt':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_ad_ptwgt', None\n",
    "                elif g_make_template_mode == 'val_tosig_mass':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl*fj_x_massdatamcwgt', 'fj_x_massdatamcwgt'\n",
    "                elif g_make_template_mode == 'val_tosig_pt':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl*fj_x_ptdatamcwgt', 'fj_x_ptdatamcwgt'\n",
    "                elif g_make_template_mode == 'val_tosig_tau21':\n",
    "                    args['wgtstr_dm'], args['wgtstr_dm_data'] = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl*fj_x_tau21datamcwgt', 'fj_x_tau21datamcwgt'\n",
    "\n",
    "                ## masks for applying sfBDT cut\n",
    "                for sam in ['subst_qcd-mg-noht', 'subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']:\n",
    "                    for i in ['1','2']:\n",
    "                        arr[sam].maskdict[f'fj_{i}_sfBDT>{sfBDT_val}'] = eval_expr(arr[sam], f'fj_{i}_sfBDT>{sfBDT_val}')\n",
    "\n",
    "                wrapperPt(arr, fitinfo, lambda pt_range, sys_name: outdir_func(_wp, sfBDT_val, pt_range, sys_name), sfBDT_val, args, ext_masks=[f'fj_x_sfBDT>{sfBDT_val}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapperPt(arr, fitinfo, outdir_func, sfBDT_val, args, ext_masks):\n",
    "    r\"\"\"Depth 1: Process the pT cut and wrap all other following steps\n",
    "    \"\"\"\n",
    "    \n",
    "    for pt_range, ptlab in zip([(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000)],\n",
    "                               ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf']):\n",
    "        print ('pt range:', pt_range)\n",
    "        \n",
    "        makeTemplatesWrapper(arr, fitinfo, lambda sys_name: outdir_func(pt_range, sys_name), sfBDT_val, args, ext_masks=ext_masks+[f'fj_x_{ptlab}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTemplatesWrapper(arr, fitinfo, outdir_func, sfBDT_val, args, ext_masks):\n",
    "    r\"\"\"Depth 2: Specify which template (nominal or any shape uncertainty) to make in this step\n",
    "    \"\"\"\n",
    "    \n",
    "    wgtstr_dm = args['wgtstr_dm']\n",
    "    if 'nominal' in g_make_unce_types.keys() and g_make_unce_types['nominal']:\n",
    "        sys_name = 'nominal'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "    \n",
    "    ## Below we extract hists for all unce type. Note: we only need such procedure in sfBDT>0.9 case (except for the validaiton when varying the sfBDT)\n",
    "    if sfBDT_val==g_sfBDT_val_list[-1] or g_make_template_mode=='val_vary_sfbdt':\n",
    "        if 'pu' in g_make_unce_types.keys() and g_make_unce_types['pu']: \n",
    "            sys_name = 'puUp'; wgtstr_dm_sys = wgtstr_dm.replace('puWeight','puWeightUp'); makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = 'puDown'; wgtstr_dm_sys = wgtstr_dm.replace('puWeight','puWeightDown'); makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        \n",
    "        if 'fracBB' in g_make_unce_types.keys() and g_make_unce_types['fracBB']: \n",
    "            sys_name = \"fracBBUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*(fj_x_nbhadrons>1) + 1.0*(fj_x_nbhadrons<=1))'; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"fracBBDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*(fj_x_nbhadrons>1) + 1.0*(fj_x_nbhadrons<=1))'; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        if 'fracCC' in g_make_unce_types.keys() and g_make_unce_types['fracCC']: \n",
    "            sys_name = \"fracCCUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*((fj_x_nbhadrons==0) & (fj_x_nchadrons>1)) + 1.0*(np.logical_not((fj_x_nbhadrons==0) & (fj_x_nchadrons>1))))'; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"fracCCDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*((fj_x_nbhadrons==0) & (fj_x_nchadrons>1)) + 1.0*(np.logical_not((fj_x_nbhadrons==0) & (fj_x_nchadrons>1))))'; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        if 'fracLight' in g_make_unce_types.keys() and g_make_unce_types['fracLight']: \n",
    "            sys_name = \"fracLightUp\"; wgtstr_dm_sys = wgtstr_dm+'*(1.2*((fj_x_nbhadrons==0) & (fj_x_nchadrons==0)) + 1.0*(np.logical_not((fj_x_nbhadrons==0) & (fj_x_nchadrons==0))))'; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"fracLightDown\"; wgtstr_dm_sys = wgtstr_dm+'*(0.8*((fj_x_nbhadrons==0) & (fj_x_nchadrons==0)) + 1.0*(np.logical_not((fj_x_nbhadrons==0) & (fj_x_nchadrons==0))))'; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        \n",
    "        ## Below unce is not as easily extracted as above by specifying a different weight string. They may need *special treatment* implemented in the depth-3 function\n",
    "        if 'qcdSyst' in g_make_unce_types.keys() and g_make_unce_types['qcdSyst']: \n",
    "            sys_name = \"qcdSystUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"qcdSystDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        if 'qcdKdeSyst' in g_make_unce_types.keys() and g_make_unce_types['qcdKdeSyst']: \n",
    "            sys_name = \"qcdKdeSystUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"qcdKdeSystDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        if 'psWeightIsr' in g_make_unce_types.keys() and g_make_unce_types['psWeightIsr']: \n",
    "            sys_name = \"psWeightIsrUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"psWeightIsrDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        if 'psWeightFsr' in g_make_unce_types.keys() and g_make_unce_types['psWeightFsr']: \n",
    "            sys_name = \"psWeightFsrUp\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = \"psWeightFsrDown\"; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "\n",
    "        if 'sfBDTRwgt' in g_make_unce_types.keys() and g_make_unce_types['sfBDTRwgt']: \n",
    "            sys_name = 'sfBDTRwgtUp'; wgtstr_dm_sys = wgtstr_dm.replace('fj_x_sfbdtwgt_g90_incl','fj_x_sfbdtwgt_g90_binned'); makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = 'sfBDTRwgtDown'; wgtstr_dm_sys = wgtstr_dm.replace('fj_x_sfbdtwgt_g90_incl','(2*fj_x_sfbdtwgt_g90_incl-fj_x_sfbdtwgt_g90_binned)'); makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "        if 'sfBDTFloAround' in g_make_unce_types.keys() and g_make_unce_types['sfBDTFloAround']: \n",
    "            sys_name = 'sfBDTFloAroundUp'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)\n",
    "            sys_name = 'sfBDTFloAroundDown'; wgtstr_dm_sys = wgtstr_dm; makeTemplates(arr, fitinfo, outdir_func(sys_name), sys_name, wgtstr_dm_sys, args, ext_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTemplates(arr, fitinfo, outputdir, sys_name, wgtstr_dm_sys, args, ext_masks):\n",
    "    r\"\"\"Depth 3: The very base implementation that apply the final pass/fail cut and make the template\n",
    "    \"\"\"\n",
    "    print(ext_masks)\n",
    "    wgtstr_dm, wgtstr_dm_data, sl_dm, sl_dm_herwig, categories_dm, base_masks = args['wgtstr_dm'], args['wgtstr_dm_data'], args['sl_dm'], args['sl_dm_herwig'], args['categories_dm'], args['base_masks']\n",
    "    \n",
    "    if not os.path.exists(outputdir):\n",
    "        os.makedirs(outputdir)\n",
    "\n",
    "    ## Create the output root file\n",
    "    print (fitinfo, outputdir, sys_name, wgtstr_dm_sys)\n",
    "    \n",
    "    ## Loop over pass and fail region\n",
    "    import ROOT, array  ## use ROOT to write file...\n",
    "    for b in ['pass', 'fail']:\n",
    "        try:\n",
    "            fw = ROOT.TFile(outputdir+f'inputs_{b}.root', 'recreate')\n",
    "            vname, nbin, xmin, xmax, (underflow, overflow), vlabel = fitinfo\n",
    "            mask_list_fin = {  # final mask list used for selection\n",
    "                'data': base_masks['data']+ext_masks+[f'fj_x_tagger_{b}'],\n",
    "                'mc':   base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}'],\n",
    "            }\n",
    "            \n",
    "            ## Tranfer the {nbin, xmin, xmax} set to the real bin edge if necessary\n",
    "            if not isinstance(nbin, int):\n",
    "                edges = nbin\n",
    "                nbin = len(edges)-1 # reset nbin to \"real\" nbin\n",
    "                edges_inroot = (len(edges)-1, array.array('f', edges))\n",
    "            else:\n",
    "                edges = np.linspace(xmin, xmax, nbin+1)\n",
    "                edges_inroot = (nbin, xmin, xmax)\n",
    "\n",
    "            hv, hist = {}, {}\n",
    "            hname_suf = '_'+sys_name if sys_name!='nominal' else ''  ## suffix to the hist name (the Higgs Combine syntax)\n",
    "            print (' -- ', b)\n",
    "            \n",
    "            # Loop over categories: flvC/flvB/flvL/data\n",
    "            for cat in categories_dm:\n",
    "                ## hv[] holds the boosted-histogram type derived from the dataframe, hist[] holds the TH1D type to be stored in ROOT\n",
    "                if cat=='data' and sys_name == 'nominal':\n",
    "                    ## Get the data hist\n",
    "                    _content = concat_array_fj12(arr, expr=vname, sam_list=[sl_dm[-1]], filter_list=base_masks['data']+ext_masks+[f'fj_x_tagger_{b}'])\n",
    "                    _weights = np.ones_like(_content) if wgtstr_dm_data is None else concat_array_fj12(arr, expr=wgtstr_dm_data, sam_list=[sl_dm[-1]], filter_list=base_masks['data']+ext_masks+[f'fj_x_tagger_{b}'])\n",
    "                    hv['data'] = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow).view(flow=True)     \n",
    "                    # Initialize the TH1D hist\n",
    "                    hist['data'] = ROOT.TH1D('data_obs', 'data_obs;'+vname, *edges_inroot) \n",
    "                if cat!='data':\n",
    "                    ## Get the MC hist for certain flavor\n",
    "                    _content = concat_array_fj12(arr, expr=vname, sam_list=sl_dm[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])\n",
    "                    _weights = concat_array_fj12(arr, expr=wgtstr_dm_sys, sam_list=sl_dm[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])\n",
    "                    hv[cat] = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                    # Initialize the TH1D hist\n",
    "                    hist[cat] = ROOT.TH1D(cat+hname_suf, cat+hname_suf+';'+vname, *edges_inroot) # init TH1 hist\n",
    "                    hist[cat].Sumw2()\n",
    "            \n",
    "                    ## For qcdSyst / qcdKdeSyst unce that is actually related to Herwig, hv[cat] is dummy here, \n",
    "                    ## and we mean to obtain hv[cat+'_herwig.value'] that will be later filled into hist[cat]\n",
    "                    if sys_name=='qcdSystUp':\n",
    "                        ## Get the Herwig fit for certain flavor\n",
    "                        wgtstr_dm_sys_herwig = wgtstr_dm_sys.replace('htwgt','htwgt_herwig').replace('sfbdtwgt_g90','sfbdtwgt_g90_herwig').replace('ad_ptwgt','ad_ptwgt_herwig').replace('datamcwgt','datamcwgt_herwig')\n",
    "                        _content = concat_array_fj12(arr, expr=vname, sam_list=sl_dm_herwig[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])\n",
    "                        _weights = concat_array_fj12(arr, expr=wgtstr_dm_sys_herwig, sam_list=sl_dm_herwig[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])                        \n",
    "                        hv[cat+'_herwig.value'] = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow).view(flow=True).value\n",
    "                        ## Store the histogram into global var so we can recycle the same hist in the \"Down\" routine\n",
    "                        g_hist_qcdsyst[(sys_name, b, cat)] = hv[cat+'_herwig.value']\n",
    "                    \n",
    "                    ## Extract the KDE shape directly from herwig shape\n",
    "                    if sys_name=='qcdKdeSystUp':\n",
    "                        wgtstr_dm_sys_herwig = wgtstr_dm_sys.replace('htwgt','htwgt_herwig').replace('sfbdtwgt_g90','sfbdtwgt_g90_herwig').replace('ad_ptwgt','ad_ptwgt_herwig').replace('datamcwgt','datamcwgt_herwig')\n",
    "                        _content = concat_array_fj12(arr, expr=vname, sam_list=sl_dm_herwig[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])\n",
    "                        _weights = concat_array_fj12(arr, expr=wgtstr_dm_sys_herwig, sam_list=sl_dm_herwig[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])                        \n",
    "                        hv_herwig_orig_value = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow).view(flow=True).value\n",
    "                        \n",
    "                        ## Calculate KDE shape, apply two times so that we specify a finer KDE bindwidth based on the first result\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(_content, weights=np.clip(_weights, 0, +np.inf))\n",
    "                        kde = gaussian_kde(_content, weights=np.clip(_weights, 0, +np.inf), bw_method=kde.factor/g_custom_kde_bw[vname])\n",
    "                        kde_int = np.zeros([nbin, 2])\n",
    "                        \n",
    "                        ## Integrate the KDE function to obtain KDE histogram\n",
    "                        for i, (low, high) in enumerate(zip(edges[:-1], edges[1:])):\n",
    "                            if low in g_custom_kde_binmask[vname]:\n",
    "                                continue\n",
    "                            kde_int[i] = [kde.integrate_box_1d(low, high), hv_herwig_orig_value[i]]\n",
    "                        # print('rescale kde sum to original herwig sum: ', kde_int[:,1].sum() / kde_int[:,0].sum())\n",
    "                        kde_int[:,0] *= kde_int[:,1].sum() / kde_int[:,0].sum()\n",
    "                        \n",
    "                        ## Fill with original madgraph hist if we plan to mask the bin for KDE. \n",
    "                        ## This is based on the fact that KDE cannot model the hist well in the marginal bins\n",
    "                        hv[cat+'_herwig.value'] = np.array([kde_int[i][0] if kde_int[i][0]!=0 else hv[cat].value[i] for i in range(nbin)])\n",
    "                        \n",
    "                        ## Store the histogram into global var so we can recycle the same hist in the \"Down\" routine\n",
    "                        g_hist_qcdsyst[(sys_name, b, cat)] = hv[cat+'_herwig.value']\n",
    "            \n",
    "                    ## Extract the PSWeight histogram\n",
    "                    if 'psWeight' in sys_name:\n",
    "                        if year==2018:  ## for 2018, calculate the hist by PSWeight vars \n",
    "                            ps_idx = {'psWeightIsrUp':2, 'psWeightIsrDown':0, 'psWeightFsrUp':3, 'psWeightFsrDown':1}\n",
    "                            wgtstr_dm_sys_ps = wgtstr_dm_sys + f\"*(PSWeight[:,{ps_idx[sys_name]}])\"\n",
    "                            _weights = concat_array_fj12(arr, expr=wgtstr_dm_sys_ps, sam_list=sl_dm[:-1], filter_list=base_masks['mc']+ext_masks+[f'fj_x_tagger_{b}', f'fj_x_{cat}'])\n",
    "                            hv[cat] = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow).view(flow=True)\n",
    "                        else:  ## for 2016/17 extract the PSWeight hist based on 2018 result (transfer the ratio for PSWeight/nominal)\n",
    "                            import re\n",
    "                            outputdir_ps_18 = outputdir.replace(f'_SF{year}_', '_SF2018_')\n",
    "                            hv_nom_18 = uproot.open(outputdir_ps_18.replace(sys_name, 'nominal')+f'inputs_{b}.root')[cat]\n",
    "                            hv_ps_18 = uproot.open(outputdir_ps_18+f'inputs_{b}.root')[cat+'_'+sys_name]\n",
    "                            hv[cat].value *= hv_ps_18.values()[1:-1] / hv_nom_18.values()[1:-1]\n",
    "                        # print (hv[cat].value)\n",
    "                    \n",
    "                    ## Extract the sfBDTFloAround histogram.\n",
    "                    ## Method: to utilize the nominal hist for sfbdt>0.95 or 0.85 and migrate the MC-to-data confidence level in the 0.90 case\n",
    "                    if 'sfBDTFloAround' in sys_name:\n",
    "                        from scipy.stats import chi2\n",
    "                        hv_data = uproot.open(outputdir.replace(sys_name, 'nominal')+f'inputs_{b}.root')['data_obs'].values()[1:-1]  ## nominal data hist for 0.90\n",
    "                        _bdtname = '95' if 'Up' in sys_name else '85'\n",
    "                        fr = uproot.open(outputdir.replace(sys_name, 'nominal').replace(f'/bdt{int(g_sfBDT_val_list[-1]*1000)}/',f'/bdt{_bdtname}0/')+f'inputs_{b}.root')\n",
    "                        fr_data, fr_mc = fr['data_obs'].values()[1:-1], fr['flvC'].values()[1:-1]+fr['flvB'].values()[1:-1]+fr['flvL'].values()[1:-1]  ## nominal data & MC hist for 0.95 or 0.85 (depends on Up or Down)\n",
    "                        \n",
    "                        ## For each bins, migrate the confidence level of MC yield F0 given data yield D0 to the target data yield D => F\n",
    "                        hv_mc = []\n",
    "                        for D, D0, F0 in zip(hv_data, fr_data, fr_mc):\n",
    "                            ## The precise calculation\n",
    "                            F = 0.5*chi2.ppf(chi2.cdf(2*F0, 2*D0+2), 2*D+2) if F0>D0 else 0.5*chi2.ppf(chi2.cdf(2*F0, 2*D0), 2*D)\n",
    "                            if F == np.inf: ## in case the formula results in inf (may occur if F0 >> D0)\n",
    "                                assert F0 > D0\n",
    "                                sigD0 = 0.5 * chi2.ppf(1-(1-0.682689492)/2, 2*D0+2) - D0\n",
    "                                sigD = 0.5 * chi2.ppf(1-(1-0.682689492)/2, 2*D+2) - D\n",
    "                                F = D + sigD/sigD0*(F0-D0)\n",
    "                            hv_mc.append(F)\n",
    "                        \n",
    "                        ## Obtain flavor template based on the flavor proportion in 0.95 or 0.85 region\n",
    "                        hv[cat].value = np.nan_to_num(hv_mc * fr[cat].values()[1:-1] / fr_mc, nan=0)\n",
    "                        \n",
    "            ## Fill the hv[cat] (for qcd*, fill hv[cat+'_herwig.value']) into TH1D and save into ROOT\n",
    "            for cat in hist.keys():\n",
    "                ## Special handling for qcdSyst / qcdKdeSyst\n",
    "                if 'qcd' in sys_name and 'SystUp' in sys_name:\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, hv[cat+'_herwig.value'][i])\n",
    "                elif 'qcd' in sys_name and 'SystDown' in sys_name:\n",
    "                    hv[cat+'_herwig.value'] = g_hist_qcdsyst[(sys_name.replace('Down','Up'), b, cat)]\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, 2 * hv[cat].value[i] - hv[cat+'_herwig.value'][i])\n",
    "                    g_hist_qcdsyst[(sys_name.replace('Down','Up'), b, cat)] = None\n",
    "\n",
    "                ## Normal routine\n",
    "                else:\n",
    "                    for i in range(nbin):\n",
    "                        hist[cat].SetBinContent(i+1, hv[cat].value[i])\n",
    "                        hist[cat].SetBinError(i+1, np.sqrt(hv[cat].variance[i]))\n",
    "                \n",
    "                ## Fix some buggy points\n",
    "                if cat!='data':\n",
    "                    for i in range(nbin):\n",
    "                        if hist[cat].GetBinContent(i+1) <= 1e-3:\n",
    "                            hist[cat].SetBinContent(i+1, 1e-3)\n",
    "                            hist[cat].SetBinError(i+1, 1e-3)\n",
    "                        elif hist[cat].GetBinError(i+1) > hist[cat].GetBinContent(i+1):\n",
    "                            hist[cat].SetBinError(i+1, hist[cat].GetBinContent(i+1))\n",
    "\n",
    "                hist[cat].Write()\n",
    "        ## Close the ROOT file if error occurs (otherwise the notebook is easily corrupted)\n",
    "        finally:\n",
    "            fw.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we launch the template maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data/MC comparison plots\n",
    "\n",
    "Based on the ak-array dict `arr`, this section aims to make data and MC plots, while MC is categorized into three flavors: C/B/L.\n",
    "With the universial make_data_mc_plots function, one can make specify any final selection, any sample list to produce the standard hist+ratio plot.\n",
    "\n",
    "The below recipe can make a default set of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ configuration  ===================\n",
    "\n",
    "def make_config_dm(sl_dm, wgtstr_dm):\n",
    "    return {\n",
    "        'data':  ('Data',       'jetht-noht',      '1.0',    ''      ),\n",
    "        'flvB':  ('QCD (flvB)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons>=1'  ),\n",
    "        'flvC':  ('QCD (flvC)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons==0 & fj_x_nchadrons>=1'  ),\n",
    "        'flvL':  ('QCD (flvL)', sl_dm[:-1],        wgtstr_dm,   'fj_x_nbhadrons==0 & fj_x_nchadrons==0'  ),\n",
    "    }\n",
    "\n",
    "categories_dm = ['flvL', 'flvB', 'flvC', 'data']\n",
    "\n",
    "bininfo_dm = [ #(savename, vname, nbin, xmin, xmax, label)\n",
    "    ('ht', 'ht', 50, 0, 2000, r'$H_{T}$ [GeV]'),\n",
    "    ('fj_x_pt', 'fj_x_pt', 20, 200, 800, r'$p_{T}(AK15)$ [GeV]'),\n",
    "    ('fj_x_eta', 'fj_x_eta', 20, -2.5, 2.5, r'$\\eta(AK15)$'),\n",
    "    ('fj_x_sdmass', 'fj_x_sdmass', 15, 50, 200, r'$m_{SD}(AK15)$ [GeV]'),\n",
    "    ('fj_x_sfBDT', 'fj_x_sfBDT', 50, 0.5, 1, r'$sfBDT(AK15)$'),\n",
    "\n",
    "    ('fj_x_ParticleNetMD_XccVsQCD', 'fj_x_ParticleNetMD_XccVsQCD', 40, 0, 1, r'ParticleNetMD_XccVsQCD(AK15)'),\n",
    "    ('fj_x_ParticleNetMD_XccVsQCD_08', 'fj_x_ParticleNetMD_XccVsQCD', 40, 0.8, 1, r'ParticleNetMD_XccVsQCD(AK15)-u'),\n",
    "    \n",
    "    (\"fj_x_btagcsvv2\", \"fj_x_btagcsvv2\", [0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99,0.995,1], None, None, r'$CSVv2$'),\n",
    "    (\"fj_x_mSV12_ptmax_log\", \"fj_x_mSV12_ptmax_log\", [-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2,3.9], None, None, r'$log(m_{SV1,p_{T}\\,max}\\; /GeV)$'),\n",
    "    (\"fj_x_mSV12_dxysig_log\", \"fj_x_mSV12_dxysig_log\", [-0.8,-0.4,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,2.5,3.2], None, None, r'$log(m_{SV1,d_{xy}sig\\,max}\\; /GeV)$'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ slim on cc-tagger, sfBDT, then make data/MC plots ===================\n",
    "\n",
    "import seaborn as sns\n",
    "def set_sns_color(*args):\n",
    "    sns.palplot(sns.color_palette(*args))\n",
    "    sns.set_palette(*args)\n",
    "\n",
    "def calc_custom_masks(sl_dm, filter_list):\n",
    "    for sam in sl_dm:\n",
    "        for mask in filter_list + ([] if sam=='jetht-noht' else ['fj_x_flvB', 'fj_x_flvC', 'fj_x_flvL']):\n",
    "            for i in ['1','2']:\n",
    "                if mask.replace('fj_x', f'fj_{i}') not in arr[sam].maskdict.keys():\n",
    "                    print('new mask calculated (fj_x -> fj_1/2): ', mask.replace('fj_x', f'fj_{i}'))\n",
    "                    if 'fj_x_pt' in mask:\n",
    "                        import re\n",
    "                        ptmin, ptmax = re.findall('fj_x_pt(\\S+)to(\\S+)', mask)[0]\n",
    "                        ptmax = '100000' if ptmax=='Inf' else ptmax\n",
    "                        arr[sam].maskdict[mask.replace('fj_x', f'fj_{i}')] = eval_expr(arr[sam], f'(fj_{i}_pt>={ptmin}) & (fj_{i}_pt<{ptmax})')\n",
    "                    elif 'fj_x_flv' in mask:\n",
    "                        arr[sam].maskdict[f'fj_{i}_flvB'] = eval_expr(arr[sam], f'fj_{i}_nbhadrons>=1')\n",
    "                        arr[sam].maskdict[f'fj_{i}_flvC'] = eval_expr(arr[sam], f'(fj_{i}_nbhadrons==0) & (fj_{i}_nchadrons>=1)')\n",
    "                        arr[sam].maskdict[f'fj_{i}_flvL'] = eval_expr(arr[sam], f'(fj_{i}_nbhadrons==0) & (fj_{i}_nchadrons==0)')\n",
    "                    elif 'fj_x_sfBDT' in mask or 'fj_x_ParticleNetMD_XccVsQCD' in mask:\n",
    "                        arr[sam].maskdict[mask.replace('fj_x', f'fj_{i}')] = eval_expr(arr[sam], mask.replace('fj_x', f'fj_{i}'))\n",
    "\n",
    "    \n",
    "def make_data_mc_plots(sl_dm, config_dm, filter_list, prefix, **kwargs):\n",
    "    r\"\"\"To make standard hist+ratio plots based on the sample list and the final selection\n",
    "    Arguments:\n",
    "        sl_dm: sample list\n",
    "        config_dm: configuration set for each categories in the plots, in the dict format. name: (label, sample/sample list, weight string, cat selection)\n",
    "        filter_list: keys of maskdict. The corresponding selections are used to produce the plots\n",
    "        prefix: prefix string used in the output plot title\n",
    "        kwargs: includes further KDE-related variables\n",
    "    \"\"\"\n",
    "    \n",
    "    calc_custom_masks(sl_dm, filter_list)\n",
    "    result_dic = {savename: {} for savename, _, _, _, _, _ in bininfo_dm}\n",
    "    for savename, vname, nbin, xmin, xmax, vlabel in bininfo_dm:\n",
    "        if 'plot_vars' in kwargs and savename not in kwargs['plot_vars']:\n",
    "            continue\n",
    "        if not isinstance(nbin, int):\n",
    "            edges, xmin, xmax, nbin = nbin, min(nbin), max(nbin), len(nbin)\n",
    "        else:\n",
    "            edges = np.linspace(xmin, xmax, nbin+1)\n",
    "\n",
    "        label, hdm = {}, {}\n",
    "        underflow = False if vlabel[-2:] in ['-u','-a'] else True\n",
    "        overflow  = False if vlabel[-2:] in ['-o','-a'] else True\n",
    "        if vlabel[-2:] in ['-u','-o','-a']:\n",
    "            vlabel = vlabel[:-2]\n",
    "        \n",
    "        if 'g_do_kde_vars' in kwargs and savename in kwargs['g_do_kde_vars'] and kwargs['g_do_kde_vars'][savename]==True:\n",
    "            g_do_kde_vars = True\n",
    "            kde = {}\n",
    "        else:\n",
    "            g_do_kde_vars = False\n",
    "        \n",
    "        ## Loop over categories to extract the hist for each flavor and data\n",
    "        for cat in categories_dm:\n",
    "            lab, sam, wgt, sel = config_dm[cat]\n",
    "            label[cat] = lab\n",
    "            if cat != 'data':\n",
    "                _content = concat_array_fj12(arr, expr=vname, sam_list=sam, filter_list=['fj_x_base_subst']+filter_list+[f'fj_x_{cat}'])\n",
    "                _weights = concat_array_fj12(arr, expr=wgt,   sam_list=sam, filter_list=['fj_x_base_subst']+filter_list+[f'fj_x_{cat}'])\n",
    "                hdm[cat] = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow)\n",
    "                if g_do_kde_vars:\n",
    "                    from scipy.stats import gaussian_kde\n",
    "                    from scipy import integrate\n",
    "                    import multiprocessing\n",
    "                    if 'custom_kde' in kwargs.keys() and savename in kwargs['custom_kde']:\n",
    "                        kde[cat] = kwargs['custom_kde'][savename][cat]\n",
    "                        kde_int_res = [\n",
    "                                integrate.quad(kde[cat][0], -np.inf if (i==0 and underflow) else edges[i], \n",
    "                                                  +np.inf if (i==len(edges)-1 and overflow) else edges[i+1]) for i in range(len(edges)-1)]\n",
    "                    else:\n",
    "                        kdetmp = gaussian_kde(_content, weights=np.clip(_weights, 0, np.inf))\n",
    "                        if 'g_custom_kde_bw' in kwargs.keys() and savename in kwargs['g_custom_kde_bw']:\n",
    "                            kdetmp = gaussian_kde(_content, weights=np.clip(_weights, 0, np.inf), bw_method=kdetmp.factor/kwargs['g_custom_kde_bw'][savename])\n",
    "                        kde[cat] = (kdetmp, _weights.sum())\n",
    "                        kde_int_res = [(kde[cat][0].integrate_box_1d(-np.inf if (i==0 and underflow) else edges[i], +np.inf if (i==len(edges)-1 and overflow) else edges[i+1]), 0.) for i in range(len(edges)-1)]\n",
    "                    hdm[cat+'_kde'] = hdm[cat].copy()\n",
    "                    hdm[cat+'_kde'].view(flow=True).value = np.array([kde_int_res[i][0] for i in range(len(edges)-1)]) * kde[cat][1]\n",
    "                    hdm[cat+'_kde'].view(flow=True).variance = np.zeros(len(edges)-1)\n",
    "                        \n",
    "            else: ## is data: no sel, weight=1\n",
    "                _content = concat_array_fj12(arr, expr=vname, sam_list=sam, filter_list=['fj_x_base']+filter_list)\n",
    "                _weights = np.ones_like(_content)\n",
    "                hdm[cat] = get_hist(_content, bins=edges, weights=_weights, underflow=underflow, overflow=overflow)\n",
    "                \n",
    "        cat_sufs = ['']\n",
    "        if g_do_kde_vars:\n",
    "            cat_sufs += ['_kde']\n",
    "        for cat_suf in cat_sufs:\n",
    "            ## Draw the standard hist_ratio plot\n",
    "            set_sns_color('cubehelix_r', 3) ## set the color palette\n",
    "            f = plt.figure(figsize=(12,12))\n",
    "            gs = mpl.gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.05) \n",
    "            \n",
    "            ## Upper histogram panel\n",
    "            ax = f.add_subplot(gs[0])\n",
    "            hep.cms.label(data=True, paper=False, year=2016, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "            ax.set_xlim(xmin, xmax); ax.set_xticklabels([]); ax.set_ylabel('Events / bin', ha='right', y=1.0)\n",
    "\n",
    "            plot_hist([hdm[cat+cat_suf] for cat in categories_dm if cat!='data'], bins=edges, label=[label[cat] for cat in categories_dm if cat!='data'], histtype='fill', edgecolor='k', linewidth=1, stack=True) ## draw stacked bkg\n",
    "            cats_mc = list(set(categories_dm) - set(['data']))\n",
    "            hdm_add = hdm[cats_mc[0]+cat_suf].copy()\n",
    "            for cat in cats_mc[1:]:\n",
    "                hdm_add += hdm[cat+cat_suf]\n",
    "            bkgtot, bkgtot_err = hdm_add.view(flow=True).value, np.sqrt(hdm_add.view(flow=True).variance)\n",
    "            ax.fill_between(edges, (bkgtot-bkgtot_err).tolist()+[0], (bkgtot+bkgtot_err).tolist()+[0], label='BKG unce.', step='post', hatch='///', edgecolor='darkblue', facecolor='none', linewidth=0) ## draw bkg unce.\n",
    "            plot_hist(hdm['data'], bins=edges, label='Data', histtype='errorbar', color='k', markersize=15, elinewidth=1.5) ## draw data\n",
    "            # ax.set_yscale('log')\n",
    "\n",
    "            ax.legend()\n",
    "            # ax.legend(loc='upper left'); ax.set_ylim(0, 1.4*ax.get_ylim()[1])\n",
    "            \n",
    "            ## Ratio panel\n",
    "            ax1 = f.add_subplot(gs[1]); ax1.set_xlim(xmin, xmax); ax1.set_ylim(0.001, 1.999)\n",
    "            ax1.set_xlabel(vlabel, ha='right', x=1.0); ax1.set_ylabel('Data / MC', ha='center')\n",
    "            ax1.plot([xmin,xmax], [1,1], 'k'); ax1.plot([xmin,xmax], [0.5,0.5], 'k:'); ax1.plot([xmin,xmax], [1.5,1.5], 'k:')\n",
    "\n",
    "            hr = hdm['data'].view(flow=True).value / hdm_add.view(flow=True).value\n",
    "            # hr_err = hr * np.sqrt(hdm['data'].view(flow=True).variance/(hdm['data'].view(flow=True).value**2) + hdm_add.view(flow=True).variance/(hdm_add.view(flow=True).value**2))\n",
    "            hr_dataerr = hr * np.sqrt(hdm['data'].view(flow=True).variance/(hdm['data'].view(flow=True).value**2))\n",
    "            ax1.fill_between(edges, ((bkgtot-bkgtot_err)/bkgtot).tolist()+[0], ((bkgtot+bkgtot_err)/bkgtot).tolist()+[0], step='post', hatch='///', edgecolor='darkblue', facecolor='none', linewidth=0) ## draw bkg unce.\n",
    "            hep.histplot(np.nan_to_num(hr, nan=-1), bins=edges, yerr=np.nan_to_num(hr_dataerr), histtype='errorbar', color='k', markersize=15, elinewidth=1) ## draw data in ratio plot\n",
    "\n",
    "            filter_list_str = '_'.join(filter_list)\n",
    "            print('save plot: ', f'plots/{g_dirname}_{year}/{prefix}__{filter_list_str}__{savename}{cat_suf}.png/pdf')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}__{filter_list_str}__{savename}{cat_suf}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}__{filter_list_str}__{savename}{cat_suf}.pdf')\n",
    "\n",
    "        ## kde/orig comparison plots\n",
    "        if g_do_kde_vars:\n",
    "            mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green'])\n",
    "            f, ax = plt.subplots(figsize=(12,12))\n",
    "            hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "            x_contin = np.linspace(xmin, xmax, 201)\n",
    "            bin_width = edges[int(nbin/2)+1] - edges[int(nbin/2)]\n",
    "            for cat, color in zip(['flvC', 'flvB', 'flvL'], ['blue', 'red', 'green']):\n",
    "                lab, sam, wgt, sel = config_dm[cat]\n",
    "                ax.plot(x_contin, kde[cat][0](x_contin) * kde[cat][1] * bin_width, label=lab+' KDE', linestyle=':', color=color)\n",
    "            for cat, color in zip(['flvC', 'flvB', 'flvL'], ['blue', 'red', 'green']):\n",
    "                lab, sam, wgt, sel = config_dm[cat]\n",
    "                hep.histplot(hdm[cat+'_kde'].view(flow=True).value, bins=edges, label=lab+' KDE integral', linestyle='--', color=color)\n",
    "                plot_hist(hdm[cat], bins=edges, label=lab, normed=False, color=color)\n",
    "            ax.set_xlim(xmin, xmax); ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.', ha='right', y=1.0); ax.legend()\n",
    "\n",
    "            filter_list_str = '_'.join(filter_list)\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}:kde_shape__{filter_list_str}__{savename}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{prefix}:kde_shape__{filter_list_str}__{savename}.pdf')\n",
    "            \n",
    "\n",
    "g_do_kde_vars = {'fj_x_btagcsvv2':True, 'fj_x_mSV12_ptmax_log':True, 'fj_x_mSV12_dxysig_log':True}\n",
    "g_custom_kde_bw = {'fj_x_btagcsvv2':15, 'fj_x_mSV12_ptmax_log':4, 'fj_x_mSV12_dxysig_log':4}\n",
    "\n",
    "g_dirname = 'test_datamc' ## config me\n",
    "if not os.path.exists(f'plots/{g_dirname}_{year}'):\n",
    "    os.makedirs(f'plots/{g_dirname}_{year}')\n",
    "\n",
    "for ptlab in ['pt200to250', 'pt250to300', 'pt300to350', 'pt350to400', 'pt400to500', 'pt500toInf'] + ['pt200toInf']:\n",
    "    ## 1. With MadGraph sample list\n",
    "    wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl'\n",
    "    sl_dm = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.5'], prefix='mg')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.9'], prefix='mg')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.9', 'fj_x_ParticleNetMD_XccVsQCD>0.95'], prefix='mg')\n",
    "\n",
    "    ## 2. With MadGraph sample list, while using the optional MC-to-data reweight scheme (on pT)\n",
    "    wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_ad_ptwgt'\n",
    "    sl_dm = ['subst_qcd-mg-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.9'], prefix='mg_ptwgt')\n",
    "    \n",
    "    ## 3. With Herwig sample list\n",
    "    wgtstr_dm = f'genWeight*xsecWeight*puWeight*{lumi[year]}*fj_x_htwgt_herwig*fj_x_sfbdtwgt_g90_herwig_incl'\n",
    "    sl_dm = ['subst_qcd-herwig-noht', 'subst_top-noht', 'subst_v-qq-noht', 'jetht-noht']\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.5'], prefix='herwig')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.9'], prefix='herwig')\n",
    "    make_data_mc_plots(sl_dm, make_config_dm(sl_dm, wgtstr_dm), filter_list=[f'fj_x_{ptlab}', 'fj_x_sfBDT>0.9', 'fj_x_ParticleNetMD_XccVsQCD>0.95'], prefix='herwig', \n",
    "                       g_do_kde_vars=g_do_kde_vars, g_custom_kde_bw=g_custom_kde_bw) ## also make the KDE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal/proxy comparison plots\n",
    "\n",
    "Based on the ak-array dict `arr`, The below recipe creates the proxy jet (from MC) and h->cc signal jet comparison plots on various jet observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the hcc signal tree\n",
    "arr['vhcc-2L'] = NanoEventsFactory.from_file(f'samples/trees/20200906_VH_extfillsv_2016_2L/mc/vhcc_tree.root', schemaclass=BaseSchema).events()\n",
    "\n",
    "boosted = \"(v_pt>200) & (ak15_pt>200) & (dphi_V_ak15>2.5) & (ak15_sdmass>50) & (ak15_sdmass<200)\"\n",
    "basecut_vhcc_2L = \"(v_mass>75) & (v_mass<105) & (((np.abs(lep1_pdgId)==11) & passTrigEl) | ((np.abs(lep1_pdgId)==13) & passTrigMu)) & \" + boosted + \" & (n_ak4<3)\"\n",
    "arr['vhcc-2L'].maskdict = {}\n",
    "arr['vhcc-2L'].maskdict['base'] = eval_expr(arr['vhcc-2L'], basecut_vhcc_2L)\n",
    "\n",
    "basesel = { # name: cut, label\n",
    "    'sv': (\"(fj_x_sj1_nsv>=1) & (fj_x_sj2_nsv>=1)\", r'$N_{SV}^{match}\\geq 1$'),\n",
    "    'tightsv': (\"((fj_x_sj1_sv1_ntracks>2) & (np.abs(fj_x_sj1_sv1_dxy)<3) & (fj_x_sj1_sv1_dlensig>4) & (fj_x_sj2_sv1_ntracks>2) & (np.abs(fj_x_sj2_sv1_dxy)<3) & (fj_x_sj2_sv1_dlensig>4))\", r'$N_{SV,tight}^{match}\\geq 1$'),\n",
    "}\n",
    "def func_basesel(name):\n",
    "    if name in basesel.keys():\n",
    "        return basesel[name]\n",
    "    elif name[:5]=='sfbdt':\n",
    "        x = float(name[5:])/1000.\n",
    "        return ('(fj_x_sfBDT>%.3f)'%x, r'$sfBDT>%.2f$'%x)\n",
    "    else:\n",
    "        raise RuntimeError('Baseline cut name not recognized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bininfo = [ #(vname, nbin, xmin, xmax, label, *vname for nominal*)   \n",
    "    ('fj_x_ParticleNetMD_XccVsQCD', 20, 0, 1, 'ParticleNetMD_XccVsQCD (AK15)', 'ak15_ParticleNetMD_HccVsQCD'),\n",
    "    ('fj_x_sdmass', 15, 50, 200, r'$m_{SD}$ (AK15)', 'ak15_sdmass'),\n",
    "    ('fj_x_tau21', 20, 0, 1, r'$\\tau_{21}$ (AK15)', 'ak15_tau21'), ##avaliable\n",
    "    \n",
    "    ('fj_x_deltaR_sj12', 40, 0, 1.5, r'$\\Delta R_{sj_{1},sj_{2}}$ (AK15)', 'ak15_deltaR_sj12'),\n",
    "    ('fj_x_pt', 40, 0, 1000, r'$p_{T}$ (AK15)', 'ak15_pt'),\n",
    "    ('fj_x_sj1_pt', 40, 0, 1000, r'$p_{T,sj_{1}}$ (AK15)', 'ak15_sj1_pt'),\n",
    "    ('fj_x_sj1_rawmass', 40, 0, 200, r'$m_{sj_{1},raw}$ (AK15)', 'ak15_sj1_rawmass'), ##avaliable\n",
    "    ('fj_x_sj2_pt', 40, 0, 1000, r'$p_{T,sj_{2}}$ (AK15)', 'ak15_sj2_pt'),\n",
    "    ('fj_x_sj2_rawmass', 40, 0, 200, r'$m_{sj_{2},raw}$ (AK15)', 'ak15_sj2_rawmass'), ##avaliable\n",
    "    \n",
    "    ('fj_x_nsv', 10, 0, 10, r'$N_{SV}$ (AK15)', 'ak15_nlooseSV'), ##avaliable\n",
    "    ('fj_x_nsv_ptgt25', 8, 0, 8, r'$N_{SV,p_{T}\\geq 25}$ (AK15)', 'ak15_nlooseSV_ptgt25'), ##avaliable\n",
    "    ('fj_x_nsv_ptgt50', 8, 0, 8, r'$N_{SV,p_{T}\\geq 50}$ (AK15)', 'ak15_nlooseSV_ptgt50'), ##avaliable\n",
    "    ('fj_x_ntracks', 20, 0, 20, r'$N_{tracks}$ (AK15)', 'ak15_nlooseSV_ntracks'), ##avaliable\n",
    "    ('fj_x_ntracks_sv12', 20, 0, 20, r'$N_{tracks\\;for\\;SV_{1,2}}$ (AK15)', 'ak15_nlooseSV_ntracks_sv12'), ##avaliable\n",
    "    ('fj_x_sj1_nsv', 20, 0, 20, r'$N_{SV\\;from\\;sj_{1}}$ (AK15)', 'ak15_sj1_nlooseSV'), ##avaliable\n",
    "    ('fj_x_sj1_ntracks', 20, 0, 20, r'$N_{tracks\\;from\\;sj_{1}}$ (AK15)', 'ak15_sj1_nlooseSV_ntracks'), ##avaliable\n",
    "    ('fj_x_sj1_sv1_pt', 20, 0, 200, r'$p_{T,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_pt'),\n",
    "    ('fj_x_sj1_sv1_mass', 20, 0, 50, r'$m_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_mass'), ##avaliable\n",
    "    ('fj_x_sj1_sv1_masscor', 20, 0, 50, r'$m_{cor\\;for\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_masscor'),\n",
    "    ('fj_x_sj1_sv1_ntracks', 20, 0, 20, r'$N_{tracks\\;from\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_ntracks'),\n",
    "    ('fj_x_sj1_sv1_dxy', 20, 0, 5, r'$d_{xy,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dxy'),\n",
    "    ('fj_x_sj1_sv1_dxysig', 20, 0, 20, r'$\\sigma_{d_{xy},\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dxysig'),\n",
    "    ('fj_x_sj1_sv1_dlen', 20, 0, 5, r'$d_{z,\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dlen'),\n",
    "    ('fj_x_sj1_sv1_dlensig', 20, 0, 20, r'$\\sigma_{d_{z},\\;SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_dlensig'),\n",
    "    ('fj_x_sj1_sv1_chi2ndof', 20, 0, 5, r'$\\chi^2 / Ndof_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_chi2ndof'),\n",
    "    ('fj_x_sj1_sv1_pangle', 40, 0, 5, r'$pAngle_{SV_{1}\\;in\\;sj_{1}}$ (AK15)', 'ak15_sj1_looseSV_pangle'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dirname = 'test_sigpxy' ## config me\n",
    "if not os.path.exists(f'plots/{g_dirname}_{year}'):\n",
    "    os.makedirs(f'plots/{g_dirname}_{year}')\n",
    "\n",
    "## Make comparison plots for normal weight (MC adopt the same weight as in the fit), or for additional mass / pT / tau21 weight\n",
    "for wgtfac, pfwgt in zip(['1','fj_x_massdatamcwgt','fj_x_ptdatamcwgt'], ['nom', 'massdatamcwgt', 'ptdatamcwgt']):\n",
    "\n",
    "    wgtstr = f'genWeight*xsecWeight*puWeight*fj_x_htwgt*fj_x_sfbdtwgt_g90_incl*{wgtfac}'\n",
    "    wgtstr_vhcc_2L = 'genWeight*xsecWeight*puWeight'\n",
    "\n",
    "    mpl.rcParams['axes.prop_cycle'] = cycler(color=['blue', 'red', 'green', 'violet', 'darkorange', 'black', 'cyan', 'yellow'])\n",
    "    do_rwgt = 0\n",
    "    for ptmin, ptmax in [(200, 250), (250, 300), (300, 350), (350, 400), (400, 500), (500, 100000), (200, 100000)]:\n",
    "        presel, presel1 = f'(fj_x_pt>{ptmin}) & (fj_x_pt<{ptmax})', f'(ak15_pt>{ptmin}) & (ak15_pt<{ptmax})'\n",
    "        label = {'subst_qcd-mg-noht': r'g(cc) (subst.)', 'vhcc-2L':r'$Z(\\ell\\ell)H(cc)$'}\n",
    "        for i in ['1','2']:\n",
    "            arr['subst_qcd-mg-noht'].maskdict[f'_tmp_fj_{i}_sigpxy_presel'] = eval_expr(arr['subst_qcd-mg-noht'], presel.replace('fj_x', f'fj_{i}'))\n",
    "        arr['vhcc-2L'].maskdict['_tmp_sigpxy_presel'] = eval_expr(arr['vhcc-2L'], presel1)\n",
    "        \n",
    "        for vname, nbin, xmin, xmax, vlabel, vname1 in bininfo:\n",
    "            f, ax = plt.subplots(figsize=(12,12))\n",
    "            hep.cms.label(data=False, paper=False, year=year, ax=ax, rlabel=r'%s $fb^{-1}$ (13 TeV)'%lumi[year], fontname='sans-serif')\n",
    "\n",
    "            for sam in ['vhcc-2L']:\n",
    "                _content = concat_array(arr, expr=vname1, sam_list=sam, filter_list=['base', '_tmp_sigpxy_presel'])\n",
    "                _weights = concat_array(arr, expr=wgtstr_vhcc_2L, sam_list=sam, filter_list=['base', '_tmp_sigpxy_presel'])\n",
    "                h = get_hist(_content, bins=np.linspace(xmin, xmax, nbin+1), weights=_weights)\n",
    "                plot_hist(h, label=label[sam]+' $N_{SV}^{match}\\geq 1$' if sam=='qcd-mg' else label[sam], normed=True)\n",
    "\n",
    "            for sam in ['subst_qcd-mg-noht']:\n",
    "                for ext in ['sv+sfbdt500', 'sv+sfbdt850', 'sv+sfbdt900', 'sv+sfbdt950']:\n",
    "                    cutstr = ' & '.join(list(filter(None, [presel]+[func_basesel(cname)[0] for cname in ext.split('+')]))) ## join the cut string\n",
    "                    print (cutstr)\n",
    "                    for i in ['1','2']:\n",
    "                        if f'fj_{i}_sigpxy_{ext}' not in arr[sam].maskdict.keys():\n",
    "                            arr[sam].maskdict[f'fj_{i}_sigpxy_{ext}'] = eval_expr(arr[sam], cutstr.replace('fj_x', f'fj_{i}'))\n",
    "                    _content = concat_array_fj12(arr, expr=vname, sam_list=sam, filter_list=['fj_x_base_subst', 'fj_x_flvC', '_tmp_fj_x_sigpxy_presel', f'fj_x_sigpxy_{ext}'])\n",
    "                    _weights = concat_array_fj12(arr, expr=wgtstr, sam_list=sam, filter_list=['fj_x_base_subst', 'fj_x_flvC', '_tmp_fj_x_sigpxy_presel', f'fj_x_sigpxy_{ext}'])\n",
    "                    h = get_hist(_content, bins=np.linspace(xmin, xmax, nbin+1), weights=_weights)\n",
    "                    plot_hist(h, label=label[sam]+' '+(rwgt_ext_label if do_rwgt else '')+' & '.join([func_basesel(cname)[1] for cname in ext.split('+')]), normed=True)\n",
    "\n",
    "            ax.legend()\n",
    "            ax.set_xlim(xmin, xmax)\n",
    "            ax.set_xlabel(vlabel, ha='right', x=1.0); ax.set_ylabel('A.U.', ha='right', y=1.0); \n",
    "            print('save plot: ', f'plots/{g_dirname}_{year}/{pfwgt}_{presel}__{vname}.png/pdf')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{pfwgt}_{presel}__{vname}.png')\n",
    "            plt.savefig(f'plots/{g_dirname}_{year}/{pfwgt}_{presel}__{vname}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
